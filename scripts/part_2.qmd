---
title: "Part 2"
author: "Albert Rapp"
date: August 28, 2024
format:
  html:
    code-link: true
    code-fold: true
    code-summary: "Show the code"
    self-contained: true
    embed-resources: true
    toc: true
    theme: Zephyr
    number-sections: false
execute: 
  warning: false
  message: false
  echo: false
editor_options: 
  chunk_output_type: console
---

# Part 2

```{r loading_libraries}
#| echo: false

library(tidyverse)
library(janitor)
library(lubridate)
library(openxlsx)
library(tidyxl)
library(unpivotr)
library(skimr)
library(tidyxl)


```

## Lesson 1

Overview of different formats for the same database. Those are **csv**, **csv** **with header**, **csv semicolon separator**, **tsv space separator**, **csv metadata,** **excel files,** **json files**, which are annoying looking - To be reviewed later on.

## Lesson 2: Same data in in different formats

Loading csv files. It is recommended that you Rstudio project sets up to relative paths. You don't need to use here::here(path)

```{r loading_csv_files}
#| echo: true
#| warning: false
#| code-fold: true

read_csv('/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/02_exibble.csv')


# getwd()

```

## Lesson 3: Read standard csv-files

### How to read csv files with no headers.

Loading the csv file directly will cause R to read the first row as the names of the columns. This is not what we want. Instead we need to tell `read_csv()` function that there are no column names using the option `col_names = FALSE`

By default, csv will assign generic column names.

```{r loading_csv_no_headers}
#| echo: true
#| warning: false
#| code-fold: true

read_csv('/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/01_exibble_no_headers.csv')


# read_csv('data/Part_2/01_exibble_no_headers.csv', 
#          col_names = FALSE)


```

Instead, we can create a vector with the column names.

```{r column_names}
#| echo: true
#| warning: false
#| code-fold: true

desired_colnames <-  c("fuit",
                       "letter",
                       "hrs",
                       "min",
                       "dates",
                       "missing_numbers",
                       "missing_NULL",
                       "missing_dates")

X02_exibble <- read_csv('/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/02_exibble.csv',
                        col_names = desired_colnames)


# X02_exibble <- read_csv("data/Part_2/02_exibble.csv",
#                         col_names = desired_colnames)


```

### Reading a metadata csv file.

It displays a message with illegible data. Notice that the first two columns contain no relevant information. The second row lists the names of the columns or variables, however.

```{r reading_}
#| echo: true
#| warning: false
#| code-fold: true


read_csv('/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/05_exibble_with_metadata.csv')


# read_csv("data/Part_2/05_exibble_with_metadata.csv")

```

We need to skip the first two rows using `skip =2`, which are not relevant. After which, the file is correctly displayed.

```{r skipping_rows_csv}
#| echo: true
#| warning: false
#| code-fold: true

read_csv("/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/05_exibble_with_metadata.csv",
         skip = 2)
```

### Read csv files in European format using read_csv2

Problem the first row has the name of the variables, but they are not recognized because of the use of ";" as delimiter. You need to replace the `read_csv()`function with the `read_csv2()`function.

```{r read_csv_european}
#| echo: true
#| warning: false
#| code-fold: true

read_csv2("/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/03_exibble_european.csv")

```

### Reading files with line breaks

RStudio reads this file as having two columns. Reading the quote file that appears to be connected with the line_break file.

```{r reading_line_breaks}
#| echo: true
#| warning: false
#| code-fold: true

read_csv("/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/09_line_break.csv")


```

Instead, we need to use the `read_delim()` function. And we need to specify we want as a delimiter a `,`.

```{r read_delim_function}
#| echo: true
#| warning: false
#| code-fold: true

#Another option

read_delim("/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/11_quote_doubled.csv",
           delim = ',',
           escape_double = TRUE)

```

## Lesson 4: Reading tsv and txt files

### Loading tsv files which have a separated value

The file is separated by spaces

```{r loading_tsv_files}
#| echo: true
#| warning: false
#| code-fold: true

read_tsv('/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/04_exibble.tsv')

```

### Loading arbitrary text files

The columns are separated with `|`

```{r loading_arbitrary_text_files}
#| echo: true
#| warning: false
#| code-fold: true

read_delim('/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/12_weird_delimiter.txt',
           delim = '|')


```

## Lesson 5: Reading Excel Files

One package useful for reading excel files is `oepnxls`. However, the data frame looks weird. This is why we transform it into tibble with the following function `as_tibble()`.

```{r read_excel_files}
#| echo: true
#| warning: false
#| code-fold: true

library(openxlsx)

openxlsx::read.xlsx('/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/06_exibble.xlsx') |> 
  as_tibble()

```

Notice the column `dates` not treated as a date but with a weird number. Consequently we could recode the variable dates as a date format using lubridate's function `as_dates()`. Notice we need to identify the column or variable with the dates information.

```{r transforming_dates_excel}
#| echo: true
#| warning: false
#| code-fold: true


library(lubridate)

openxlsx::read.xlsx('/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/06_exibble.xlsx') |> 
  as_tibble() |> 
  mutate(dates = lubridate::as_date(dates))


```

We could also use the `janitor` package to transfer dates into a date format

```{r janitor_date_transform}
#| echo: true
#| warning: false
#| code-fold: true

openxlsx::read.xlsx('/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/06_exibble.xlsx') |> 
  as_tibble() |> 
  mutate(dates = janitor::excel_numeric_to_date(dates))

```

## Lesson 6: Reading JSON files

JSON files require a particular package: `jsonlite` package. Notice that json files have a nested structure. Notice that json file contains \[\[80\]\].

The string `str()` function comes handy. It displays the list signifying rows. Each row displays the names of the columns, and their contents.

### Using glimpse

You can also use the `glimpse` function.

```{r reading_json_files}
#| echo: true
#| warning: false
#| code-fold: true

library(jsonlite)

jsonlite::read_json('/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/07_exibble.json') |>
  str()


```

Using glimpse function to describe the organization of the json files

```{r json_glimpse}
#| echo: true
#| warning: false
#| code-fold: true

library(jsonlite)

jsonlite::read_json('/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/07_exibble.json') |>
  glimpse()



```

### Using bind_rows() function

Use bind rows function to find together the different rows that make up the json file. It allows us to create a nice tibble file.

```{r binding_rows_json}
#| echo: true
#| warning: false
#| code-fold: true

library(jsonlite)

jsonlite::read_json('/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/07_exibble.json') |>
  bind_rows()



```

### GLIMPSE and bind_rows functions

You can use both of them to describe the components of the file. And, then create the tibble file

```{r glimpse_bind_json}

#| echo: true
#| warning: false
#| code-fold: true

library(jsonlite)

jsonlite::read_json('/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/07_exibble.json') |>
  glimpse() |> 
  bind_rows()

#OR we can request glimpse after binding the rows

jsonlite::read_json('/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/07_exibble.json') |>
  bind_rows() |> 
  glimpse()

```

### Reading a second json file

This file is a special case. It has three columns. You cannot use `bind_rows()` function for this kind of file. But you can extract information from it

```{r another_json_file}
#| echo: true
#| warning: false
#| code-fold: true

library(jsonlite)
jsonlite::read_json('/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/08_nested_example.json') |> 
  glimpse()

```

Extracting information from `08_nested_example/json` using the `pluck()` function, which can go to a particular column. In this case, we want to go to the information column and then the origin column.

```{r extract_info_json}

#| echo: true
#| warning: false
#| code-fold: true

library(jsonlite)

jsonlite::read_json('/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/08_nested_example.json') |> 
  pluck('information','origin') |> 
  glimpse()


```

### Next we could pass this information as tibble

We need to unnest them, though.

```{r passing_structure_tibble}
#| echo: true
#| warning: false
#| code-fold: true

jsonlite::read_json('/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/08_nested_example.json') |> 
  pluck('information','origin') |> 
  as_tibble() |> 
  unnest(cols = c(country,season))

#Or

jsonlite::read_json('/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/08_nested_example.json') |> 
  pluck('information','origin') |> 
  as_tibble() |> 
  unnest(cols = everything())



```

## Lesson 7: Turning missing values into true NAs

A simple procedure to recode several values as missing using CSV files

```{r finding_na}
#| echo: true
#| warning: false
#| code-fold: true

read_csv("/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/02_exibble.csv") |> 
  select(1:6)


```

### Problematic variables

Notice that these two columns have values that need to be re-coded as missing.

```{r encounter_na_values}
#| echo: true
#| warning: false
#| code-fold: true

read_csv("/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/02_exibble.csv") |> 
  select(-(1:6))

```

This means that while the numbers look like they could do what numbers do, they can’t. The following mutate will prompt the error message: "non-numeric argument to binary operator"

```{r mutate_with_not_NAs_correctly_coded}
#| echo: true
#| warning: false
#| code-fold: true

# read_csv("/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/02_exibble.csv") |> 
#   select(-(1:6)) |> 
#   mutate(number = missing_NULL + 2)

```

### Recoding missing while reading a csv file

One can actually tell `read_csv()` what constitutes a missing value. With the `na` argument, you can specify a vector of strings that should be treated as `NA`. See also [Albert Rapp'](https://3mw.albert-rapp.de/p/loading-csv-with-na?utm_source=3mw.albert-rapp.de&utm_medium=newsletter&utm_campaign=loading-files-with-missing-numbers&_bhlid=0c880d03679a6b7398f77d3c6766fec6142212af)s 3 minutes Wednesday. In this case, spaces, '(null)', and Unknown are alternative codes for missing.

```{r recoding_missing_reading}
#| echo: true
#| warning: false
#| code-fold: true

read_csv("/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/02_exibble.csv",
         na = c("", "(null)", "Unknown")) |>
  select(-(1:6)) |> 
  glimpse()

```

### Handling missing values in excel files

Notice variables missing_Null and missing_dates are incorrectly identified as characters.

```{r handling_na_excel}
#| echo: true
#| warning: false
#| code-fold: true

openxlsx::read.xlsx("/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/06_exibble.xlsx") |> 
  as_tibble() |> 
  select(7:8) |> 
  glimpse()

```

### Recoding missing in a xlsx file at the outset

You can actually tell `read_xlsx()` what constitutes a missing value. With the `na` argument, you can specify a vector of strings that should be treated as `NA`.

```{r recoding_na_excel}
#| echo: true
#| warning: false
#| code-fold: true

openxlsx::read.xlsx("/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/06_exibble.xlsx",
                    na.strings = c("", "(null)", "Unknown")) |> 
  as_tibble() |> 
  select(-c(1:6))


```

However, the `na.strings()` conversion do not transform the variables back to their correct format. In this case, the missing_dates column is reported to be a character, but it is a date.

## Lesson 8: Transforming/Parsing numbers

Correcting or parsing numbers in an excel file. Notice that this option codes missing values for the variable missing_NULL while recoding from character to numeric. However, it did not recode the value of Unknown in missing_dates as missing. The `parse_number(x, na = c("", "NA"))` function is part of the `readr` package.

```{r correct_na_format_excel}
#| echo: true
#| warning: false
#| code-fold: true

#* Notice this option codes missing values for one missing_NULL only

openxlsx::read.xlsx(
  "/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/06_exibble.xlsx") |> 
  as_tibble() |> 
  select(-c(1:6)) |> 
  mutate(missing_NULL = parse_number(
    missing_NULL,
    na = c("", "NA", "(null)", "Unknown")
  ))

```

The alternative script below re-codes the missing values for both missing_NULL and missing_dates at the outset, while recoding missing_NULL and missing_dates as numeric and dates respectively thereafter.

```{r alternative_correct_na_format_excel}
#| echo: true
#| warning: false
#| code-fold: true

library(lubridate)

openxlsx::read.xlsx(
  "/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/06_exibble.xlsx",
  na.strings = c("", '(null)', "Unknown")) |> 
  as_tibble() |> 
  select(-c(1:6)) |> 
  mutate(missing_NULL = parse_number(missing_NULL),
         missing_dates = ymd_hms(missing_dates))


```

## Lesson 9: Transforming/Parsing dates & date times

Notice the use of quiet in the recoding of missing_dates variable as date. There is no option in the lubridate function to handle missing values. This is why we need to use the quite argument, which codes the string `NULL` or "Unknown" as missing. However, in the previous example, it is depicted how to code all variables missing values at the outset, and then recode them accordingly as numeric and date formats.

Notice that the readr's `parse_number()` function allows one to specify missing values using `na =`.

```{r parsing_dates_datetimes}

#| echo: true
#| warning: false
#| code-fold: true

openxlsx::read.xlsx(
  "/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/06_exibble.xlsx"
  ) |> 
  as_tibble() |> 
  select(-c(1:6)) |> 
  mutate(
    missing_NULL = parse_number(
      missing_NULL,
      #Notice parse_function allows one to report missing values
      na = c("", "NA", "(null)", "Unknown")),
    missing_dates = 
      #Notice lubridate has no option for NAs instead use quite = TRUE
      lubridate::ymd_hms(missing_dates, quiet = TRUE))


```

For extracting dates and times using the readr function parse_datetime. Notice this function allows one to specify the missing values.

```{r parse_dates_readr}
#| echo: true
#| warning: false
#| code-fold: true

openxlsx::read.xlsx(
  "/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/06_exibble.xlsx"
  ) |> 
  as_tibble() |> 
  select(-c(1:6)) |> 
  mutate(missing_NULL = parse_number(
    missing_NULL,
    na = c("", "NA", "(null)", "Unknown")
  ),
  missing_dates = parse_datetime(
    missing_dates,
    format = '%Y-%m-%d %H:%M:%S',
#Notice the readr function parse_time allows one to specify missing values
    na = "Unknown"
  )
  )

```

For extracting just dates, use the readr function `parse_date()` as shown below. Notice the `parse_date()` function allows one to specify the value standing for missing values using the option `na =`

```{r parse_dates_no_time}
#| echo: true
#| warning: false
#| code-fold: true

openxlsx::read.xlsx(
  "/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/06_exibble.xlsx"
  ) |> 
  as_tibble() |> 
  select(-c(1:6)) |> 
  mutate(missing_NULL = parse_number(
    missing_NULL,
    na = c("", "NA", "(null)", "Unknown")
  ),
  missing_dates = parse_date(
    missing_dates,
    format = '%Y-%m-%d %H:%M:%S',
    na = "Unknown"
  )
  )


```

## Lesson 10: Handling a mix of date(time) formats

Example below displays a vector with dates with different formats.

```{r mix_formats}
#| echo: true
#| warning: false
#| code-fold: true

tibble(
  mixed_datetimes = c(
    '23.12.2024 12:23',
    '23.12.2024',
    '2023-12-23',
    'NULL'
  ) )

```

The readr functions such as `parse_datetime()` and `parse_number()` won't work for this example. On the other hand, the `lubridate` package works best for this situation.

The option `orders` can tell lubridate the correct order of the date to be followed. This option save us the complicated format `format = '%Y-%m-%d %H:%M:%S'` used in parse_date from readr. Instead we use lubridate to specify day, month, year and hour "c(`dmyHM`)". And then we specify the correct format for each element of the vector.

Notice there is no option in the lubridate date function to handle missing values. This is why we need to use the `quiet` argument, which codes the string `NULL` as missing.

```{r handling_mix_date_formats}
#| echo: true
#| warning: false
#| code-fold: true

tibble(
  mixed_datetimes = c(
    '23.12.2024 12:23',
    '23.12.2024',
    '2023-12-23',
    'NULL')) |> 
  mutate(mixed_datetimes = parse_date_time(mixed_datetimes,
                                           orders = c('dmyHM','dmy','ymd'),
                                            quiet = TRUE)) 

```

## Lesson 11: Combining data sets column-wise & row-wise

This lesson introduces the equivalent concepts of merging from Stata and SPSS

### Binding rows

Binding rows is flexible. You do not need to order the columns in a particular manner provided they have the same label. `binding rows` function use the column label from the first dataset.

```{r combining_rows}
#| echo: true
#| warning: false
#| code-fold: true

tib1 <-  tibble(a = 1:5, b = 1:5)

tib2 <-  tibble(a = 6:10, b = 6:10)

bind_rows(tib1,tib2)

tib1 <-  tibble(b = 1:5, a = 1:5)

tib2 <-  tibble(a = 6:10, b = 6:10)

bind_rows(tib1,tib2)

#Another option of adding a column c with incomplete cases

tib1 <-  tibble(b = 1:5, a = 1:5)

tib2 <-  tibble(a = 6:10, b = 6:10, c = 6:10)

bind_rows(tib1,tib2)

```

### Binding columns

The only difference between binding columns and binding rows is that the former is strict about the number of rows. They have to have the same number of rows.

```{r binding_columns}
#| echo: true
#| warning: false
#| code-fold: true


tib1 <-  tibble(x = 1:5)

tib2 <-  tibble(y = fruit[1:5])

bind_cols(tib1,tib2)

```

## Lesson 12: Combining datasets by left\_ and right_join

This lesson illustrates how to combine two data sets. Each dataset has different rows, and different names for the key variable. In the flights dataset the key variable is `origin`. In the airports dataset the key variable is `faa`.

### Creating flights data set

```{r flights_data}
#| echo: true

flights <-  nycflights23::flights |> 
  as_tibble() 

```

### Creating airlines data set

```{r airlines_data}
#| echo: true
#| warning: false
#| code-fold: true

airlines <-  nycflights23::airlines |> 
  as_tibble() 


```

### left_join

Selecting some variables from the flights dataset. We want to add the name of the carrier. They key matching variable in both data sets is `carrier`.

```{r selecting_columns_flights_dataset}
#| echo: true
#| warning: false
#| code-fold: true

flights |> 
  select(year:dep_time, carrier) |> 
  left_join(airlines, by = join_by('carrier') )

```

Another example of left join that may not be as easier as the previous example. In this example we have abbreviations for airport destinations. For merging, we need another database with the full name of the airports. This name is found in the `airports` data set under the label `faa`.

In the example below, we are filtering from the airports' column faa the names that appear in the flights dataset variable origin. In other words the two variables are the same (faa = origin) but have different labels. The trick is to eliminate duplicates.

If we try to merge the two files as before a mistake will occur because R does not know that faa is the same as origin.

```{r left_join_calling_for_key}
#| echo: true
#| warning: false
#| code-fold: true

library(nycflights23) 

#* Eliminating duplicate names of airports.  
#* The end result of filtering is selecting NY airports only


airports <- nycflights23::airports |> 
  #Eliminating duplicate airports using unique function
  filter(faa %in% unique(flights$origin))

# The following script generates an error. There is no common key

# flights |>
#   select(year:dep_time, origin) |>
#   left_join(airports)



```

### Adding key in left_join.

The last script generated an error when trying to merge flights with airports. There is no common key across the two data sets. The key has a different label across the two databases. Under the flights data set the airport abbreviation appears under the variable `origin`. In the `airport` data set the abbreviation is `faa`. In other words `origin == faa`.

```{r adding_key_left_join}
#| echo: true
#| warning: false
#| code-fold: true

flights |> 
  select(year:dep_time, origin) |> 
  left_join(airports, by = join_by(origin == faa) )

```

### Adding key in right_join.

We get exactly the same dataset made up of 435,352 rows. The reason it produced the same results is the filtering we applied to the airports dataset `filter(faa %in% unique(flights$origin))`

```{r right_join_adding_key}
#| echo: true
#| warning: false
#| code-fold: true

flights |> 
  select(year:dep_time, origin) |> 
  right_join(airports,
             by = join_by(origin == faa) )
  
```

Let's see what happens when eliminating the filtering that allowed both dataset to have the same airports.

Redoing the right and left join does not produce the same number of rows. The left join yield 435,342 rows while the right produces 436,590 rows.

```{r right_join_no_filtering}
#| echo: true
#| warning: false
#| code-fold: true

airports <- nycflights23::airports 


flights |> 
  select(year:dep_time, origin) |> 
  left_join(airports,
             by = join_by(origin == faa))


flights |> 
  select(year:dep_time, origin) |> 
  right_join(airports,
             by = join_by(origin == faa))



```

Notice there is a discrepancy of about 1,000 airports. The number of airports produced under left_join is smaller than the one produced by right_join.

Providing a more detailed explanation about the discrepancies on the number of cases.

The **left_join** makes the number of cases in the first database to **prevail** in the **merger**. The contrary is true when doing right_join.

```{r illustrating_rows_merged_left_join}
#| echo: true
#| warning: false
#| code-fold: true

reduced_left <- flights |> 
  select(year:dep_time, origin) |> 
  slice(1, .by = origin) |> 
  left_join(airports,
             by = join_by(origin == faa))


reduced_left

```

Reducing the airports by right_join. It produces 1,248 missing rows.

```{r illustrating_reduced_right}
#| echo: true
#| warning: false
#| code-fold: true

reduced_right <- flights |> 
  select(year:dep_time, origin) |> 
  slice(1, .by = origin) |> 
  right_join(airports,
             by = join_by(origin == faa))  

reduced_right


```

## Lesson 13: Other join types

Other joins being illustrated with a small dataset. In this example the left_join kept all the airports from the `flights` data set, or 1,251. But the columns are empty except for the New York airports that are contained in the `flights` dataset.

```{r other_joins}
#| echo: true
#| warning: false
#| code-fold: true

flights |> 
  select(year:dep_time, origin) |> 
  slice(1, .by = origin) |> 
  left_join(airports,
             by = join_by(origin == faa)) 

airports |> 
  left_join(
    flights |> 
      select(year:dep_time, origin) |> 
      slice(1, .by = origin), 
    by = join_by(faa == origin))



```

### Full join

Using two rows, or two rows per airpot

```{r full_join}

airports |> 
  full_join(
    flights |> 
      select(year:dep_time, origin) |> 
      slice(1:2, .by = origin), 
    by = join_by(faa == origin))



```

### Anti join

It is more useful than full join because it can tell where unexpected things may go wrong. In this case it tells you the airports that could not be matched. In this case, 1,248 airports that could not be matched.

```{r anti_join}
#| echo: true
#| warning: false
#| code-fold: true

airports |> 
  anti_join(
    flights |> 
      select(year:dep_time, origin) |> 
      slice(1, .by = origin), 
    by = join_by(faa == origin))



```

## Lesson 14: Join by multiple criteria

Using join based on multiple criteria.

### Creating wind speeds dataset

```{r wind_speeds_df}
#| echo: true
#| warning: false
#| code-fold: true


wind_speeds <-  nycflights23::weather |> 
  select(origin:hour, wind_speed)


wind_speeds

```

Matching based on multiple criteria instead of just two columns. We want to enrich the flights data set with information about wind speeds.

```{r multiple_criteria_join}
#| echo: true
#| warning: false
#| code-fold: true

library(nycflights23)


flights |> 
  select(year:day,hour,origin,tailnum) |> 
  left_join(wind_speeds)


```

Notice that without specifying the matching criteria the left_join worked. It did so because the two data set share some variables in common as shown in the output:

\`by = join_by(year, month, day, hour, origin)\`

```{r explicit_multiple_join_criteria}
#| echo: true
#| warning: false
#| code-fold: true


flights |> 
  select(year:day,hour,origin,tailnum) |> 
  left_join(wind_speeds,
            by = join_by(year,month,day,hour,origin)) 


```

Matching on a certain types of column names. Simulating that the flight's column origin is labeled `dep_airport` . If we try to match these two files it won't work.

```{r incorrect_selective_multiple_matching}
#| echo: true
#| warning: false
#| code-fold: true

# flights |> 
#   select(year:day,hour,dep_airport = origin,tailnum) |> 
#   left_join(wind_speeds,
#             by = join_by(year,month,day,hour,origin)) 



```

To make it match, you have to specify that dep_airport == origin in the join_by statement.

```{r correct_selective_matching}
#| echo: true
#| warning: false
#| code-fold: true

flights |> 
  select(year:day,hour,dep_airport = origin,tailnum) |> 
  left_join(wind_speeds,
            by = join_by(year,month,day,hour,dep_airport == origin)) 



```

## Lesson 15: Rearranging data from wide to long format with pivot_longer()

Differences between wide and long format

Notice that in the long format variables are reported as key while the cell contents are reported as values.

![](images/differences_wide_longer.PNG){fig-align="center" width="414"}

### Pivot longer

illustrating these two concepts using the scores data set titled scores, which is in a wide format. Accordingly, date is the id, scores A to C are the key and the values are the score.

Let's illustrate the utility of pivot_longer by summarizing

```{r changing_wide_to_longer}
#| echo: true
#| warning: false
#| code-fold: true

scores <- read_csv('/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/13_scores_by_date.csv')

scores |> 
  #Other options for cols: cols = 2:5 cols = contains('score')
  pivot_longer(cols = -date, 
               names_to = "score_category",
               values_to = "score")

scores |> 
  pivot_longer(cols = -date, #cols = 2:5 cols = contains('score')
               names_to = "score_category",
               values_to = "score") |> 
  summarise(
    mean_score = mean(score),
    .by = score_category
  )

```

### Another example

This file needs to be rearranged before pivoting longer. Notice that `payment` the value column appears first. In this file, the first row lists the name of the columns.

```{r pivot_longer_example_payments}
#| echo: true
#| warning: false
#| code-fold: true

payments <-  read_csv('/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/14_payments.csv')


payments |> 
  pivot_longer(
    cols = contains('date')
  )

#We need meaningful names for the new columns

payments |> 
  pivot_longer(
    cols = contains('date'),
    names_to = "payment_date",
    values_to = "date"
  )

```

## Lesson 16: Advanced column name tricks with pivot_longer()

### scores data set example

Regarding the scores data set. The end result of the pivot longer is to have score categories with the label score_cat repeated. This does not make sense afterwards. We could do some cleaning. We can target the score_category column and using regex to remove unwanted strings. The `names_prefix()` can be used to removed the unwanted label `score_` at the start of the string.

```{r cleaning_scores_dataset}
#| echo: true
#| warning: false
#| code-fold: true

scores <- read_csv('/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/13_scores_by_date.csv')


#First option
scores |> 
  pivot_longer(
    cols = contains("score"),
    names_to = "score_type",
    values_to = "scores"
  )

#* Second option: removing "score_" prefix

scores |> 
  pivot_longer(cols = -date, 
               names_to = "score_category",
               values_to = "score",
               names_prefix = "score_")


```

### payments example

In the payments data base, we have suffixes that need to be removed. Under the `name` column, the suffix `_payment_date` needs to be removed. This string appears at the end of the column. Unfortunately pivot longer does not have a suffix function. However, it has the `names_pattern()` function, which works similarly to `str_extract()`. We need to add a group `(\\w*)` of characters preceding the strings to be removed. This group of characters is to be kept.

Another option is to use the regular expression find me anything `(.)`. But one dot stands for only one letter. But adding `+` makes it plural. This means: "of this dot or character, find me as many as you can". Notice the dot signifies to count at the end of the string. Notice the use of parenthesis `(\\w+ )` framing the reg exp signifies to treat the strings as a group.

```{r cleaing_payments_example}
#| echo: true
#| warning: false
#| code-fold: true

#One option to handle the group of characters
payments |> 
  pivot_longer(
    cols = contains('date'),
    names_to = "payment_date",
    values_to = 'date',
    names_pattern = "(\\w+)_payment_date"
  )

#The next option is to list the characters to keep as a group

payments |> 
  pivot_longer(
    cols = contains('date'),
    names_to = "payment_date",
    values_to = 'date',
    names_pattern = "(last|next|final)_payment_date"
  ) |> 
  arrange(utility,date)

# Another regular expression
# Find me anything (.)

payments |> 
  pivot_longer(
    cols = contains('date'),
    names_to = "payment_date",
    values_to = 'date',
    names_pattern = "(.+)_payment_date"
  ) |> 
  arrange(utility,date)
```

## Lesson 17: Making long data wide with pivot_wider()

Pivot longer is more frequently used than `pivot_wider()`. However, pivot_wider is extremely useful when creating tables. In pivot wider you need to revert the `names_to` and `values_to`.

```{r making_long_data_pivot_wider}
#| echo: true
#| warning: false
#| code-fold: true


payments |> 
  pivot_longer(
    cols = contains('date'),
    names_to = "payment_date",
    values_to = "date",
    names_pattern = "(.+)_payment_date"
  ) |> 
  pivot_wider(
    names_from = 'payment_date',
    values_from = 'date'
  )

```

### Using names_glue function

Notice that the column labeled `next` appears within ticks. In R the word `next` is a reserve name. We can use the pivot_wider function `names_glue()`. If we want a variable name we use curly brackets `{ }`. And then we write the text we want to add outside the curly brackets. In this case we want to rebuild the original value labels last_payment_date, next_payment_date and final_payment_date. Thus we add the name of the variable containing the labels; namely, payment_date

```{r removing_reserve_label_pivot_wider}
#| echo: true
#| warning: false
#| code-fold: true

payments |> 
  pivot_longer(
    cols = contains('date'),
    names_to = "payment_date",
    values_to = "date",
    names_pattern = "(.+)_payment_date"
  ) |> 
  pivot_wider(
    names_from = 'payment_date',
    values_from = 'date',
    #identify variable within {}, then add legend outside {}
    names_glue = '{payment_date}_payment_date'
  )


```

### Identifying id column

pivot_wider has an option to identify the variable or variables that are key. It is identified as `id_cols()`. By identifying utility as id column eliminates the payment column.

```{r pivot_longwer_id_columns}
#| echo: true
#| warning: false
#| code-fold: true

payments |> 
  pivot_longer(
    cols = contains('date'),
    names_to = "payment_date",
    values_to = "date",
    names_pattern = "(.+)_payment_date"
  ) |> 
  pivot_wider(
    id_cols = c('utility'),
    names_from = 'payment_date',
    values_from = 'date',
    #identify variable within {}, then add legend outside {}
    names_glue = '{payment_date}_payment_date'
  )


#Includying payment as an id column along with utility

payments |> 
  pivot_longer(
    cols = contains('date'),
    names_to = "payment_date",
    values_to = "date",
    names_pattern = "(.+)_payment_date"
  ) |> 
  pivot_wider(
    id_cols = c('utility', 'payment'),
    names_from = 'payment_date',
    values_from = 'date',
    #identify variable within {}, then add legend outside {}
    names_glue = '{payment_date}_payment_date'
  )

```

## Lesson 18: Cleaning up Excel Pivot Tables

Watch [tidyxl](https://nacnudus.github.io/tidyxl/) animation. More information on tidyxl is [here](https://cran.r-project.org/web/packages/tidyxl/vignettes/tidyxl.html).

![](images/original_table.png "Original table"){width="280"} ![](images/coerced_table-01.png "Original table"){width="360"}

Using `xlsx_cells()` function from tidyxl package. The tidyxl package is new. The `xlsx_cells()` function imports data from spreadsheets without coercing it into a rectangle. Each cell is represented by a row in a data frame, giving the cell's address, contents, formula, height, width, and keys to look up the cell's formatting in the return value of `xlsx_formats()`.

The original file, 17_rand_numbers.xlsx, is a pivot table. Variables are organized by country, region, age and gender. The tidyxl package help us to prepare the excel pivot data to be processed by other packages, like unpivotr with its function `rectify()`. The function transform the data into a rectangular fashion.

```{r cleaning_pivot_part_1}
#| echo: true
#| warning: false
#| code-fold: true

library(unpivotr)
library(tidyxl)


rand_numbers_cells <- tidyxl::xlsx_cells(
  '/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/17_rand_numbers.xlsx')

rand_numbers_cells |> 
  unpivotr::rectify()

```

Once the file is rectified we can strip the labels step by step. The first thing we need to do is to get rid of the labels instead of the values.

We start by stripping the labels step by step. The first is to target Gender, Male, Female and NAs. These descriptors are not data but metadata if you will. And, to get rid off those descriptors, we need to eliminate those entries under the "character column", in the rad_numbers_cell dataset. We are targeting in the "character" column by removing entries that have the words "Gender" and "Age " or missing. In other words, we need to filter the cells where we have missing values or have the strings Gender or Age: `filter(is.na(character)| !(character %in% c('Gender', 'Age'))`. The end result is a new column, subsequently labeled "country", which has the values of Country A and Country B in it.

Next we can start rectifying it by `behading` it. [`behead()`](http://127.0.0.1:42565/help/library/unpivotr/help/behead) takes one level of headers from a pivot table and makes it part of the data. The main purpose is to eliminate \`3(C)\` column. We want to behave on the left style, because the country labels are at the left of our original pivot table. So we use `left-up`. And then we want to assign the contents into a column we would label `country`.

But, if we look at the cells, we discover where the column country is.

Note that Albert is filtering NAs and the Gender and Age labels in the column labeled `character`. However, filtering NAs serves no purpose.

```{r cleaning_pivot_part_2}
#| echo: true
#| warning: false
#| code-fold: true

rand_numbers_cells |>
#Notice: eliminating missing values and characters that are neither Gen or Age
  filter(is.na(character) | !(character %in% c('Gender', 'Age'))) |>
  unpivotr::rectify()


#This option also works
rand_numbers_cells |> 
#Notice: eliminating missing values and characters that are neither Gen or Age
  filter(!(character %in% c('Gender', 'Age'))) |>
  unpivotr::rectify()

```

Next we can use the `behade` function of the unpivotr library to create the new variable `country` containing the values of Country A and Country B. The end result is that column 3(C) is eliminated once its values have been extracted and incorporated into the new column `country`.

```{r cleaning_pivot_part_2_behading}
#| echo: true
#| warning: false
#| code-fold: true

#Now we can start behading this 
  
rand_numbers_cells |> 
  filter(is.na(character) | !(character %in% c('Gender', 'Age')
      )
    ) |>
  unpivotr::behead(direction = 'left-up', name = 'country') |> 
  unpivotr::rectify()

```

Next step is targeting the region values, which are located in `4(D)`. At this point the character variable column only has region values. We extract the region values and put them a new column labeled 'region'.

```{r cleaning_pivot_part_3}
#| echo: true
#| warning: false
#| code-fold: true

rand_numbers_cells |> 
  filter(
  is.na(character) | !(character %in% c('Gender', 'Age'))) |>
  unpivotr::behead(direction = 'left-up',name = 'country') |>
  unpivotr::behead(direction = 'left',name = 'region') |> 
  unpivotr::rectify()
```

Doing the same for gender.

```{r cleaning_pivot_part_4}
#| echo: true
#| warning: false
#| code-fold: true

rand_numbers_cells |> 
  filter(
    is.na(character) | !(
      character %in% c('Gender', 'Age')
      )
    ) |>
  unpivotr::behead(direction = 'left-up',name = 'country'
  ) |>
   unpivotr::behead(direction = 'left',name = 'region'
  ) |> 
  unpivotr::behead(direction = 'up-left',name = 'gender'
  ) |>
  unpivotr::rectify()


```

Next we need to do is to get rid of the ages.

```{r cleaning_pivot_part_5}
#| echo: true
#| warning: false
#| code-fold: true

rand_numbers_cells |> 
  filter(
    is.na(character) | !(
      character %in% c('Gender', 'Age')
      )
    ) |>
  unpivotr::behead(direction = 'left-up',name = 'country'
  ) |>
   unpivotr::behead(direction = 'left',name = 'region'
  ) |>
  unpivotr::behead(direction = 'up-left',name = 'gender'
  ) |>
  unpivotr::behead(direction = 'up',name = 'age'
  ) |>
  unpivotr::rectify()
  
```

### Final data creation

We no longer need to rectify. All what we need is to select the new columns. Those are located at the end of the data frame. To see those 4 new columns (country, region, gender and age) you need to create a temporary file. And also include the variable/column numeric. For this procedure to work, you need to eliminate the rectify function.

```{r cleaning_pivot_part_final}
#| echo: true
#| warning: false
#| code-fold: true


#Selecting country, region, age, gender, numeric columns

rand_numbers_cells |> 
  filter(
    is.na(character) | !(
      character %in% c('Gender', 'Age')
      )
    ) |>
  unpivotr::behead('left-up',name = 'country'
  ) |>
   unpivotr::behead('left',name = 'region'
  ) |>
  unpivotr::behead('up-left',name = 'gender'
  ) |>
  unpivotr::behead('up',name = 'age'
  ) |>
  #Excludying rectity
  #unpivotr::rectify() |>
  select(country:age,numeric)

#Alternative filtering option

rand_numbers_cells |> 
  filter(!(character %in% c('Gender', 'Age')
      )
    ) |>
  unpivotr::behead('left-up',name = 'country'
  ) |>
   unpivotr::behead('left',name = 'region'
  ) |>
  unpivotr::behead('up-left',name = 'gender'
  ) |>
  unpivotr::behead('up',name = 'age'
  ) |>
  #Excludying rectity
  #unpivotr::rectify() |>
  select(country:age,numeric)


```

## Lesson 19 Beheading with multiple data formats

### Locating unwanted labels

Dealing with a complex and unbalanced spreadsheet with several pivot tables. `16_faker_sales_modified.xlsx`, under the column `1(A),` we have a couple of labels, City, Department, that are not part of the data, but they describe the data

```{r beheading_multiple_formats_1}
#| echo: true
#| warning: false
#| code-fold: true

library(tidyxl)

sales_cells <- tidyxl::xlsx_cells('/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/16_faker_sales_modified.xlsx')

#Looking at the date after rectifying it
sales_cells |> 
  unpivotr::rectify()

```

### Eliminating unwanted labels

We want to eliminate information in the first column dealing with City and Department. To do so we use the filter command `filter(col > 1)` eliminates `1(A)` column displaying the labels City and Department and then a bunch of NAs

```{r beheading_multiple_formats_2}
#| echo: true
#| warning: false
#| code-fold: true

sales_cells |> 
#Selecting after column number 2
  #filter(col > 1) |> 
  unpivotr::rectify()

sales_cells |> 
#Eliminating `1(A) column`
  filter(col > 1) |> 
  unpivotr::rectify()

```

### Cleaning city rows

Using up-left again mode. The next row under 2(B) displays the sales department, groceries. We are left with the data observations displaying customer number, name, gender, etc.

Using `filter(col > 1)` we make certain that the rectified file

```{r beheading_multiple_formats_3}
#| echo: true
#| warning: false
#| code-fold: true

sales_cells |> 
  filter(col > 1) |> 
   unpivotr::behead(
     'up-left',
     name = 'city') |>
  unpivotr::behead(
     'up-left',
     name = 'department') |>
  unpivotr::behead(
     'up',
     name = 'variable') |>
    unpivotr::rectify()

```

### Beheading variables created

The variables created are city, department store and variable.

#### Beheading city

After rectifying the data, we see that Chicago is only at the left most corner of the data. Notice that left to Chicago there are many columns with NAs. After them, the next city appears since the cells were merged across cities.

```{r behading_city_variable}
#| echo: true
#| warning: false
#| code-fold: true

#Notice that Chicago located at the left most corner of the data
sales_cells |> 
  filter(col > 1) |> 
  # unpivotr::behead(direction = 'up-left', name = 'city') |> 
  unpivotr::rectify()

```

Beheading Chicago as the `up_left` corner of the dataset

```{r behading_city_variable_direction_up_left_corner}
#| echo: true
#| warning: false
#| code-fold: true

sales_cells |> 
  filter(col > 1) |> 
  unpivotr::behead(direction = 'up-left', name = 'city') |> 
  unpivotr::rectify()

#* Notice department variable is now at the up top left of the
#* rectified file

```

Notice that after beheading Chicago, the value Grocery of the department variable, appears at the up left position of the data set. Departments are organized by Groceries, Outdors, etc.

#### Beheading department

Next step is to behead department

```{r beheading_department}
#| echo: true
#| warning: false
#| code-fold: true

sales_cells |> 
  filter(col > 1) |> 
   unpivotr::behead(
     'up-left',
     name = 'city') |>
  unpivotr::behead(
     'up-left',
     name = 'department') |> 
  unpivotr::rectify()

#Notice that after beheading department
```

And finally we can use all the names appearing in the first row, namely, productID, price,..sex, ..jobtitle, price,..., and collapse them into something we call variable.

```{r beheading_miscellaneous_into_variable}
#| echo: true
#| warning: false
#| code-fold: true

sales_cells |> 
  filter(col > 1) |> 
   unpivotr::behead(
     'up-left',
     name = 'city') |>
  unpivotr::behead(
     'up-left',
     name = 'department') |>
  unpivotr::behead(
     'up',
     name = 'variable') |> 
  unpivotr::rectify()

```

#### Beheaded sales cells dataset

Creating the new file whereby we select the variables we created; namely, city, department and variable. However, we need to add other variables that were present in the original database. They were coded as characters. Those are the values originally listed under productID, price, full name, sex, job title. Most of them are characters while the values under price are numeric. The row label would also help us to identify things, so we could include it in the variable selection.

```{r beheaded_sales_cells_df}
#| echo: true
#| warning: false
#| code-fold: true

beheaded_sales_cells <- sales_cells |> 
  filter(col > 1) |> 
   unpivotr::behead(
     'up-left',
     name = 'city') |>
  unpivotr::behead(
     'up-left',
     name = 'department') |>
  unpivotr::behead(
     'up',
     name = 'variable') |> 
 # unpivotr::rectify()
  select(city:variable, character, numeric, row)

```

### Further cleaning the file

We want to have the character and numeric columns in one single column using the function `coalesce()`. Given a set of vectors, `coalesce()` finds the first non-missing value at each position. It's inspired by the SQL `COALESCE` function which does the same thing for SQL `NULL`s. **Note:** We need to *temporarily convert* the *numeric variable* *as* *character* otherwise coalesce won't work. Below, it shows we get an error message by not properly handing the variable numeric

`beheaded_sales_cells |>  mutate( using coalesce cell_values = coalesce(character,numeric))`

"Error in `mutate()`: ℹ In argument: `cell_values = coalesce(character, numeric)`. Caused by error in `coalesce()`: ! Can't combine `..1` <character> and `..2` <double>. Run `rlang::last_trace()` to see where the error occurred."

After the merging or coalescing of character and numeric columns into cell values, we won't need the two columns.

Notice that coalesce merges the columns `character` with the column `numeric`. The end result is the variable or column `cell_values`, with a mixture of characters and numbers from the two variables `numeric` and `character`. Notice that while coalescing the two columns using mutate, we declare the variable numeric as a character (`as.character(numeric))`.

```{r cleaning_the_file_behaded}
#| echo: true
#| warning: false
#| code-fold: true

beheaded_sales_cells |> 
  mutate(
    #Notice we are merging chacter and numeric columns using coalesce
    cell_values = coalesce(character, as.character(numeric))
  ) |> 
  select(-c(character,numeric))
```

#### Wider transforming

Now we can transform the database into a wider format. Take the names of the column `variable` and the values from the `cell_values` column. The rest of the variables, `city` and `department`, would serve as unique identifier.

```{r widening_behaded_file}
#| echo: true
#| warning: false
#| code-fold: true

beheaded_sales_cells |> 
  mutate(
    cell_values = coalesce(character, as.character(numeric))
  ) |> 
  select(-c(character,numeric)) |> 
  pivot_wider(
    names_from = 'variable',
    values_from = 'cell_values'
  ) |> 
  #Getting rid of the row column
  select(-row) |> 
  #Recoding price as numeric
  mutate(price = parse_number(price))


```

## Lesson 20 Beheading colored cells

In the previous example the data were well organized. Country and Region had their our column, while gender and age had their own row. But the data set below is organized in a subheading manner; this is to say Region information is listed under Country column. The only thing that helps to distinguish country from region is the use of blue color for the row Country.

```{r behading_colored_cells_1}
#| echo: true
#| warning: false
#| code-fold: true

rand_cells <- tidyxl::xlsx_cells('/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/18_rand_numbers_harder.xlsx')

#Passing rand_cells to unpivotr & rectify it

rand_cells |> 
  unpivotr::rectify()


```

After rectify it, the file shows that Region and Country are in the same column labeled `3(C)` . We can proceed by telling `behade()`function to treat Regions as missing values under specific conditions. For that we need to add another `behead_if()` to specify those conditions. We can create a logical column to aid in the selection of the variable.

Notice that after specifying the behead_condition, the characters of Country are recoded as NAs in the column `3(C).`

```{r behading_colored_cells_2}
#| echo: true
#| warning: false
#| code-fold: true

#Creating a logical condition to aid in beheading

behead_condition <- str_detect(
  rand_cells$character,	'Country')

rand_cells |> 
  #Adding special conditions to treat Regions as NAs
  unpivotr::behead_if(
    behead_condition,
    direction = 'left-up',
    name = "country"
  ) |> 
  unpivotr::rectify()



```

Next we could take the code we had in a previous example

```{r behading_colored_cells_3}
#| echo: true
#| warning: false
#| code-fold: true

behead_condition <- str_detect(
  rand_cells$character,	'Country')

rand_cells |> 
  #Adding special conditions to treat Regions as NAs
  unpivotr::behead_if(
    behead_condition,
    direction = 'left-up',
    name = "country"
  ) |> 
  unpivotr::behead('left',
                   name = 'region'
  ) |>
  unpivotr::behead('up-left',
                   name = 'gender'
  ) |>
  unpivotr::behead('up',
                   name = 'age'
  ) |> 
  select(country:age,numeric)

#We can also address the missing values

rand_cells |> 
  #Adding special conditions to treat Regions as NAs
  unpivotr::behead_if(
    behead_condition,
    direction = 'left-up',
    name = "country"
  ) |> 
  unpivotr::behead('left',
                   name = 'region'
  ) |>
  unpivotr::behead('up-left',
                   name = 'gender'
  ) |>
  unpivotr::behead('up',
                   name = 'age'
  ) |> 
  select(country:age,numeric) |> 
  #Replacing NA for the label Total in region column
  mutate(
    region = if_else(is.na(region), 'Total', region)
  )


```

This approach works if we have the excel file `18_rand_numbers_harder.xlsx` neatly organized. We can do so by relying on the cell colors as identifier. For that we can recalculate the behead_condition based on the background colors used in the excel file. For that we use the excel format `xlsx_format()` function. Executing this it reveals the hidden features using glimpse as well.

Notice glimpse reveals there is an entry `$fill` which can tell us the color. We can pluck the value.

```{r revealing_color_codes}
#| echo: true
#| warning: false
#| code-fold: true

tidyxl::xlsx_formats(
  '/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/18_rand_numbers_harder.xlsx') |> 
  pluck('local','fill') |> 
  glimpse()

```

The pluck function along glimpse reveals that the fill has four colors: "FFDEE7E5", "FFDEE6EF", "FFDEE7E5" and "FFDEE6EF". Next we can provide the address as to where to find the colors.

```{r plucking_4_colors_rows}
#| echo: true
#| warning: false
#| code-fold: true

#Plucking the 4 colors used in the excel file

#Creating a vector of background colors

bg_colors <- tidyxl::xlsx_formats(
  '/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/18_rand_numbers_harder.xlsx') |> 
  pluck('local','fill','patternFill','fgColor', 'rgb') |> 
  glimpse()

```

Notice Albert was able to identify the color code using excel fill color. I consulted [YouTube](https://www.youtube.com/watch?v=iCB-IV6SVOQ) on identifying RGB and HEX color in EXCEL for Mac users. In this case, the first color was `DEE6EF`.

In our original `rand_cells` dataframe we have a column labeled `local_format_id`. If we look at rand_cells\$local_format_id we discover a bunch of numbers, most of it with the number 8.

If we look at `bg_colors` vector, we find 8 entries. We can find the color each cell has by just going into bg_colors vector, and then using `rand_cells' local_format_id` as the index as follows:

`bg_colors[rand_cells$local_format_id]`

It gives us a whole bunch of colors and missing values.

Next we want to check in which of those cells can I find this string `DEE6EF`

Then we can do this our next behead condition. This option works as well as the previous version of beheading, but in this occasion we have made the condition dependent of the color of the cell.

```{r background_colors}
#| echo: true
#| warning: false
#| code-fold: true

#Depicting the background colors
bg_colors[rand_cells$local_format_id]

#Checking the cell location of the string DEE6EF

str_detect(
  bg_colors[rand_cells$local_format_id], 'DEE6EF'
)

#Making this string our new behead condition

behead_condition <- str_detect(
  bg_colors[rand_cells$local_format_id], 'DEE6EF'
)

rand_cells |> 
  #Adding new behead condition to detect DEE6EF color
  unpivotr::behead_if(
    behead_condition,
    direction = 'left-up',
    name = "country"
  ) |> 
  unpivotr::behead('left',
                   name = 'region'
  ) |>
  unpivotr::behead('up-left',
                   name = 'gender'
  ) |>
  unpivotr::behead('up',
                   name = 'age'
  ) |> 
  select(country:age,numeric) |> 
  #Replacing NA for the label Total in region column
  mutate(
    region = if_else(is.na(region), 'Total', region)
  )


```

## Lesson 21 Putting it together

Albert put together three data sets to clean. The first two files are excel. The third file is an csv file. The first two data sets describe the location of the warehouse, the product and the supplier id. The third data set provides information about the supplier itself (id, name, contact person, email, contract dates, and supplier rating. All three data sets are interconnected.

First step is to clean the each of the three data sets. Then, we would conduct some data analysis.

### First data set

#### Creating a tibble file

Loading the data set, and transform it as tibble. Doing so it duplicates the variable labels or headers.

```{r first_data_clean_1}
#| echo: true
#| warning: false
#| code-fold: true

openxlsx::read.xlsx(
  '/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/19a_Shipments_Details_mod.xlsx') |> 
  as_tibble()

```

#### Eliminating unwanted header

The headers added by tibble are not needed. We can use the r`ead.xlsx` function `startRow` = 2. It has the same effect as the csv function skip = 2. Next we can use janitor to clean names. We can also use skimr library `skim()` function to request summary statistics.

```{r removing_first_header}
#| echo: true
#| warning: false
#| code-fold: true

openxlsx::read.xlsx(
  '/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/19a_Shipments_Details_mod.xlsx',
  startRow = 2) |> 
  as_tibble() |> 
  janitor::clean_names() |>
  skimr::skim()

```

skim reveals that inside the numeric part we have all of the dates (e.g., shipment, expected delivery, actual delivery and weight). However, all dates were transformed into numbers. They need to be re-coded as dates.

We can the across function to recode them as dates using the lubridate function as_date. We could also use janitor's function excel_numeric_to_date().

```{r recoding_numbers_as_dates}
#| echo: true
#| warning: false
#| code-fold: true

# Using lubridate::as_date() function
openxlsx::read.xlsx(
  '/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/19a_Shipments_Details_mod.xlsx',
  startRow = 2) |> 
  as_tibble() |> 
  janitor::clean_names() |> 
  mutate(
    across(.cols = contains('date'),
           function(x) lubridate::as_date(x))
  )

#* Another option to transform numeric into date is using 
#* janitor::excel_numeric_to_date()

openxlsx::read.xlsx(
  '/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/19a_Shipments_Details_mod.xlsx',
  startRow = 2) |> 
  as_tibble() |> 
  janitor::clean_names() |> 
  mutate(
    across(.cols = contains('date'),
           function(x) janitor::excel_numeric_to_date(x))
  ) |> 
  skimr::skim()

#* Option followed by Albert Rapp

openxlsx::read.xlsx(
  '/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/19a_Shipments_Details_mod.xlsx',
  startRow = 2) |> 
  as_tibble() |> 
  janitor::clean_names() |> 
  mutate( 
    across(contains('date'),
           janitor::excel_numeric_to_date))


```

#### days_shipped_to_delivered

Next we create a new column labeled day_shipped_to_delivered, which is going to be an interval. For this we use `lubridate` . Once we have an interval, we can divide it by an interval of length 1, or one day. We rely on the `days()` function `/days(1)`.

Using `skim()` reveals that the mean of days_shipped_to_delivered is 19.5 days, ranging from 5 to 36 days.

```{r interval_dates_to_delivered}
#| echo: true
#| warning: false
#| code-fold: true

library(lubridate)

openxlsx::read.xlsx(
  '/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/19a_Shipments_Details_mod.xlsx',
  startRow = 2) |> 
  as_tibble() |> 
  janitor::clean_names() |> 
  mutate(
    across(.cols = contains('date'),
           function(x) as_date(x)),
    dates_shipped_to_delivered = interval(
      start = shipment_date,
      end =  actual_delivery_date) / days(x = 1)
  ) |> 
  skimr::skim()

```

#### Using expected delivery date

We can estimate the difference between expected delivery date and the actual one.

The average days expected to actual delivery is 1.41, and the minimum is to expect the package to be delivered 3 days before the promised delivery date. The highest delay is 7 days.

```{r days_expectedt_to_actual}
#| echo: true
#| warning: false
#| code-fold: true

library(lubridate)

openxlsx::read.xlsx(
  '/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/19a_Shipments_Details_mod.xlsx',
  startRow = 2) |> 
  as_tibble() |> 
  janitor::clean_names() |> 
  mutate(
    across(.cols = contains('date'),
           function(x) as_date(x)),
    days_shipped_to_delivered = interval(
      start = shipment_date,
      end =  actual_delivery_date) / days(x = 1),
    days_expected_to_actual = interval(
      start = expected_delivery_date,
      end =  actual_delivery_date) / days(x = 1)
  ) |> 
  skimr::skim()

```

#### Additional calculations

Calculating mean delivery days and expected days to delivered by country of origin. We can save the resulting table

```{r deliveries_by_country}
#| echo: true
#| warning: false
#| code-fold: true

shipment_test <- shipments <- openxlsx::read.xlsx(
  '/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/19a_Shipments_Details_mod.xlsx',
  startRow = 2) |> 
  as_tibble() |> 
  janitor::clean_names() |> 
  mutate(
    across(.cols = contains('date'),
           function(x) as_date(x)),
    days_shipped_to_delivered = interval(
      start = shipment_date,
      end =  actual_delivery_date) / days(x = 1),
    days_expected_to_actual = interval(
      start = expected_delivery_date,
      end =  actual_delivery_date) / days(x = 1)
  )

shipments |> 
  summarise(
    mean_delivery_days = mean(
      days_shipped_to_delivered, na.rm = TRUE),
    mean_expected_days = mean(
      days_expected_to_actual, na.rm = TRUE
    ),
    .by = origin_country
  )

mean_delays_by_country <- shipments |> 
  summarise(
    mean_delivery_days = mean(
      days_shipped_to_delivered, na.rm = TRUE),
    mean_expected_days = mean(
      days_expected_to_actual, na.rm = TRUE
    ),
    .by = origin_country
  )

```

### Second data set

#### *Dealing with duplicated id using .name_repair*

The first time the first script below runs, we get a data set instead of a tibble. As before, in this script we create a tibble, clean the variable names and eliminate the first row. Notice we have two variables with the same name of id. We get the following error message:

**Error in `as_tibble()`: ! Column name `ID` must not be duplicated. Use `.name_repair` to specify repair.**

We could start eliminating the first row. However, we get an error message due to the fact two variables are labeled ID (locations 1 and 3 columns). However, error message suggests using tibble's `.name_repair =` function.

We can create a custom function to repair the names of the variables `name_repair` within the as_tibble function. Steps:

1.  Calculate the position of the column or name using `seq_along()`. Basically numbers each column in the database. For more information about seq_along() see [**seq_along in R: A Comprehensive Guide**](https://sqlpad.io/tutorial/seq_along-r-comprehensive-guide/)

2.  Next we can say that comes in the first two columns, Warehouse and ID. And then the things that are in the 3th and 6th columns. We can add a prefix

3.  If the case number is less or equal to 2, assign the prefix `warehouse_` to the variable and if the number is less than 7, assign the prefix \`product\_\`. In all cases, we don't use prefix.

4.  Given the prefix, we use the `paste0` function to affix the labels `warehouse_` and`product_`to concatenate things

5.  Then we can clean the names using janitor::clean_names() function

6.  Finally, we can transform date variables are dates

```{r second_data_function_approach_duplicate}
#| echo: true
#| warning: false
#| code-fold: true

#* This option generates error due to duplicated IDs

# openxlsx::read.xlsx(
#   'data/Part_2/19b_Warehouse_Inventory_mod.xlsx',
#   startRow = 2 ) |>
#   as_tibble()


#* Using .name_repair()
openxlsx::read.xlsx(
  '/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/19b_Warehouse_Inventory_mod.xlsx',
  startRow = 2) |>
  as_tibble(
    #Creating a function within .name_repair
    .name_repair = function(x) {
    #Sequencing the variables or counting them
      nmbr <-  seq_along(x)
      prefix <-  case_when(
        nmbr <= 2 ~ 'warehouse_',
        nmbr < 7 ~ 'product_',
        TRUE ~ "" #Assign no prefix. Keep variable name as is
      )
      #Putting prefix and variable together
      paste0(prefix,x)
    }
  ) |> 
  janitor::clean_names() |> 
  #Transforming character variable into date
  mutate(
    product_last_restocked_date = 
      as_date(product_last_restocked_date)
  )

```

#### *Dealing with duplicated id using beheading style*

Using `tidyxl` library with the `xlsx_cells()` function instead of openxlsx library. Procedure:

1.  Get rid of the first row that only has Warehouse and Product

2.  We behead the first row and end up with a column labeled `variable_type` after eliminating the label Warehouse. Using unpivotr reveals that the content of `variable_type` is the label `Warehouse`.

3.  Using beheading using up moves ID and Location labels under the label variable. Last column

```{r second_data_beheading_style}
#| echo: true
#| warning: false
#| code-fold: true

warehouse_cells <- tidyxl::xlsx_cells(
  '/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/19b_Warehouse_Inventory_mod.xlsx')

warehouse_cells |> 
  unpivotr::rectify()

#* Creating variable_type which contains the labels 
#* Warehouse and Product via up-left
warehouse_cells |> 
  unpivotr::behead(
    direction = 'up-left',
    name = 'variable_type'
  ) |> 
  unpivotr::rectify()

#Now we are beheading at the up mode

warehouse_cells |> 
  unpivotr::behead(
    direction = 'up-left',
    name = 'variable_type'
  ) |> 
  unpivotr::behead(
    direction = 'up',
    'variable'
  ) |> 
  unpivotr::rectify()

#* Time to select the variables we need

warehouse_cells |> 
  unpivotr::behead(
    direction = 'up-left',
    name = 'variable_type'
  ) |> 
  unpivotr::behead(
    direction = 'up',
    'variable'
  ) |> 
  select(
    variable,
    variable_type,
    character,
    numeric,
    date,
    row
  )

#We need to stich things together again

warehouse_cells |> 
  unpivotr::behead(
    direction = 'up-left',
    name = 'variable_type'
  ) |> 
  unpivotr::behead(
    direction = 'up',
    'variable'
  ) |> 
  select(
    variable,
    variable_type,
    character,
    numeric,
    date,
    row
  ) |> 
#* We need to coalesce or stich things together the columms. In this case
#* we are coalescing 3 variables or columns: 1)character, 2)numeric, 3)date
#* or variables character and date
#* Notice we are declaring date as numeric
  mutate(
    cell_value = coalesce(
      character,
      as.character(numeric),
      as.character(date)
    ),
    variable_id = paste0(
      variable_type,
      '_',
      variable
    )
  ) |> 
  select(variable_id,cell_value,row ) |> 
  pivot_wider(
    names_from = variable_id,
    values_from = cell_value
    ) |> 
  select(-row) |> 
  janitor::clean_names() |> 
  mutate(
    product_last_restocked_date = ymd(
      product_last_restocked_date),
    product_stock_level = as.numeric(
      product_stock_level)
    ) |> 
  skimr::skim()

#Saving the data

warehouses <- warehouse_cells |> 
  unpivotr::behead(
    direction = 'up-left',
    name = 'variable_type'
  ) |> 
  unpivotr::behead(
    direction = 'up',
    'variable'
  ) |> 
  select(
    variable,
    variable_type,
    character,
    numeric,
    date,
    row
  ) |> 
  mutate(
    cell_value = coalesce(
      character,
      as.character(numeric),
      as.character(date)
    ),
    variable_id = paste0(
      variable_type,
      '_',
      variable
    )
  ) |> 
  select(variable_id,cell_value,row ) |> 
  pivot_wider(
    names_from = variable_id,
    values_from = cell_value
    ) |> 
  select(-row) |> 
  janitor::clean_names() |> 
  mutate(
    product_last_restocked_date = ymd(
      product_last_restocked_date),
    product_stock_level = as.numeric(
      product_stock_level)
    )

```

### Third data set

#### Suppliers

Moving to the supplier information dataset. While the dataset was loaded without a problem, the column names need to be cleaned.

1.  Cleaning variable names using janitor

2.  Saving cleaned file

3.  Do a subset of the suppliers dataset with the supplier id as well as supplier rating.

4.  Notice that the supplier id is very similar to the product_supplier_id in the warehouses dataset. Consequently, we could rename it as supplier_id

```{r third_data_suppliers}
#| echo: true
#| warning: false
#| code-fold: true

suppliers <- read_csv(
  '/Users/albertocabrera/Documents/GitHub/r_data_cleanning_master_class/data/Part_2/19c_Suppliers_Info.csv') |> 
  janitor::clean_names()

#* Subsetting suppliers dataset to include supplier id
#* and supplier rating

suppliers |> 
  select(supplier_id, supplier_rating)

# Renaming product_supplier_id in warehouses dataset 
warehouses <- warehouses |> 
  rename(supplier_id = product_supplier_id)

```

#### Summary information

Merging warehouses and suppliers rating dataset. Notice we are selecting our smaller subset of suppliers dataset.

Next we can do some data wrangling requesting summary information

```{r merging_ratings_supplies}
#| echo: true
#| warning: false
#| code-fold: true

warehouses |> 
  left_join(
    suppliers |> 
  select(supplier_id, supplier_rating),
  by = 'supplier_id'
  ) |>
  summarise(
    mean_supplier_rating = mean(supplier_rating, na.rm = TRUE),
    n = n(),
    .by = warehouse_location
  )


```

We can also include the supplier country and warehouse location as part of the summary of ratings. Next, we create a mean ratings by location table

```{r merging_ratings_supplies_country}
#| echo: true
#| warning: false
#| code-fold: true

warehouses |> 
  left_join(
    suppliers |> 
  select(supplier_id, supplier_rating, supplier_country),
  by = 'supplier_id'
  ) |>
  summarise(
    mean_supplier_rating = mean(supplier_rating, na.rm = TRUE),
    n = n(),
    .by = c(warehouse_location, supplier_country)
  )

#Creating a mean ratings by location table

mean_ratings_by_location <- warehouses |> 
  left_join(
    suppliers |> 
  select(supplier_id, supplier_rating, supplier_country),
  by = 'supplier_id'
  ) |>
  summarise(
    mean_supplier_rating = mean(supplier_rating, na.rm = TRUE),
    n = n(),
    .by = c(warehouse_location, supplier_country)
  )


```

#### Merging & visualizing

Since we have the countries now, we could look at our shipments dataset. In this dataset we have country as well. So it would be possible to examine if the ratings are associated with delivery delays. We create a new merged dataset labeled join_dat

Next we can visualize the association between supplier average ratings and average delay in package deliveries. And displaying what country each point belongs to.

```{r merging_shipments_with_mean_country_delays }
#| echo: true
#| warning: false
#| code-fold: true

join_dat <- mean_delays_by_country |> 
  left_join(
    mean_ratings_by_location,
#Notice the key in the 2 data sets have different names
    by = join_by('origin_country' == 'supplier_country')
  )

join_dat |> 
  rename(mean_delay_days = mean_expected_days) |> 
  ggplot(aes(x = mean_supplier_rating,
              y = mean_delay_days)) +
  geom_point(size = 5) +
  geom_text(
    aes(label = origin_country),
    size = 5,
    hjust = 0,
    vjust = 0,
    nudge_x = 0.02
  ) +
  theme_minimal(
    base_size = 18,
    base_family = 'Source Sans Pro'
  ) +
  labs(x = "Mean supplier rating",
       y = "Mean delay in days")


```

#### Visualizing shipments with ratings

Notice that points are over one another requiring the use of jitter

```{r another_visualization}
#| echo: true
#| warning: false
#| code-fold: true

shipments |> 
  left_join(
    mean_ratings_by_location,
    by = join_by('origin_country' == 'supplier_country')
  ) |> 
  ggplot(aes(x = mean_supplier_rating,
              y = days_expected_to_actual)) +
  geom_violin(aes(group = origin_country)) +
  geom_jitter(size = 3, alpha = 0.5) +
  theme_minimal(
    base_size = 18,
    base_family = 'Source Sans Pro'
  ) +
  labs(x = "Mean supplier rating",
       y = "Delay in days")


```
