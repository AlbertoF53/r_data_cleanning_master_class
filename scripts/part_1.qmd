---
title: "Part 1"
author: "Albert Rapp"
date: June 19, 2024
format:
  html:
    code-link: true
    code-fold: true
    self-contained: true
    embed-resources: true
    toc: true
    theme: Zephyr
    number-sections: false
execute: 
  warning: false
  message: false
  echo: false
editor_options: 
  chunk_output_type: console
---

# Part I

## Lesson 1

Learning about the mechanics of wrangling data. Particularly `mutate()` and `summarise()` functions in an efficient manner of the databases to be used: penguins and ames.

```{r libraries_data}
#| echo: true

library(palmerpenguins)
library(tidyverse)
library(modeldata)
library(janitor)
library(skimr)
library(e1071)

ames <- ames
penguins <- penguins

```

The dimensions of the penguins database: 344 observations and 7 columns or variables

```{r penguins_dimensions}
#| echo: true

dim(penguins)
```

To have a good overview of the database, we can use the `skim()` function from the skimr package. It provides

```{r skimr_penguins}
#| echo: true

palmerpenguins::penguins |> 
  skimr::skim()

```

We can apply the skim function to the ames database

```{r skimr_ames}
#| echo: true

modeldata::ames |> 
  skimr::skim()

```

If you want to focus on one variable, you can deposit it into the skim() function. For instance, examining the properties of Lot Frontage and Lot_Area

```{r skim_Lot_Frontage}
#| echo: true

modeldata::ames |> 
  skimr::skim(Lot_Frontage,Lot_Area)

```

Notice the ames data has not nice variable names. Good idea to use janitor::clean_names to convert capital letters into lower letters.

```{r janitoring_ames}
#| echo: true

modeldata::ames |> 
  janitor::clean_names() |> 
  skimr::skim(lot_frontage,lot_area)

```

## Lesson 2

The focus is the `mutate()` function. It also deals with vectorized vs iterative calculations with mutate. It introduces the `map()` function. This function aligns a function to a vector, compelling the estimating of one-by-one instead of vector mashing. The variations of map() discussed include the `map_lgl()` and the `map_dbl()`. The first one produces a logical vector. The second produces numeric values of 1 (True) and 0 (False).

### Handling missing cases

It is important to handle missing values before doing mutate(). We can verify missing cases were removed using skimr.

```{r missing_values_penguins}
#| echo: true

library(palmerpenguins)
library(tidyverse)
library(modeldata)
library(janitor)

penguins_wo_NAs <-  palmerpenguins::penguins |> 
  filter(!is.na(sex))

skimr::skim(penguins_wo_NAs)


```

### Handling vectors

Using mutate to create a new variable. Next use select() to display it. Mutate is a vectorised function. This means that it takes the two vectors listed in the mutate function as a whole. In other words the values of each column are not taken one-by-one. And then they are operated via a vectorized division (/).

Imagine these two vectors are "smashed together" and the result is divided.

```{r mutate_penguins_basic}
#| echo: true

penguins_wo_NAs |> 
  mutate(
    bill_flipper_ratio = bill_length_mm / bill_depth_mm, na.rm = TRUE
  ) |> 
  select(bill_length_mm, bill_depth_mm, bill_flipper_ratio)

```

Example where the vectorized condition may not be met. Using a custom function to illustrate this problem. Next let's apply the function to one of the vectors used in estimating bill_flipper_ratio. In this case, bill_length_mm.

Notice that the application of the function prompted an error message. It was traced back to the `if (x > 39) ...:` The result shows the condition has `length > 1`.

```{r vector_principle}
#| echo: true

# large_quantity <-  function(x) {
#   if (x > 39) {
#     return(TRUE)
#   } else {
#     return(FALSE)
#   }
# }

# penguins_wo_NAs |>
#   mutate(
#     large_bill_length = large_quantity(bill_length_mm)
#   ) |> 
#   select(bill_length_mm,large_bill_length)
# 
# #Reconstructing the essence of the function
# c(34, 234) > 39


```

### Functional programming or `map()`

#### map_lgl() enforces the one-by-one calculation instead of vector calculation

To address the limitation of working with vectors, we need `functional programming`. `map()` allows one-to-one calculation called upon by the `large_quantity` function within the mutate() function.

In other words, instead of using the vector smashing approach to handle observations contained in the variable.

We rely on the `map()` function. In this case the logical map function ( `map_lgl()`). And then we instruct R to apply the `large_quantity()` function to each component, one-by-one, of the column or vector **bill_lenght_mm**. Notice, map_lgl() lists the vector or column first, and then the function to be applied to it. In other words, map_lgl() applies the function to each element of the vector or column instead of smashing the vector.

"The map functions transform their input by applying a function to each element of a list or atomic vector and returning an object of the same length as the input."

```{r functional_programming_penguins}
#| echo: true

large_quantity <-  function(x) {
  if (x > 39) {
    return(TRUE)
  } else {
    return(FALSE)
  }
}

penguins_wo_NAs |>
  mutate(
    large_bill_length = map_lgl(
      bill_length_mm,
      large_quantity) ) |> 
  select(bill_length_mm,large_bill_length)


```

Another variation is the `map()` itself. But instead of producing individual values, map() produces lists of values.

```{r map_list}

large_quantity <-  function(x) {
  if (x > 39) {
    return(TRUE)
  } else {
    return(FALSE)
  }
}

test <- penguins_wo_NAs |>
  mutate(
    large_bill_length = map(
      bill_length_mm,
      large_quantity) ) |> 
  select(bill_length_mm,large_bill_length)

test
```

Still another variation of the `map()` function, `the map_dbl()`. It reports numbers instead of logical statements. Notice, we list first the column to be modified followed by the function (`large_quantity`).

```{r map_numbers}
#| echo: true

large_quantity <-  function(x) {
  if (x > 39) {
    return(TRUE)
  } else {
    return(FALSE)
  }
}

test <- penguins_wo_NAs |>
  mutate(
    large_bill_length = map_dbl(
      bill_length_mm,
      large_quantity) ) |> 
  select(bill_length_mm,large_bill_length)

test

```

## Lesson 3

This section cover the `summarise()`function. This function is the opposite of the `mutate()` function. It also works with vectors, but the difference is the amount of output. While mutate can be used to produce vectors, summarise turns out single elements.

```{r summarise_function}
#| echo: true

penguins_wo_NAs |> 
  summarise(
    mean_flipper_length = mean(flipper_length_mm, na.rm = TRUE),
    sd_flipper_length = sd(flipper_length_mm, na.rm = TRUE) )

```

You can also save the lengths into one single list. In other words, the last column, labeled `flipper_lenghts()` contains a vector of values. This last column has 333 values corresponding to 333 penguins' flipper lengths.

```{r listing_summary_results}
#| echo: true

test <- penguins_wo_NAs |> 
  summarise(
    mean_flipper_length = mean(flipper_length_mm, na.rm = TRUE),
    sd_flipper_length = sd(flipper_length_mm, na.rm = TRUE),
    flipper_lengths = list(flipper_length_mm))

test

```

## Lesson 4

Handling missing values with common calculations. If not handled, the summarise function would generate NAs. One option is to use `na.rm = TRUE.`

```{r handling_nas}

palmerpenguins::penguins |> 
  mutate(
    bill_flipper_ratio = bill_length_mm / bill_depth_mm,
    mean_column = mean(bill_length_mm, na.rm = TRUE)
  ) |> 
  select(bill_length_mm, bill_depth_mm, bill_flipper_ratio, mean_column)


palmerpenguins::penguins |> 
  summarise(
    mean_flipper_length = mean(flipper_length_mm, na.rm = TRUE),
    sd_flipper_length = sd(flipper_length_mm, na.rm = TRUE),
    flipper_lengths = list(flipper_length_mm))

```

## Lesson 5

Repeated calculations for subsets of data. Two options to automate repeated calculations. `for()` loop. Another is to rely on loop calculations. An example is to relay on the data without missing values.

One option is to rely on the option .by = () to specify subgroups. Be careful about omitting the "." It would produce an output but creating a new variable labeled species.

```{r repeated_calc}
#| echo: true

penguins_wo_NAs |> 
  summarise(
    mean_flipper_length = mean(flipper_length_mm, na.rm = TRUE),
    sd_flipper_length = sd(flipper_length_mm, na.rm = TRUE),
    flipper_lengths = list(flipper_length_mm),
    .by = species)


```

Warning regarding grouping in a series of concatenated computations as show below. group_by() affects the original structure of the data by the first variable in listed in the `group_by()` function.

Notice the output indicates `#Groups : species[3]` Or 3 species by 5 islands. Adding another `summarise()` function generates a problem. Suppose we want to estimate the mean of the mean_flipper_length estimated in the previous run as well as the sd of the sd_flipper_lenght variable previously created. The end result is an sd_flipper_length with two NAs.

During the first calculation, `group_by()` strips the last group alluded in the function, namely island. The end result is a 5 X 5 tibble table organized by three groups corresponding to the 3 species. This is an opportunity to mistakes if one ignores the structure imposed on the data by group_by() function. Grouped calculations behave differently.

```{r grouping_danger}
#| echo: true

penguins_wo_NAs |> 
  group_by(species, island) |> 
  summarise(
    mean_flipper_length = mean(flipper_length_mm),
    sd_flipper_length = sd(flipper_length_mm),
    flipper_lengths = list(flipper_length_mm)) |> 
  summarise(
    mean_flipper_length = mean(mean_flipper_length),
    sd_flipper_length = sd(sd_flipper_length)
  )



```

To remove the underlying grouping structure, remove the grouping using the option .group = "drop' in the `summarise()` function. This option restores the original tibble table of 5 X 6.

Another option is to use `.by = c(species,island)` which restores the original data structure.

```{r eliminating_underscore_grouping}
#| echo: true

penguins_wo_NAs |> 
  group_by(species, island) |> 
  summarise(
    mean_flipper_length = mean(flipper_length_mm),
    sd_flipper_length = sd(flipper_length_mm),
    flipper_lengths = list(flipper_length_mm),
    .groups = 'drop')


penguins_wo_NAs |> 
  summarise(
    mean_flipper_length = mean(flipper_length_mm),
    sd_flipper_length = sd(flipper_length_mm),
    flipper_lengths = list(flipper_length_mm),
    .by = c(species, island))




```

## Lesson 6

### Unique function

Grouped calculations with mutate. In order to avoid a long list of means by species, use the `unique()` function.

```{r group_calculations}
#| echo: true

palmerpenguins::penguins |> 
  mutate(
    mean_column = mean(bill_length_mm, na.rm = TRUE),
    .by = species
  ) |> 
  select(species,mean_column) |> 
  unique()


```

### Centering a variable by its mean

A situation to use the scale function instead of reporting the means for each group as we did with mutate(). "`scale` is generic function whose default method centers and/or scales the columns of a numeric matrix."

Notice, we are centering each penguin's species bill length to their mean. Assuring transparency by requesting alpha = 0.5.

```{r group_calculations_summarise}
#| echo: true 

palmerpenguins::penguins |> 
  mutate(
    bill_length_mm = scale(bill_length_mm),
    .by = species) |> 
  ggplot() +
  geom_density(aes(x = bill_length_mm, fill = species),
               alpha = 0.5)

```

Another option to depict density distribution. Non centered

```{r depicting_density_non_centered}
#| echo: true

palmerpenguins::penguins |> 
  # mutate(
  #   bill_length_mm = scale(bill_length_mm),
  #   .by = species) |> 
  ggplot() +
  geom_density(aes(x = bill_length_mm, fill = species),
               alpha = 0.5)


```

## Lesson 7

### Missing data with group calculations

Notice not all combinations between species and islands are displayed. For instance Gentoe - Togerson, Chinstrap - Biscoe are not displayed.

```{r missing_data_grp_calc}
#| echo: true

penguins_wo_NAs |> 
  summarise(
    mean_flipper_length = mean(flipper_length_mm, na.rm = TRUE),
    sd_flipper_length = sd(flipper_length_mm, na.rm = TRUE),
    flipper_lengths = list(flipper_length_mm),
    .by = c(species,island) )

```

To display all combinations between species and islands, even for those combinations with no observations we could use the following `complete()` function.

`complete()` allows to display all combinations among variables with missing data. I also allows to change the default `NA` value for another, say 0.

```{r displaying_missing_comb}
#| echo: true

penguins_wo_NAs |> 
  summarise(
    mean_flipper_length = mean(flipper_length_mm, na.rm = TRUE),
    sd_flipper_length = sd(flipper_length_mm, na.rm = TRUE),
    flipper_lengths = list(flipper_length_mm),
    .by = c(species,island) ) |> 
  complete(
    species, island,
    fill = list(mean_flipper_length = 0,
                sd_flipper_length = 0)
  )

```

## Lesson 8

### Using across() + mutate() instead of repetitions.

Original mutate

```{r orginal_mutate}
#| echo: true

penguins_wo_NAs |> 
  summarise(
    mean_bill_depth = mean(bill_depth_mm),
    mean_bill_length = mean(bill_length_mm),
    mean_flipper_length = mean(flipper_length_mm),
    mean_body_mass = mean(body_mass_g))


```

#### Using 'across()\` function. Several variations.

Also using glue() to affix a name to the corresponding output

Notice the output does not display the prefix `mean_`. If you want it you need the option `.names` that it is a glue specification.

```{r across_variations_mutate}
#| echo: true

library(glue)

#Long option
penguins_wo_NAs |> 
  summarise(across(
    .cols = c("bill_length_mm",
              "bill_depth_mm",
              "flipper_length_mm",
              "body_mass_g"),
    .fns = mean,
    .names = 'mean_{.col}'
  ))

#Columns option
penguins_wo_NAs |> 
  summarise(across(.cols = 3:6, 
                   function(x) mean(x, na.rm = TRUE),
                   .names = 'mean_{.col}'))

#where(is.numeric)
penguins_wo_NAs |> 
  summarise(across(where(is.numeric), 
                   function(x) mean(x, na.rm = TRUE),
                   .names = 'mean_{.col}') )

#Selecting columns with specific names
penguins_wo_NAs |> 
  summarise(across(contains('bill'), 
                   function(x) mean(x, na.rm = TRUE),
                   .names = 'mean_{.col}') )

#Selecting columns with specific names
penguins_wo_NAs |> 
  summarise(across(starts_with('flipper'), 
                   function(x) mean(x, na.rm = TRUE),
                   .names = 'mean_{.col}') )

```

#### Removing unwanted strings

Note that we could remove unwanted strings such as `_mm` and `_g`.

```{r removing_unwanted_strings}
#| echo: true

penguins_wo_NAs |> 
  summarise(across(
    .cols = c("bill_length_mm",
              "bill_depth_mm",
              "flipper_length_mm",
              "body_mass_g"),
    .fns = mean,
    .names = 'mean_{.col |> str_remove("_mm") |> str_remove("_g")}'
  ))


```

Another option is to include the .fn in the names function. But unfortunately it did not do the trick because the prefix mean was removed. Instead a prexif `1_` was added .

```{r adding_fn_names_function}
#| echo: true

penguins_wo_NAs |> 
  summarise(across(
    .cols = c("bill_length_mm",
              "bill_depth_mm",
              "flipper_length_mm",
              "body_mass_g"),
    .fns = mean,
    .names = '{.fn}_{.col |> str_remove("_mm") |> str_remove("_g")}'
  ))


```

To make it work correctly, we need to use .fns = list(mean = mean).

```{r adding_fn_list_names_function}
#| echo: true

penguins_wo_NAs |> 
  summarise(across(
    .cols = c("bill_length_mm",
              "bill_depth_mm",
              "flipper_length_mm",
              "body_mass_g"),
    .fns = list(avg = mean),
    .names = '{.fn}_{.col |> str_remove("_mm") |> str_remove("_g")}'
  ))


```

You can compute more summary statistics

```{r extending_summary_statistice}
#| echo: true

penguins_wo_NAs |> 
  summarise(across(
    .cols = c("bill_length_mm",
              "bill_depth_mm",
              "flipper_length_mm",
              "body_mass_g"),
    .fns = list(avg = mean, standard_deviation = sd),
    .names = '{.fn}_{.col |> str_remove("_mm") |> str_remove("_g")}'
  ))


```

## Lesson 9

### Using across() + mutate( ) instead of repetitions

Using across() inside mutate() facilitates cleaning multiple columns in one single shot. In this example, we would be using across() along with the parse_number() to extract the first number inside a text.

We get a warning but the numbers were parsed or extracted from the strings. The end result is a tibble table with numbers instead of characters.

```{r across_mutate_option}
#| echo: true

tibble(
  text1 = c("I have 10 apples", "I have 20 appples"),
  text2 = c("I have 30 apples", "I have 40 apples")
) |> 
  mutate(
    across(
      .cols = everything(),
      .fns = parse_number
    )
  )

```

If you want to include both numbers and text, you need to add a new column as follows:

```{r across_mutate_more_columns}
#| echo: true

tibble(
  text1 = c("I have 10 apples", "I have 20 appples"),
  text2 = c("I have 30 apples", "I have 40 apples")
) |> 
  mutate(
    across(
      .cols = everything(),
      .fns = parse_number,
      .names = "number_{.col}"
    )
  )

```

## Lesson 10

### Adding calculations next to across()

Recapping a prior example below. The operations listed after summarize is just one of the calculations that could be further added

```{r adding_calc_across}
#| echo: true

penguins_wo_NAs |> 
  summarise(across(
    .cols = c("bill_length_mm",
              "bill_depth_mm",
              "flipper_length_mm",
              "body_mass_g"),
    .fns = mean,
    .names = 'mean_{.col |> str_remove("_mm")}'
  ))


```

Adding more operations to summarise(). Say you want to add a standard deviation, say sd for mean_bill_length. All what you need to do is to add a line.

```{r adding_more_calc_summarise}
#| echo: true

penguins_wo_NAs |> 
  summarise(across(
    .cols = c("bill_length_mm",
              "bill_depth_mm",
              "flipper_length_mm",
              "body_mass_g"),
    .fns = mean,
    .names = 'mean_{.col |> str_remove("_mm")}'
  ),
  sd_bill_length = sd(bill_length_mm)
  )

```

## Lesson 11

### Grouped calculations with across()

In the last lesson we learned that not only we can use across() for multiple computations, but also we can combine it with other analyses. This means we could do across calculations on a group basis as well. In short, we can perform powerful estimations in a concise manner.

```{r grouped_calculations_across}
#| echo: true

penguins_wo_NAs |> 
  summarise(across(
    .cols = c("bill_length_mm",
              "bill_depth_mm",
              "flipper_length_mm",
              "body_mass_g"),
    .fns = mean,
    .names = 'mean_{.col |> str_remove("_mm")}'
  ),
  sd_bill_length = sd(bill_length_mm),
  .by = c(species,island)
  )





```

An extension of this idea also apply to names =. See below

```{r extension_idea_str_remove}
#| echo: true

penguins_wo_NAs |> 
  summarise(across(
    .cols = c("bill_length_mm",
              "bill_depth_mm",
              "flipper_length_mm",
              "body_mass_g"),
    .fns = list(avg = mean, standard_deviation = sd),
    .names = '{.fn}_{.col |> str_remove("_mm") |> str_remove("_g")}'
  ),
  sd_bill_lenght = sd(bill_length_mm),
  .by = c(species,island))


```

## Lesson 12

### Getting to know The `reframe()` function

`reframe()` is a function sitting between mutate and summarize. This function has an arbitrary length between one, which is associated with `summarise()` and multiple, which is associated with `mutate()`.

The `range()` function is relevant. It tells you the minimum and maximum values of a column.

If we use range() within summarise() we get an error message, asking to use `reframe()` instead.

```{r reframe_function}
#| echo: true

range(penguins_wo_NAs$bill_length_mm)

penguins_wo_NAs |> 
  summarise(range_bill_lenght_mm = range(bill_length_mm))

#Using reframe() function instead of summarise()
penguins_wo_NAs |> 
  reframe(range_bill_lenght_mm = range(bill_length_mm))

```

Since `reframe()` along `range()` would return unnamed results, we could add `type()` for adding the labels to be added into the output.

```{r reframe_range_type}
#| echo: true

penguins_wo_NAs |> 
  reframe(
    type = c("min", "max"),
    range_bill_lenght_mm = range(bill_length_mm))



```

We can do more things with reframe(). We can use group statistics.

```{r reframe_flexibility}
#| echo: true

penguins_wo_NAs |> 
  reframe(
    type = c("min", "max"),
    range_bill_lenght_mm = range(bill_length_mm),
    .by = species)


```

We can also add across() and request range() within .fns =

```{r reframe_group_across}
#| echo: true

penguins_wo_NAs |> 
  reframe(
    type = c("min", "max"),
    across(.cols = c("bill_length_mm",
              "bill_depth_mm",
              "flipper_length_mm",
              "body_mass_g"),
           .fns = range
             ),
    .by = species)



```

But we could also format it as a table. To do, we need to call upon pivot \_longer and pivot_wider

```{r table_reframe}
#| echo: true

#Pivot longer
penguins_wo_NAs |> 
  reframe(
    type = c("min", "max"),
    across(.cols = c("bill_length_mm",
              "bill_depth_mm",
              "flipper_length_mm",
              "body_mass_g"),
           .fns = range
             ),
    .by = species) |> 
  pivot_longer(
    cols = -c(1:2),
    names_to = "quantity",
    values_to = 'value'
  ) |> 
  pivot_wider(
    id_cols = c(quantity),
    names_from = c(species,type),
    values_from = value
  )

```

## Lesson 13

Motivation for tidyselect helpers. They are powerful techniques. Allows one to select the correct columns or variables. For instance, the `.fns =` allows one to label the variables. Then this label is collected via the `.names =` function (see below)

```{r motivation_tidyselect}
#| echo: true

penguins_wo_NAs |> 
  summarise(across(
    .cols = c("bill_length_mm",
              "bill_depth_mm",
              "flipper_length_mm",
              "body_mass_g"),
    #Adding new labels
    .fns = list(avg = mean, standard_deviation = sd),
    .names = '{.fn}_{.col |> str_remove("_mm")}'
  ),
  .by = c(species,island))



```

Instead of listing the columns where the means and standard deviations are to be computed, we could use tidy helpers to facilitate locating them. The tidyselect to be used is `where`. It works in combination with another function, `is.numeric()`, that finds the desired property of the column or variable.

```{r using_tidyhelpers_columns}
#| echo: true

penguins_wo_NAs |> 
  summarise(across(
    .cols = where(is.numeric),
    .fns = list(avg = mean),
    .names = '{.fn}_{.col |> str_remove("_mm")}'
  ),
  .by = c(species,island))


```

Notice this omnibus statement includes the column year, which is categorical. In other to exclude this kind of variable from the omnibus statement we can add the `-`(minus) to exclude the categorical variable. Notice we need to encircle it within the concatenate function c( ). `c(where(is.numeric), -year)` concatenates the tidy helper while excluding the column year.

```{r where_is_numeric_but_no_categorical}
#| echo: true

penguins_wo_NAs |> 
  summarise(across(
    .cols = c(where(is.numeric), -year),
    .fns = list(avg = mean),
    .names = '{.fn}_{.col |> str_remove("_mm")}'
  ),
  .by = c(species,island))



```

## Lesson 14

Another custom function for `across()` function. Using the original penguins dataset. Performing the function, you will get missing data.

How to avoid these situations of ending up with `NA`s. We can use the mean function with `na.rm` option. The way to do so is to specify a new function that also specifies na.rm = TRUE. We can do so via an anonymous `function(x)`

Two options:

```{r across_other_function}
#| echo: true

#Option 1: removing NAs within the mean function
palmerpenguins::penguins |> 
  summarise(across(
    .cols = c(where(is.numeric), -year),
    .fns = function(x) mean(x, na.rm = TRUE),
    .names = 'mean_{.col |> str_remove("_mm")}'
  ),
  .by = c(species,island))

#Option 2: removing NAs in the function
palmerpenguins::penguins |> 
  summarise(across(
    .cols = c(where(is.numeric), -year),
    .fns = \(x) mean(x, na.rm = TRUE),
    .names = 'mean_{.col |> str_remove("_mm")}'
  ),
  .by = c(species,island))


#Option 3 creating a prefix for the new variables

palmerpenguins::penguins |> 
  summarise(across(
    .cols = c(where(is.numeric), -year),
    .fns = list(avg = function(x) mean(x, na.rm = TRUE)),
    .names = '{.fn}_{.col |> str_remove("_mm")}'
  ),
  .by = c(species,island))


```

## Lesson 15

Selecting the right data by column/variable types. Using select along with `where(is.numeric)` screens the database in search of variables that are numeric. We can eliminate as well variable with are not numeric.

The `where()` function can be used in several scenarios.

```{r right_data_column_type}
#| echo: true


penguins_wo_NAs |> 
  select(where(is.numeric), -year)

#* Using where() for selecting character variables. For penguins database the 
#* characters were defined as factor

penguins_wo_NAs |> 
  select(where(is.factor))


#* For selecting factor and character variables use the following

penguins_wo_NAs |> 
  select(where(is.factor), where(is.character))

```

Alternatively, you can define a custom function. Inside the `where()` function you can use the anonymous function. Combine the two with an or \|\|. Consequently the defined function is TRUE if the corresponding column/variable is a factor or a character. If numeric, it would yield a FALSE.

```{r custom_function_where}
#| echo: true

penguins_wo_NAs |> 
  select(where(function(x) is.factor(x) || is.character(x)) )



```

Defining custom function to identify numeric columns. And we could add another condition such as identifying the mean to be above a particular value.

So here we defined an anonymous function that checks whether a variable is numeric and where the mean of the variable is above 200.

The difference between the double && and the single & operator is that the single operator generates a warning message. The argument is not numeric or logical.

```{r custum_function_numeric}
#| echo: true

penguins_wo_NAs |> 
  select(where(function(x) is.numeric(x) && mean(x) > 200 ) )

```

Another handy option is to get columns where specific percentage of values are missing. In this case asking for variables with more than 1% of missing cases.

It usually practical to name this function for future use.

```{r anonymous_functions_for_per_na}
#| echo: true

palmerpenguins::penguins |> 
  select(where(function(x) mean(is.na(x)) > 0.01) )

#Creating function
more_than_one_percent_missing <- function(x) mean(is.na(x)) > 0.01

#Using function

palmerpenguins::penguins |> 
  select(where(more_than_one_percent_missing ))

```

## Lesson 16

Select the right data by column name

1.  Selecting consecutive columns. Between bill_length and sex column

2.  Selecting columns by specific variable's labels. Selecting variables ending with `_mm`

3.  Selecting columns with start with a particular string. Selecting variables starting with \`bill\`.

4.  Selecting columns that contain similar strings. Selecting variables with the string `lenght`

```{r select_right_data}
#| echo: true

#Selecting consecutive columns
penguins_wo_NAs |> 
  select(bill_length_mm:sex)

#Selecting columns ending with _mm
penguins_wo_NAs |> 
  select(ends_with('_mm'))

#Selecting columns starting with bill
penguins_wo_NAs |> 
  select(starts_with('bill'))

#Selecting columns the string lenght
penguins_wo_NAs |> 
  select(contains('length'))



```

Using regular expression to extend the gamma and scope of search for variables. For instance search for variables that start with the letters f or b, and are followed by any amount of characters `.*` and then end with the string `_mm`. Or `.*(_mm)$`

```{r select_regex}
#| echo: true

penguins_wo_NAs |> 
  select(matches("^[fb].*(_mm)$"))

```

Two more ways to select columns/variables based on a vector that contains the names. You can use this vector to select the columns or variables of interest. Two optional functions `any_of()` or `all_of()` functions to select the components of the vector.

```{r vector_names_any_of}
#| echo: true

my_cols <-  c("bill_lenght_mm", "bill_depth_mm")

# any_of
penguins_wo_NAs |> 
  select(any_of(my_cols))


# palmerpenguins::penguins |> 
#   select(all_of(my_cols))

# # all_of
# my_cols <-  c("bill_lenght_mm", "bill_depth_mm")
# penguins_wo_NAs |> 
#   select(all_of(my_cols))


```

## Lesson 17

Bringing it all together.

Using the housing dataset from the `ames` database from `modeldata` library.

```{r bringing_it_together}
#| echo: true

library(modeldata)
library(janitor)
library(skimr)
library(modeldata)

ames <- modeldata::ames |> 
  clean_names() |> 
  as_tibble()
  
#Examining dataset using skimr

ames |> 
  clean_names() |> 
  skim()

```

Focusing our attention to the numeric variables. We can also transform the variables a tibble

```{r ames_numeric_variables}
#| echo: true

library(modeldata)
library(janitor)
library(skimr)
library(modeldata)


ames |> 
  select(where(is.numeric)) |> 
  skim() |> 
  as_tibble()

ames_numeric <- ames |> 
  select(where(is.numeric)) |> 
  as_tibble()


           

```

Conducting summary statistics by focusing on variables that are continuos. To do so we define to define a custom function that checks for those variables that are counts. Notice the function screens weather the values of the variable range from 0 to 10. This anonymous function would give us a string of FALSE or TRUE values. Using the `all()` function to check if the values of the variables range from 0 to 10.

Next we can use this information to identify continuous variables. The ames_numeric anonymous function identified 9 count variables. iIf you want to identify continuous variables negage the `!all(x %in% 0:10)`. This revised anonymous function yielded 25 variables that are continous

```{r checking_for_counting_variables}
#| echo: true

library(modeldata)
library(janitor)
library(skimr)
library(modeldata)


ames_numeric |> 
  #Checking that the range of values goes from 0 to 10 across all variables
  select(where(function(x) all(x %in% 0:10) ) )


#If you want to identify continuous variables negate the !all(x %in% 0:10)

ames_non_counts <- ames_numeric |> 
  select(where(function(x) !all(x %in% 0:10) ) )

```

Using the data subset of continuous variables. Let's focus on columns that contain the word `area`. Next identify variables with the `sf` string

```{r ames_continous_variables}
#| echo: true

ames_non_counts |> 
  select(contains('area'))


ames_non_counts |> 
  select(contains('sf'))

#Examining  if some variables are the summ of two others

ames_non_counts |> 
  filter(gr_liv_area != first_flr_sf + second_flr_sf
  )

```

Focusing on sale price, and calculating averages. Notice, Albert introduces the function to request skewness. Skewness belongs to the e1071 library. And to displayed in a nice format, we could pivot_longer the results.

In pivoting_longer, we opt to arrange all of the columns using `cols = everything()`. Notice the median is lower than the mean signifying the distribution is positively skewed.

```{r sale_price_summ_table}
#| echo: true

library(tidyverse)
library(gt)

#Calculating averages using across

ames |> 
  summarise(
    across(
      .cols = sale_price,
      .fns = list(
        min = min,
        avg = mean,
        median = median,
        max = max,
        standard_dev = sd,
        skewness = e1071::skewness),
      .names = '{.fn}'
    )
  ) |> 
#Pivoting longer to create a summary stats table
  pivot_longer(
    cols = everything()
  )

```

The neighborhood column has 28 values, which makes it difficult to handle. Consequently, we could lump together the values. The function`fct_lump_n()` lumps all levels except for the `n` most frequent (or least frequent if `n < 0`). In this case, we are focusing on the top 5 most frequent categories lumping together the rest. It produces 6 neighborhoods. Other is a collection of many neighborhoods.

```{r neighborhood_lumped}
#| echo: true

library(modeldata)
library(janitor)
library(skimr)
library(modeldata)


ames |> 
  count(neighborhood) |> 
  print(n = Inf)

ames |>
  mutate(neighborhood = fct_lump_n(
    neighborhood,
    n = 5)
    ) |> 
  skim(neighborhood)

ames_top_five <-  ames |>
  mutate(neighborhood = fct_lump_n(
    neighborhood,
    n = 5)
    ) 
ames_top_five |> 
  select(neighborhood) |> 
  count(neighborhood) |> 
  print(n = Inf)
```

With this new database, we can do data visualizations comparing the house prices across those 5 neighborhoods. We can use facet_wrap for the neighborhood. We can also use the `scale_x_log10()` to normalize the x-axis.

```{r visualizing_prices_neighborhoods}
#| echo: true

ames_top_five |> 
  ggplot(aes(x = sale_price)) +
  geom_density(fill = 'grey80') +
  facet_wrap(~ neighborhood, nrow = 6) +
  scale_x_log10(labels = scales::label_dollar()) +
  theme_minimal(
    base_family = 'Source Sans Pro',
    base_size = 16
  )


```

We can make this figure more informative by adding labels for the minimum and maximum sale price. To do so, we can add a geom_text() layer. The values for the layer can be extrated from the function `reframe()` .

The `reframe()` function allows to calculate the sale price range. It would give a tibble that has the range of prices . Next we can add the label type to capture the labels minimum and maximum. And lets create those values by neighborhood.

We can save the values in a dataframe, say ranges

```{r reframe_informative_labels}
#| echo: true

ames_top_five |> 
  reframe(
    type = c("min", "max"),
    sale_price = range(sale_price),
    .by = neighborhood
  )

ranges <- ames_top_five |> 
  reframe(
    type = c("min", "max"),
    sale_price = range(sale_price),
    .by = neighborhood
  )

```

Modifying ggplot to add a `geom_text()` layer that captures the labels associated to each of the 5 neighborhoods. In the `geom_text()`, we specify the data, the aes(). For y we specify -1 and for x we rely on sale_price's ranges, and for label we refer to the variable containing the sale price. Notice we rely on y = -1 to place the labels below the chart.

```{r label_layer_geom_text}
#| echo: true

ames_top_five |> 
  ggplot(aes(x = sale_price)) +
  geom_density(fill = 'grey80') +
  geom_text(data = ranges, 
            aes(y = -1, label = sale_price)) +
  facet_wrap(~ neighborhood, nrow = 6) +
  scale_x_log10(labels = scales::label_dollar()) +
  theme_minimal(
    base_family = 'Source Sans Pro',
    base_size = 16
  )

```

Our first attempt with the labels produce characters to big. We can do this legend nicer by passing it to `scales::dollar()` function.

```{r adjusting_geom_label}
#| echo: true

ames_top_five |> 
  ggplot(aes(x = sale_price)) +
  geom_density(fill = 'grey80') +
  geom_text(data = ranges, 
            aes(y = -1, label = sale_price |> 
                  scales::dollar()) ) + 
  facet_wrap(~ neighborhood, nrow = 6) +
  scale_x_log10(labels = scales::label_dollar()) +
  theme_minimal(
    base_family = 'Source Sans Pro',
    base_size = 16
  )


```

To make it more obvious what those labels are, we can add another `geom_point()` layer. It adds points to the min and max. We can also shape as a triangle-like instead of a point (shape = 5).

```{r passing_labels}
#| echo: true

ames_top_five |> 
  ggplot(aes(x = sale_price)) +
  geom_density(fill = 'grey80') +
   geom_point(data = ranges, 
            aes(y = -0.3),
            shape = 5,
            size = 2) +
  geom_text(data = ranges, 
            aes(y = -1.2, label = sale_price |> 
                  scales::dollar()), size = 2.5 ) + 
  
  facet_wrap(~ neighborhood, nrow = 6) +
  scale_x_log10(labels = scales::label_dollar()) +
  theme_minimal(
    base_family = 'Source Sans Pro',
    base_size = 16
  )

```

Another data visualization, but this time dealing with count variables. In this case, we identified 9 count variables. Let's check the column names. There are variables that have `sf` and

```{r viz_count_variables}
#| echo: true

# ames_numeric |> 
#   select(where(function(x) all(x %in% 0:10) ) ) |> 
#  colnames()
#   

```

We can reduce the number of counts lower. We end up with 7 variables instead of 9. We can also exclude variables related to basement and kitchen

```{r reduced_var_counts}
#| echo: true

# selected_counts <- ames_numeric |> 
#   select(where(function(x) all(x %in% 0:5) ) ) |> 
#   select(
#     -contains('bsmt'),
#     -contains('kitchen')
#   ) 
# 
# selected_counts |> 
#   skim()
```

We can mutate again and with `across()`, and transforming the count variables into factor variables. Notice that for most variables the frequencies gravitate around 0 or 1. Consequently, we could lump together the variables.

```{r viz_counts_vars}
#| echo: true

# selected_counts |> 
#   mutate(
#     across(
#       .cols = everything(),
#       .fns = factor ) 
#     ) |> 
#   skim()

```

### Lumping factor variables

Lumping together factor variables with limited range. Instead of using `fct_lump_n`, we could use if_else to create variables capturing the lump sum. In so doing, we can use `across()` along an anonymous or custom function that leaves numbers smaller than 2 as is, but numbers greater than 2 are replaced with the text `2+`. The function to do so is `if_else().`

```{r changing_factors_lim_range}
#| echo: true

# selected_counts |> 
#   mutate(
#     across(
#       .cols = everything(),
#       .fns = function(x) if_else(x < 2, x, '2+')
#   ) )

```

However, our function generated an error since we are dealing with numeric values not characters. Consequently, we need to transform all the variablse into a character format. And the `skim()` function reveals we have three unique values

```{r changing_factor_into_character}
#| echo: true

# selected_counts |> 
#   mutate(
#     across(
#       .cols = everything(),
#       .fns = function(x) if_else(x < 2, 
#                                  as.character(x), '2+')
#   ) ) |> 
#   skim()


```

And then we could also transform back into a factor by adding the `factor()` function at the end.

```{r back_into_factor}
#| echo: true

# selected_counts |> 
#   mutate(
#     across(
#       .cols = everything(),
#       .fns = function(x) if_else(x < 2, 
#                                  as.character(x), 
#                                  '2+'
#                                  ) |> 
#         factor(levels = c(0:1,'2+')) ) ) |> 
#   skim()

```

We can modify the dataset to include the house sale price

```{r house_sale_counts}

# selected_counts <- ames_numeric |> 
#   select(
#     sale_price,
#     where(function(x) all(x %in% 0:5) ) ) |> 
#   select(
#     -contains('bsmt'),
#     -contains('kitchen')
#   ) 


```

And then, we have to make sure our `across()` call includes all variables, but the sale_price column. And now we have all count variables but sale_price which is a continuous variable.

```{r includying_house_price_factors}

# selected_counts |> 
#   mutate(
#     across(
#       .cols = -sale_price,
#       .fns = function(x) if_else(x < 2, 
#                                  as.character(x), 
#                                  '2+'
#                                  ) |> 
#         factor(levels = c(0:1,'2+')) ) ) |> 
#   skim()
# 
# grouped_counts <- selected_counts |> 
#   mutate(
#     across(
#       .cols = -sale_price,
#       .fns = function(x) if_else(x < 2, 
#                                  as.character(x), 
#                                  '2+'
#                                  ) |> 
#         factor(levels = c(0:1,'2+')) ) )

```

The dataset needs to be transformed into a long format for ggplot(). We need to rearrange everything but the sale price.

```{r grp_count_pivot_longer}
#| echo: true

# grouped_counts |> 
#   pivot_longer(
#     cols = -sale_price
#   ) |> 
#   ggplot(aes(x = sale_price)) +
#   geom_density(fill = 'grey80')
# 
# grouped_counts |> 
#   pivot_longer(
#     cols = -sale_price
#   ) |> 
#   ggplot(aes(x = sale_price)) +
#   geom_density(fill = 'grey80') +
#   scale_x_log10(label = scales::label_dollar()) +
#   facet_grid(
#     rows = vars(name),
#     cols = vars(value)
#     ) +
#   theme_minimal(
#     base_family = 'Source Sans Pro',
#     base_size = 16
#   )



```

We can center all distributions to their mean price using the `scale()` function. But we need to scale the distributions by the type of amenity. Notice we got lots of warning messages because of negative values that led to infinite values during the log-10 transformation.

```{r scaling_price_ammenities}
#| echo: true

# grouped_counts |> 
#   pivot_longer(
#     cols = -sale_price
#   ) |> 
#   mutate(
#     sale_price = scale(sale_price),
#     .by = name
#   ) |> 
#   ggplot(aes(x = sale_price)) +
#   geom_density(fill = 'grey80') +
#   scale_x_log10(label = scales::label_dollar()) +
#   facet_grid(
#     rows = vars(name),
#     cols = vars(value)
#     ) +
#   theme_minimal(
#     base_family = 'Source Sans Pro',
#     base_size = 16
#   )


```

Changing the log-10 transformation

```{r repalcing_log_10}
#| echo: true

# grouped_counts |> 
#   pivot_longer(
#     cols = -sale_price
#   ) |> 
#   mutate(
#     sale_price = scale(sale_price),
#     .by = name
#   ) |> 
#   ggplot(aes(x = sale_price)) +
#   geom_density(fill = 'grey80') +
#   scale_x_continuous() +
#   facet_grid(
#     rows = vars(name),
#     cols = vars(value)
#     ) +
#   theme_minimal(
#     base_family = 'Source Sans Pro',
#     base_size = 16
#   )


```

However, the scaling works better if you log scale the variable before

```{r log_before}
#| echo: true

# grouped_counts |> 
#   pivot_longer(
#     cols = -sale_price
#   ) |> 
#   mutate(
#     sale_price = scale(sale_price |> 
#                          log()),
#     .by = name
#   ) |> 
#   ggplot(aes(x = sale_price)) +
#   geom_density(fill = 'grey80') +
#   scale_x_continuous() +
#   facet_grid(
#     rows = vars(name),
#     cols = vars(value)
#     ) +
#   theme_minimal(
#     base_family = 'Source Sans Pro',
#     base_size = 16
#   )

```
