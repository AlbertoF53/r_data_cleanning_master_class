---
title: "Part 1"
author: "Albert Rapp"
date: June 19, 2024
format:
  html:
    code-link: true
    code-fold: true
    self-contained: true
    embed-resources: true
    toc: true
    theme: Zephyr
    number-sections: false
execute: 
  warning: false
  message: false
  echo: false
editor_options: 
  chunk_output_type: console
---

# Part I

## Lesson 1

Learning about the mechanics of wrangling data. Particularly `mutate()` and `summarise()` functions in an efficient manner of the databases to be used: penguins and ames.

```{r libraries_data}
#| echo: true

library(palmerpenguins)
library(tidyverse)
library(modeldata)
library(janitor)
library(skimr)
library(e1071)
library(gt)
library(gtExtras)

ames <- ames
penguins <- penguins

```

The dimensions of the penguins database: 344 observations and 7 columns or variables

```{r penguins_dimensions}
#| echo: true

dim(penguins)
```

To have a good overview of the database, we can use the `skim()` function from the skimr package. It provides a summary statistics, and report the proportion of missing cases

```{r skimr_penguins}
#| echo: true

palmerpenguins::penguins |> 
  skimr::skim()

```

An alternative to skimr for descriptive statistics is `gt_plt_summary()` function of the gtExtras package. For it to work, you need to select the variables in advance.

```{r an_alternative_gt}
#| echo: true

library(tidyverse)
library(gtExtras)
library(gt)

penguins |>
  select(bill_length_mm,bill_depth_mm) |>
  gt_plt_summary()

```

If you want to focus on one variable, you can deposit it into the skim() function. For instance, examining the properties of Lot Frontage and Lot_Area

```{r skim_Lot_Frontage}
#| echo: true

modeldata::ames |> 
  skimr::skim(Lot_Frontage,Lot_Area)

```

Notice the ames data has not nice variable names in that they are capitalized. Good idea to use janitor::clean_names to convert capital letters into lower letters.

```{r janitoring_ames}
#| echo: true

modeldata::ames |> 
  janitor::clean_names() |> 
  skimr::skim(lot_frontage,lot_area)

```

## Lesson 2

The focus of this lesson is the `mutate()` function. It also deals with vectorized vs iterative calculations associated with with the `mutate()` function.

It also introduces the `map()` function. This function aligns a function to a vector, compelling the estimating of one-by-one instead of vector mashing. The variations of map() discussed include the `map_lgl()` (map logical) and the `map_dbl()` (map numeric or double). The first one produces a logical vector. The second produces numeric values of 1 (True) and 0 (False).

### Handling missing cases

It is important to handle missing values before doing mutate(). We can verify missing cases were removed using skimr. Remember that the skim function revealed that sex had 11 missing cases, or the completion ratio was 97%.

```{r missing_values_penguins}
#| echo: true

library(palmerpenguins)
library(tidyverse)
library(modeldata)
library(janitor)

#Removing missing cases using !is.na(sex)

penguins_wo_NAs <-  palmerpenguins::penguins |> 
  filter(!is.na(sex))

skimr::skim(penguins_wo_NAs)


```

### Handling vectors

**Mutate** *is a* *vectorised function*. This means that it takes the two vectors listed in the mutate function as a whole. In other words, the values of each column are not taken one-by-one. The values are handled as a vector, and then they are operated via a vectorized division (/).

Imagine these two vectors are "smashed together" and the result is divided.

```{r mutate_penguins_basic}
#| echo: true

penguins_wo_NAs |> 
  mutate(
    bill_flipper_ratio = bill_length_mm / flipper_length_mm, na.rm = TRUE
  ) |> 
  select(bill_length_mm, bill_depth_mm, bill_flipper_ratio)

```

Example where the vectorized condition may not be met. Using a custom function to illustrate this problem. Next let's apply the function to one of the vectors used in estimating bill_flipper_ratio. In this case, bill_length_mm. Our hope is that we would get a vector of TRUE and FALSE.

Notice that the application of the function prompted an error message. It was traced back to the `if (x > 39) ...:` The result shows the condition has `length > 1`.

```{r vector_principle}
#| echo: true

large_quantity <-  function(x) {
  if (x > 39) {
    return(TRUE)
   } else {
    return(FALSE)
  }
}

# penguins_wo_NAs |>
#   mutate(
#     large_bill_length = large_quantity(bill_length_mm)
#     ) |>
#   select(bill_length_mm,large_bill_length)


#Reconstructing the essence of the function as a vector
c(34, 234) > 39

```

### Functional programming or `map()`

***map_lgl() enforces the one-by-one calculation instead of the vector calculation***

To address the limitation of working with vectors, we need `functional programming`. `map()` allows one-to-one calculation operation. This operation is called upon by the `large_quantity` function within the mutate() function.

In other words, instead of using the `vector smashing` approach to handle observations contained in the variable, we rely on handling each variable listed in the mutate function.

We rely on the `map()` function. In this case the logical map function ( `map_lgl()`). And then we instruct R to apply the `large_quantity()` function to each component, one-by-one, of the column or vector **bill_lenght_mm**. Notice, map_lgl() lists the vector or column first, and then the function to be applied to it. In other words, map_lgl() applies the function to each element of the vector or column instead of smashing the vector.

*"The map functions transform their input by applying a function to each element of a list or atomic vector and returning an object of the same length as the input."*

```{r functional_programming_penguins}
#| echo: true

large_quantity <-  function(x) {
  if (x > 39) {
    return(TRUE)
  } else {
    return(FALSE)
  }
}

penguins_wo_NAs |>
  mutate(
    large_bill_length = map_lgl(
      bill_length_mm,
    #Applying large_quantity function to vector bill_lenght_mm
      large_quantity) ) |> 
  select(bill_length_mm,large_bill_length)


```

Another variation is the `map()` itself. But instead of producing individual values, map() produces lists of values. However, the list of variables are reported in a vector.

```{r map_list}
#| echo: true


large_quantity <-  function(x) {
  if (x > 39) {
    return(TRUE)
  } else {
    return(FALSE)
  }
}

penguins_wo_NAs |>
  mutate(
    large_bill_length = map(
      bill_length_mm,
  #Applyng large function to vector bill_lenth_mm
      large_quantity) ) |> 
  select(bill_length_mm,large_bill_length)


```

Still another variation of the `map()` function is the `map_dbl()`. It reports numbers (1 = TRUE, 0 = FALSE) instead of logical statements. Notice, we list first the column to be modified followed by the function (`large_quantity`).

```{r map_numbers}
#| echo: true

large_quantity <-  function(x) {
  if (x > 39) {
    return(TRUE)
  } else {
    return(FALSE)
  }
}

penguins_wo_NAs |>
  mutate(
    large_bill_length = map_dbl(
      bill_length_mm,
      large_quantity) ) |> 
  select(bill_length_mm,large_bill_length)


```

## Lesson 3

This section covers the `summarise()`function. This function is the opposite of the `mutate()` function. It also works with vectors, but the difference is the amount of output. While mutate can be used to produce vectors, **summarise** *turns* out **single elements**, or **atomic values**.

```{r summarise_function}
#| echo: true

penguins_wo_NAs |> 
  summarise(
    mean_flipper_length = mean(flipper_length_mm, na.rm = TRUE),
    sd_flipper_length = sd(flipper_length_mm, na.rm = TRUE) )

```

You can also save the lengths into one single list using the `list()` function. In other words, the last column, labeled `flipper_lenghts()` contains a vector of values. This last column has 333 values corresponding to 333 penguins' flipper lengths.

```{r listing_summary_results}
#| echo: true

penguins_wo_NAs |> 
  summarise(
    mean_flipper_length = mean(flipper_length_mm, na.rm = TRUE),
    sd_flipper_length = sd(flipper_length_mm, na.rm = TRUE),
    flipper_lengths = list(flipper_length_mm))


```

## Lesson 4

Handling missing values with common calculations. If not handled, the summarise function would generate NAs. One option is to use `na.rm = TRUE.`

```{r handling_nas}
#| echo: true


palmerpenguins::penguins |> 
  summarise(
    mean_flipper_length = mean(flipper_length_mm, na.rm = TRUE),
    sd_flipper_length = sd(flipper_length_mm, na.rm = TRUE),
    flipper_lengths = list(flipper_length_mm))

```

## Lesson 5

Repeated calculations for subsets of data. Two options to automate repeated calculations: `for()` loop. However this approach is messy and tedious.

```{r for_loop_repetead}
#| echo: true

for (selected_species in unique(penguins_wo_NAs$species)) {
  penguins_wo_NAs |> 
    filter(species == selected_species) |> 
    summarise(
      mean_flipper_length = mean(flipper_length_mm, na.rm = TRUE),
      sd_flipper_lenth = sd(flipper_length_mm, na.rm = TRUE)) |> 
    print()
}
```

Another is to rely on loop calculations. The best alternative is to rely on the group approach.

An example is to rely on the data without missing values. One option is to rely on the option .by = () to specify subgroups. Be careful about omitting the "." It would produce an output but would miss summarizing results by species.

```{r repeated_calc}
#| echo: true

penguins_wo_NAs |> 
  summarise(
    mean_flipper_length = mean(flipper_length_mm, na.rm = TRUE),
    sd_flipper_length = sd(flipper_length_mm, na.rm = TRUE),
    flipper_lengths = list(flipper_length_mm),
    .by = species)

```

Warning regarding grouping in a series of concatenated computations as show below. group_by() affects the original structure of the data by the first variable listed in the `group_by()` function.

Notice the output indicates `#Groups : species[3]` Or 3 species by 5 islands. Adding another separate `summarise()` function generates a problem. Suppose we want to estimate the mean of the mean_flipper_length estimated in the previous run as well as the sd of the sd_flipper_lenght variable previously created. The end result is an sd_flipper_length with two NAs.

During the first calculation, `group_by()` strips the last group alluded in the function; namely `island` variable. In other words, the variable or column island is eliminated. The end result is a 5 X 5 tibble table organized by three groups corresponding to the 3 species. This is an opportunity to mistakes if one ignores the structure imposed on the data by group_by() function. Grouped calculations behave differently.

```{r grouping_danger}
#| echo: true

penguins_wo_NAs |> 
  group_by(species, island) |> 
  summarise(
    mean_flipper_length = mean(flipper_length_mm),
    sd_flipper_length = sd(flipper_length_mm),
    flipper_lengths = list(flipper_length_mm))  |> 
  #The next second summarise creates a problem
  summarise(
    mean_flipper_length = mean(mean_flipper_length),
    sd_flipper_length = sd(sd_flipper_length)
  )

```

To remove the underlying grouping structure, remove the grouping using the option .groups = "drop' in the `summarise()` function. This option restores the original tibble table of 5 X 5.

```{r eliminating_underscore_grouping}
#| echo: true

penguins_wo_NAs |> 
  group_by(species, island) |> 
  summarise(
    mean_flipper_length = mean(flipper_length_mm),
    sd_flipper_length = sd(flipper_length_mm),
    flipper_lengths = list(flipper_length_mm),
    .groups = 'drop')

```

Another alternative to `.groups = "drop"` is the function `.by = c(species,island)` , which also restores the original data structure. Notice that in this case, the .`by =` function replaces the `group_by()` function.

```{r _by_option_groups}
#| echo: true

# Notice: we don't use the group_by() function

penguins_wo_NAs |> 
  summarise(
    mean_flipper_length = mean(flipper_length_mm),
    sd_flipper_length = sd(flipper_length_mm),
    flipper_lengths = list(flipper_length_mm),
    .by = c(species, island))

```

## Lesson 6

### Unique function

Using grouping `.by =`function with mutate can produce a long list of means. In this case, a long list of means by species. Notice, the mean column only reports one single mean value in the screen. To see the rest of the means across species, you need to create a column, say test, and print it.

```{r group_calculations}
#| echo: true

palmerpenguins::penguins |> 
  mutate(
    mean_column = mean(bill_length_mm, na.rm = TRUE),
    .by = species) |> 
  select(bill_length_mm, flipper_length_mm, mean_column)

```

In other to make sense of the the listing, let's just select the species column and the mean column.

```{r listing_all_means_species}
#| echo: true

palmerpenguins::penguins |> 
  mutate(
    mean_column = mean(bill_length_mm, na.rm = TRUE),
    .by = species) |> 
  select(species, mean_column)

```

In order to avoid a long list of 344 means by species, use the `unique()` function. The unique function eliminates duplicates. There were many duplicates of the mean for each species which unique eliminated by reporting three species.

The `.by = species` forces that the calculations take place separately by the components of the column referenced to, in this case, 3 species of penguins. `unique()`, on the other hand, excludes duplicated means within each species or rows.

```{r unique_group_function}
#| echo: true

palmerpenguins::penguins |> 
  mutate(
    mean_column = mean(bill_length_mm, na.rm = TRUE),
    .by = species) |>
  select(species,mean_column)|>
  unique()

```

### Centering a variable by its mean

Centering, using `scale()` function, a variable represents an ideal situation to use the function `.by =` in a mutate operator instead of using summarise. A situation to use the `scale()` instead of reporting the means for each group as we did with mutate(). `scale()` is a generic *R* function whose default method centers and/or scales the columns of a numeric matrix."

Notice, we are centering, or scaling each penguin's species bill length to its mean. Assuring transparency by requesting alpha = 0.5.

Centering is a technique to help display the distribution as normal. It is also a popular technique in machine learning. It also helps to locate the three species's distribution within a common line. It allows comparisons across species, or group scaling.

```{r group_calculations_summarise}
#| echo: true 

penguins |> 
  filter(!is.na(sex)) |> 
  mutate(bill_length_mm = scale(bill_length_mm), 
         .by = species) |> 
  ggplot() +
  geom_density(aes(x = bill_length_mm, 
                   fill = species,
                   alpha = 0.5))

```

Without centering, you can see that the three distribution have different scales. The lengths within each distribution vary between 30 to 60. So the location within each species is different.

```{r depicting_density_non_centered}
#| echo: true

palmerpenguins::penguins |> 
  # mutate(
  #   bill_length_mm = scale(bill_length_mm),
  #   .by = species) |>
  ggplot() +
  geom_density(aes(x = bill_length_mm, fill = species),
               alpha = 0.5) 

```

Another option is to center within each species instead of across species. As shown below it produced a graph without a common center. This is why if you want to make group comparisons, you need to use the .by option, which produces a common center or zero point.

```{r centering_no_species}
#| echo: true

penguins |> 
  filter(!is.na(sex)) |> 
  mutate(bill_length_mm = scale(bill_length_mm), 
         # by = species
         ) |>
  ggplot() +
  geom_density(aes(x = bill_length_mm, 
                   fill = species,
                   alpha = 0.5))


```

## Lesson 7

### Missing data with group calculations

Notice not all combinations between species and islands are displayed. For instance Chinstrap -Biscoe, Chinstrap - Torgesen, Gentoo - Dream, and Gentoo-Torgeson combinations are not displayed.

```{r missing_data_grp_calc}
#| echo: true

penguins |> 
  summarise(
    mean_flipper_length = mean(flipper_length_mm, na.rm = TRUE),
    sd_flipper_length = sd(flipper_length_mm, na.rm = TRUE),
    flipper_lengths = list(flipper_length_mm),
    .by = c(species,island) ) 


```

To display all combinations between species and islands, even for those combinations with no observations we could use the following `complete()` function.

`complete()` allows one to display all combinations among variables even with missing data. It also allows to change the default `NA` value for another, say 0, using the `fill()` function.

```{r displaying_missing_comb}
#| echo: true

penguins_wo_NAs |> 
  summarise(
    mean_flipper_length = mean(flipper_length_mm, na.rm = TRUE),
    sd_flipper_length = sd(flipper_length_mm, na.rm = TRUE),
    flipper_lengths = list(flipper_length_mm),
    .by = c(species,island) ) |> 
  complete(
    species, island,
    fill = list(mean_flipper_length = 0,
                sd_flipper_length = 0)
  )

```

## Lesson 8

### Using across() + mutate() instead of repetitions.

**Original mutate**

```{r orginal_mutate}
#| echo: true

penguins_wo_NAs |> 
  summarise(
    mean_bill_depth = mean(bill_depth_mm),
    mean_bill_length = mean(bill_length_mm),
    mean_flipper_length = mean(flipper_length_mm),
    mean_body_mass = mean(body_mass_g))


```

#### Using 'across()\` function. Several variations.

across() function facilitates eliminating repetition.

Also using glue() to affix a name to the corresponding output. This is done via the functions `.names = 'mean_{.col}'`. This glue function affix the legend `mean_` to all the columns/variables referenced in the `.cols = c(` function.

Notice the output does not display the prefix `mean_`. If you want it you need the option `.names` , which is a glue specification.

```{r across_variations_mutate}
#| echo: true

library(glue)

#Long option
penguins_wo_NAs |> 
  summarise(across(
    .cols = c("bill_length_mm",
              "bill_depth_mm",
              "flipper_length_mm",
              "body_mass_g"),
    .fns = mean,
    .names = 'mean_{.col}' #Glueing 
  ))


#Columns option
penguins_wo_NAs |> 
  summarise(across(.cols = 3:6, 
                   function(x) mean(x, na.rm = TRUE),
                   .names = 'mean_{.col}'))


#where(is.numeric)
penguins_wo_NAs |> 
  summarise(across(where(is.numeric), 
                   function(x) mean(x, na.rm = TRUE),
                   .names = 'mean_{.col}') )

#Selecting columns with specific names
penguins_wo_NAs |> 
  summarise(across(contains('bill'), 
                   function(x) mean(x, na.rm = TRUE),
                   .names = 'mean_{.col}') )

#Selecting columns with specific names
penguins_wo_NAs |> 
  summarise(across(starts_with('flipper'), 
                   function(x) mean(x, na.rm = TRUE),
                   .names = 'mean_{.col}') )

```

#### Removing unwanted strings

Note that we could remove unwanted strings such as `_mm` and `_g`.

```{r removing_unwanted_strings}
#| echo: true

penguins_wo_NAs |> 
  summarise(across(
    .cols = c("bill_length_mm",
              "bill_depth_mm",
              "flipper_length_mm",
              "body_mass_g"),
    .fns = mean,
    .names = 'mean_{.col |> 
    str_remove("_mm") |> 
    str_remove("_g")}'
  ))


```

Another option is to include the .fn in the names function. But unfortunately it did not do the trick because the prefix mean was removed. Instead a prexif `1_` was added .

```{r adding_fn_names_function}
#| echo: true

penguins_wo_NAs |> 
  summarise(across(
    .cols = c("bill_length_mm",
              "bill_depth_mm",
              "flipper_length_mm",
              "body_mass_g"),
    .fns = mean,
    .names = '{.fn}_{.col |> str_remove("_mm") |> str_remove("_g")}'
  ))

```

To make it work correctly, we need to use `.fns = list(mean = mean)`. This is the best option to display the names of the summary statistics called for. The `list()` gives a name to the function.

```{r adding_fn_list_names_function}
#| echo: true

penguins_wo_NAs |> 
  summarise(across(
    .cols = c("bill_length_mm",
              "bill_depth_mm",
              "flipper_length_mm",
              "body_mass_g"),
    .fns = list(avg = mean), #list gives a name to the function 
    .names = '{.fn}_{.col |> str_remove("_mm") |> str_remove("_g")}'
  ))

```

You can compute more summary statistics by adding estimation of standard deviation.

```{r extending_summary_statistics}
#| echo: true

penguins_wo_NAs |> 
  summarise(across(
    .cols = c("bill_length_mm",
              "bill_depth_mm",
              "flipper_length_mm",
              "body_mass_g"),
    .fns = list(avg = mean, standard_deviation = sd),
    .names = '{.fn}_{.col |> str_remove("_mm") |> str_remove("_g")}'
  ))


```

## Lesson 9

### Using across() + mutate( ) instead of repetitions

Using `across()` inside `mutate()` facilitates cleaning multiple columns in one single shot. In this example, we would be using `across()` along with the `parse_number()` to extract the first number inside a text.

We get a warning but the numbers were parsed or extracted from the strings. The end result is a tibble table with numbers instead of characters.

`parse_number` extracts the number of any string whose text has numbers embedded into them.

```{r across_mutate_option}
#| echo: true

tibble(
  text1 = c("I have 10 apples", "I have 20 appples"),
  text2 = c("I have 30 apples", "I have 40 apples")
) |> 
  mutate(
    across(
      .cols = everything(),
      .fns = parse_number
    )
  )

```

If you want to include both numbers and text, you need to add a new column as follows:

```{r across_mutate_more_columns}
#| echo: true

tibble(
  text1 = c("I have 10 apples", "I have 20 appples"),
  text2 = c("I have 30 apples", "I have 40 apples")
) |> 
  mutate(
    across(
      .cols = everything(),
      .fns = parse_number,
      .names = "number_{.col}"
    )
  )

```

## Lesson 10

### Adding calculations next to across()

Recapping a prior example below. The operations listed after summarize is just one of the calculations that could be further added. Below the `str_remove()` removes text listed within parenthesis across all columns.

```{r adding_calc_across}
#| echo: true

penguins_wo_NAs |> 
  summarise(across(
    .cols = c("bill_length_mm",
              "bill_depth_mm",
              "flipper_length_mm",
              "body_mass_g"),
    .fns = mean,
    .names = 'mean_{.col |> str_remove("_mm")}'
  ))

#Or, you can glue the label associated to the mean
penguins_wo_NAs |> 
  summarise(across(
    .cols = c("bill_length_mm",
              "bill_depth_mm",
              "flipper_length_mm",
              "body_mass_g"),
    .fns = list(Avg = mean),
    .names = '{.fn}_{.col |> str_remove("_mm")}'
  ))


```

Adding more operations to summarise(). Say you want to add a standard deviation, say sd for mean_bill_length. All what you need to do is to add a line.

```{r adding_more_calc_summarise}
#| echo: true


penguins_wo_NAs |> 
  summarise(across(
    .cols = c("bill_length_mm",
              "bill_depth_mm",
              "flipper_length_mm",
              "body_mass_g"),
    .fns = mean,
    .names = 'mean_{.col |> str_remove("_mm")}'
  ),
  sd_bill_length = sd(bill_length_mm)
  )



```

## Lesson 11

### Grouped calculations with across()

In the last lesson we learned that not only can we use across() for multiple computations, but also we can combine it with other analyses. This means we could do across calculations on a group basis as well. In short, we can perform powerful estimations in a concise manner.

In the example below, we can add the estimation of means and standard deviations by species.

```{r grouped_calculations_across}
#| echo: true

penguins_wo_NAs |> 
  summarise(across(
    .cols = c("bill_length_mm",
              "bill_depth_mm",
              "flipper_length_mm",
              "body_mass_g"),
    .fns = mean,
    .names = 'mean_{.col |> str_remove("_mm")}'
  ),
  .by = species
  )

```

We can extend the grouping to species by islands

```{r adding_by_for_groups}
#| echo: true

penguins_wo_NAs |> 
  summarise(across(
    .cols = c("bill_length_mm",
              "bill_depth_mm",
              "flipper_length_mm",
              "body_mass_g"),
    .fns = mean,
    .names = 'mean_{.col |> str_remove("_mm")}'
  ),
  .by = c(species,island)
  )

```

An extension of this idea is to add another variable outside the function, in this case estimating the standard deviation of bill_lenght_mm.

```{r extension_idea_str_remove}
#| echo: true

penguins_wo_NAs |> 
  summarise(across(
    .cols = c("bill_length_mm",
              "bill_depth_mm",
              "flipper_length_mm",
              "body_mass_g"),
    .fns = list(Average = mean),
    .names = '{.fn}_{.col |> str_remove("_mm") |> str_remove("_g")}'
  ),
  .by = c(species,island),
  std_bill_length = sd(bill_length_mm)
  )

```

## Lesson 12

### Getting to know the `reframe()` function

`reframe()` is a function sitting between mutate and summarize. This function has an arbitrary length between one, which is associated with `summarise(),` and multiple, which is associated with `mutate()`.

The `range()` function is relevant. It tells you the minimum and maximum values of a column.

If we use range() within summarise() we get an error message, asking to use `reframe()` instead.

In short, `reframe()` is a highly flexible function that replicates many of the flexibility of `summarise()` function. It can generate labels and grouping by another variable. It can also accommodate `across()`and its multiple options.

```{r reframe_function}
#| echo: true

range(penguins_wo_NAs$bill_length_mm)

# penguins_wo_NAs |>
#   summarise(range_bill_lenght_mm = range(bill_length_mm))

#Using reframe() function instead of summarise()
penguins_wo_NAs |> 
  reframe(range_bill_lenght_mm = range(bill_length_mm))

```

### Adding labels via `type( )` to `reframe()`

Since `reframe()` along `range()` would return unnamed results, we could add `type()` for adding the labels to be added into the output.

```{r reframe_range_type}
#| echo: true

penguins_wo_NAs |> 
  reframe(
    type = c("min", "max"),
    range_bill_lenght_mm = range(bill_length_mm),
    .by = c(species) )

```

### Grouping with reframe( )

We can do more things with `reframe( )`. We can use group statistics as well.

```{r reframe_flexibility}
#| echo: true

penguins_wo_NAs |> 
  reframe(
    type = c("min", "max"),
    range_bill_lenght_mm = range(bill_length_mm),
    .by = species)

```

We can also add across() and request range() within `.fns =`. This approach insert the `across( )` function within the `reframe( )`function.

```{r reframe_group_across}
#| echo: true

penguins_wo_NAs |> 
  reframe(
    type = c("min", "max"),
    across(.cols = c("bill_length_mm",
              "bill_depth_mm",
              "flipper_length_mm",
              "body_mass_g"),
           .fns = range
             ),
    .by = species)

```

### Creating tables with `reframe()`

But we could also format it as a table. To do so, we need to call upon `pivot _longer( )` function as well as the and `pivot_wider( )` function.

Note: We could have used this option for the CHCI report documenting entering years across cohorts by programs.

```{r table_reframe}
#| echo: true

library(gt)
library(gtExtras)

#Pivot longer
penguins_wo_NAs |> 
  reframe(
    type = c("min", "max"),
    across(.cols = c("bill_length_mm",
              "bill_depth_mm",
              "flipper_length_mm",
              "body_mass_g"),
           .fns = range
             ),
    .by = species) |> 
  pivot_longer(
    cols = -c(1:2),
    names_to = "quantity",
    values_to = 'value'
  ) |> 
  pivot_wider(
    id_cols = c(quantity),
    names_from = c(species,type),
    values_from = value
  ) |> 
  gt()

```

## Lesson 13

### Motivation for using tidyselect helpers

They are powerful techniques. Allows one to select the correct columns or variables. For instance, the `.fns =` allows one to label the variables. Then those labels are collected via the `.names =` function (see below):

```{r motivation_tidyselect}
#| echo: true

penguins_wo_NAs |> 
  summarise(
    across(
    .cols = c("bill_length_mm",
              "bill_depth_mm",
              "flipper_length_mm",
              "body_mass_g"),
    #Adding labels to the summary statistics
    .fns = list(avg = mean),
    #Glueing summary stat labels & column names 
    .names = '{.fn}_{.col |> str_remove("_mm")}'),
  .by = c(species,island))

```

Instead of listing the columns where the means and standard deviations are to be computed, we could use tidy helpers to facilitate locating them. The tidyselect to be used is `where`. It works in combination with another function, `is.numeric()`, that finds the desired property of the column or variable.

```{r using_tidyhelpers_columns}
#| echo: true

penguins_wo_NAs |> 
  summarise(across(
    .cols = where(is.numeric),
    .fns = list(avg = mean),
    .names = '{.fn}_{.col |> str_remove("_mm")}'
  ),
  .by = c(species,island))


```

Notice this omnibus statement includes the column year, which is categorical. In other to exclude this kind of variable from the omnibus statement we can add the `-`(minus) to exclude the factor variable `year`. Notice we need to encircle it within the concatenate function c( ) . The script `c(where(is.numeric), -year)` concatenates the tidy helper while excluding the column year.

```{r where_is_numeric_but_no_categorical}
#| echo: true

penguins_wo_NAs |> 
  summarise(across(
    .cols = c(where(is.numeric), -year),
    .fns = list(avg = mean),
    .names = '{.fn}_{.col |> str_remove("_mm")}'
  ),
  .by = c(species)
  )



```

## Lesson 14

### Using anonymous function

Another custom function for `across()` function. Using the original penguins dataset. Performing the function, you will get missing data.

How to avoid these situations of ending up with `NA`s. We can use the mean function with `na.rm` option. The way to do so is to specify a new function that also specifies `na.rm = TRUE`. We can do so via an anonymous `function(x)`

Three options to use anonymous functions:

```{r across_other_function}
#| echo: true

#Option 1: removing NAs within the mean function
palmerpenguins::penguins |> 
  summarise(across(
    .cols = c(where(is.numeric), -year),
    .fns = function(x) mean(x, na.rm = TRUE),
    .names = 'mean_{.col |> str_remove("_mm")}'
  ),
  .by = c(species,island))

#Option 2: removing NAs in the function
palmerpenguins::penguins |> 
  summarise(across(
    .cols = c(where(is.numeric), -year),
    .fns = \(x) mean(x, na.rm = TRUE),
    .names = 'mean_{.col |> str_remove("_mm")}'
  ),
  .by = c(species) )




#Option 3 creating a prefix for the new variables

palmerpenguins::penguins |> 
  summarise(across(
    .cols = c(where(is.numeric), -year),
    .fns = list(avg = function(x) mean(x, na.rm = TRUE)),
    .names = '{.fn}_{.col |> str_remove("_mm")}'
  ),
  .by = c(species,island))


```

## Lesson 15

This lesson shows the most important `tidy` select helpers; namely, `is.numeric`, `is.factor`, `is.character`.

Selecting the right data by column/variable types. Using select along with `where(is.numeric)` screens the database in search of variables that are numeric. We can eliminate non numeric variables as well. In the example below, we exclude the column year.

The `where()` function can be used in several scenarios. One is to select numeric variables. Another for selecting factor variables. And finally selecting both factor and character variables.

### Using `where( )` function to select variables by measurement (numeric, factor, character)

In this example we only want to include numeric variables. We are excluding year. This variable although coded as numerical is at the end categorical.

```{r right_data_column_type}
#| echo: true

#Scenario 1: selecting numeric variables while excludying count variable

penguins_wo_NAs |> 
  select(where(is.numeric), -year) 

#Scenario 2: selecting variables that are factors

penguins_wo_NAs |> 
  select(where(is.factor))


# Scenario 3: selecting factor and character variables

penguins_wo_NAs |> 
  select(where(is.factor), where(is.character))

```

### Using anonymous functions inside `where()`

Alternatively, you can define a custom function. Inside the `where()` function you can use the anonymous function. Combine the two with an or \|\|.

Consequently the defined function is TRUE if the corresponding column/variable is a factor or a character. If numeric, it would return a FALSE.

```{r custom_function_where}
#| echo: true

penguins_wo_NAs |> 
  select(where(function(x) is.factor(x) || is.character(x)) )


```

### Anonymous functions and if conditions for selecting variables

Defining custom function to identify numeric columns. And we could add another condition such as identifying the mean to be above a particular value.

So here we defined an anonymous function that checks whether a variable is numeric and where the mean of the variable is above 200.

The difference between the double && and the single & operator is that the single operator generates a warning message. The argument is not numeric or logical.

```{r custum_function_numeric}
#| echo: true

penguins_wo_NAs |> 
  select(where(function(x) is.numeric(x) && mean(x) > 200 ) )

```

### Anonymous function to identify NAs

Another handy option is to get columns where specific percentage of values are missing. In this case asking for variables with more than 1% of missing cases.

It is usually practical to name this function for future use.

```{r anonymous_functions_for_per_na}
#| echo: true

palmerpenguins::penguins |> 
  select(where(function(x) mean(is.na(x)) > 0.01) )


```

### Creating function to identify variables with missing cases

Sometimes is better to save a function and give it a name for future use. The name should tell what the function does. In this case, we want to save a function that identifies the columns or variables having more than 1% missing cases. We label it `more_than_one_percent_missing`.

```{r anonimous_function_missing_cases}
#| echo: true

#Creating function
more_than_one_percent_missing <- function(x) mean(is.na(x)) > 0.01

#Using function

palmerpenguins::penguins |> 
  select(where(more_than_one_percent_missing ))

```

## Lesson 16

This lesson further examines tidy helpers for selecting columns based on names or strings embedded into the column/variable.

1.  Select the right data by column name

2.  Selecting consecutive columns. Between bill_length and sex column

3.  Selecting columns by specific variable's labels.

    1.  Selecting variables ending with `_mm`

4.  Selecting columns with start with a particular string.

    1.  Selecting variables starting with the string `bill`.

5.  Selecting columns that contain similar strings.

    1.  Selecting variables with the string `lenght`

### Selecting consecutive columns

Selecting columns from bill_length_mm to sex.

```{r selecting_consecutive_columns}
#| echo: true
#Selecting consecutive columns
penguins_wo_NAs |> 
  select(bill_length_mm:sex)

```

### Selecting columns ending with a particular string

```{r selecting_columns_end_string}
#| echo: true

#Selecting columns ending with _mm
penguins_wo_NAs |> 
  select(ends_with('_mm'))

```

### Selecting columns that start with a particular string

Selecting variables containing the string `bill`.

```{r selecting_columns_start_string}
#| echo: true

#Selecting columns starting with bill

penguins_wo_NAs |> 
  select(starts_with('bill'))

```

### Selecting columns contain a particular string

```{r selecting_columns_string_length}
#| echo: true

#Selecting columns the string lenght
penguins_wo_NAs |> 
  select(contains('length'))


```

### Extending gamma and scope column search

Using regular expressions to extend the scope of the search for variables. For instance searching for variables that start with the letters **f** or **b**, and are followed by any amount of characters `.*` and then end (`$`) with the string `_mm`. The corresponding regular expression is: `^[fb].*(_mm)$`

```{r select_regex}
#| echo: true

penguins_wo_NAs |> 
  select(matches("^[fb].*(_mm)$"))


```

Two more ways to select columns/variables based on a vector that contains the names. You can use a vector to select the columns or variables of interest. Two optional functions `any_of()` or `all_of()` functions to select the components of the vector.

#### Selecting all function

```{r select_all_function}
#| echo: true

#Vector with the names of columns of interest

my_cols <-  c("bill_length_mm", "bill_depth_mm")


palmerpenguins::penguins |>
  select(all_of(my_cols))

```

#### Selecting any of function

`any_of( )` tidy helper is preferred to the `all_of( )` tidy helper. It helps to discriminate the variables you want to search when you list includes many columns.

```{r select_any}
#| echo: true

#Vector with the names of columns of interest

my_cols <-  c("bill_length_mm", "bill_depth_mm", "marital_status")


# any_of
penguins_wo_NAs |>
  select(any_of(my_cols))


```

## Lesson 17

### Bringing it all together.

Using the housing dataset from the `ames` database from `modeldata` library.

```{r bringing_it_together}
#| echo: true

library(modeldata)
library(janitor)
library(skimr)
library(modeldata)
library(gt)
library(gtExtras)

ames <- modeldata::ames |> 
  janitor::clean_names()


  
#Examining dataset using skimr

ames |> 
  clean_names() |> 
  skim()

#Examining a couple of numeric variables using gt_plt_summary
ames |> 
  clean_names() |> 
  select(sale_price,garage_cars) |> 
  gt_plt_summary()


```

### Transforming columns as tibble

Focusing our attention to numeric variables. We can also transform the variables a tibble while accessing the `ames` file. We select those `ames`' variables that are numeric.

```{r ames_numeric_variables}
#| echo: true

library(modeldata)
library(janitor)
library(skimr)
library(modeldata)
library(tidyverse)


ames |> 
  select(where(is.numeric)) |> 
  skim() |> 
  as_tibble()

ames_numeric <- ames |> 
  select(where(is.numeric)) |> 
  as_tibble()

```

Conducting summary statistics by focusing on variables that are counts. To do so we define a custom function that checks for those variables that are counts. Notice the function screens whether the values of the variable range from 0 to 10. This anonymous function would give us a string of FALSE or TRUE values. Using the `all()` function to check if the values of the variables range from 0 to 10. Notice the tidy helper `all_of()` is not applied, just `all`

Next we can use this information to identify continuous variables. The `ames_numeric` anonymous function identified 9 count variables. If you want to identify continuous variables negate the `!all(x %in% 0:10)`. This revised anonymous function yielded 25 variables that are continuous.

```{r checking_for_counting_variables}
#| echo: true

library(modeldata)
library(janitor)
library(skimr)
library(modeldata)

#Selecting counting variables whose values range from 0 to 10

ames_numeric |> 
  #Selecting columns with integers from 0 to 10
  select(where(function(x) all(x %in% 0:10) ) )


#To identify continuous variables negate the !all(x %in% 0:10)

ames_non_counts <- ames_numeric |> 
  select(where(function(x) !all(x %in% 0:10) ) )

```

Using the data subset of continuous variables. Let's focus on columns that contain the word `area`. Next identify variables with the `sf` string

```{r ames_continous_variables}
#| echo: true

ames_non_counts |> 
  select(contains('area'))


ames_non_counts |> 
  select(contains('sf'))

```

Eyeballing, it appears that garage and living area variables are the product of adding first floor square feet and second floor square feet. If this hypothesis is true excluding observations that result in adding these two variables would produce an empty rows across all variables.

The hypothesis was true for some rows or cases but not for 40 cases.

```{r testing_hypothesis_about_gr_liv_area}
#|echo: true

#* Examining if garage and living area variable is the product of adding
#* firts floor square feet and second floor square fit

ames_non_counts |> 
  filter(gr_liv_area != first_flr_sf + second_flr_sf
  )
```

### Sale price and calculating averages

#### Focusing on sale price.

```{r focusing_on_sale_price}
#| echo: true
#| warning: false

ames |> 
  skim(sale_price)

#* Checking sale price distribution as well. Notice median < mean 
#* suggesting the distribution is positively skewed.

ames |> 
  select(sale_price) |> 
  gt_plt_summary()
```

#### Calculating average prices

Notice, Albert introduces the function to request skewness. Skewness belongs to the `e1071` library. And to have it displayed in a nice format, we could pivot_longer.

Using across to estimate min, max, average, median, sd, and skeweness.

To have a nice display of the summary statistics, we could opt for pivoting longer the dataset.

In pivoting_longer, we opt to arrange all of the columns using `cols = everything()`. Notice the median is lower than the mean signifying the distribution is positively skewed.

The end result is a summary statistics table

```{r sale_price_summ_table}
#| echo: true

library(tidyverse)
library(gt)

#Calculating averages using across

ames |> 
  na.omit() |> 
  summarise(
    across(
      .cols = sale_price,
      .fns = list(
        min = min,
        avg = mean,
        median = median,
        max = max,
        standard_dev = sd,
        skewness = e1071::skewness),
      .names = '{.fn}'
    )
  ) |> 
#Pivoting longer to create a summary stats table
  pivot_longer(
    cols = everything()
  )

```

#### Focusing on the neighborhood column

The neighborhood column has 28 values, which makes it difficult to handle. Consequently, we could lump together values. The function`fct_lump_n()` lumps all levels except for the `n` most frequent (or least frequent if `n < 0`). In this case, we are focusing on the top 5 most frequent neighborhoods lumping together the rest. It produces 6 neighborhoods. Other is a collection of many neighborhoods.

```{r neighborhood_lumped}
#| echo: true

library(modeldata)
library(janitor)
library(skimr)
library(modeldata)


ames |> 
  count(neighborhood) |> 
  print(n = Inf)

ames |>
  mutate(neighborhood = fct_lump_n(
    neighborhood,
    n = 5)
    ) |> 
  skim(neighborhood)

ames_top_five <-  ames |>
  mutate(neighborhood = fct_lump_n(
    neighborhood,
    n = 5)
    ) 

ames_top_five |> 
  select(neighborhood) |> 
  count(neighborhood) |> 
  print(n = Inf)
```

#### Visualizing the data

With this new database, we can do data visualizations comparing the house prices across those 5 neighborhoods. We can use facet_wrap for the neighborhood. We can also use the `scale_x_log10()` to normalize the x-axis.

```{r visualizing_prices_neighborhoods}
#| echo: true

ames_top_five |> 
  ggplot(aes(x = sale_price)) +
  geom_density(fill = 'grey80') +
  facet_wrap(~ neighborhood, nrow = 6) +
  scale_x_log10(labels = scales::label_dollar()) +
  theme_minimal(
    base_family = 'Source Sans Pro',
    base_size = 16
  )

```

#### Refraining data visualization with minimum and maximum sale price

We can make this figure more informative by adding labels for the minimum and maximum sale price. To do so, we can add a geom_text() layer. The values for the layer can be extracted from the function `reframe()` .

The `reframe()` function allows one to calculate the sale price range. It would give a tibble that has the range of prices. Next we can add the label type to capture the labels minimum and maximum via `type = c("min", "max")`. And lets create those values by neighborhood using the function `.by = neighborhood`.

We can save the values in a dataframe, say `ranges`. In short, the `reframe( )` function generates a long data file containing the min and max sale prices across neighborhoods.

```{r reframe_informative_labels}
#| echo: true

ames_top_five |> 
  reframe(
    type = c("min", "max"),
    sale_price = range(sale_price),
    .by = neighborhood
  )

ranges <- ames_top_five |> 
  reframe(
    type = c("min", "max"),
    sale_price = range(sale_price),
    .by = neighborhood
  )
```

#### Adding text layer

Modifying ggplot to add a `geom_text()` layer capturing the labels associated to each of the 5 neighborhoods.

In the `geom_text()`, we specify the data, the aes(). For y we specify -1 and for x we rely on sale_price's ranges, and for label we refer to the variable containing the sale price. Notice we rely on y = -1 to place the labels below the chart.

```{r label_layer_geom_text}
#| echo: true

ames_top_five |> 
  ggplot(aes(x = sale_price)) +
  geom_density(fill = 'grey80') +
  geom_text(data = ranges, 
            aes(y = -1, label = sale_price)) +
  facet_wrap(vars(neighborhood), nrow = 6) +
  scale_x_log10(labels = scales::label_dollar()) +
  theme_minimal(
    base_family = 'Source Sans Pro',
    base_size = 16
  )

```

#### Improving the legend

Our first attempt with the labels produce characters too big. We can do this legend nicer by passing it to `scales::dollar()` function.

```{r adjusting_geom_label}
#| echo: true

ames_top_five |> 
  ggplot(aes(x = sale_price)) +
  geom_density(fill = 'grey80') +
  geom_text(data = ranges, 
            aes(y = -1, label = sale_price |> 
                  scales::dollar()) ) + 
  facet_wrap(~ neighborhood, nrow = 6) +
  scale_x_log10(labels = scales::label_dollar()) +
  theme_minimal(
    base_family = 'Source Sans Pro',
    base_size = 16
  )


```

To make it more obvious what those labels stand for, we can add another `geom_point()` layer. It adds points to the min and max ranges in the chart. We can also shape as a triangle-like instead of a point (shape = 5).

```{r passing_labels}
#| echo: true

ames_top_five |> 
  ggplot(aes(x = sale_price)) +
  geom_density(fill = 'grey80') +
   geom_point(data = ranges, 
            aes(y = -0.3),
            shape = 5,
            size = 2) +
  geom_text(data = ranges, 
            aes(y = -1.2, label = sale_price |> 
                  scales::dollar()), size = 2.5 ) + 
  facet_wrap(~ neighborhood, nrow = 6) +
  scale_x_log10(labels = scales::label_dollar()) +
  theme_minimal(
    base_family = 'Source Sans Pro',
    base_size = 16
  )

```

### Visualizing count variables

Another data visualization dealing with count variables. In this case, we identified 9 count variables. Let's check the column names. There are variables that include kitchen and basement

```{r viz_count_variables}
#| echo: true

ames_numeric |>
  select(where(function(x) all(x %in% 0:10) ) ) |>
  colnames()

```

We can reduce the number of count variables We end up with 7 variables instead of 9 by excluding variables related to basement and kitchen. Keeping variables with less than 6 values reduces the number of variables to 4.

As shown by skim, the values for the 4 variables range from 0 to 5.

```{r reduced_var_counts}
#| echo: true

selected_counts <- ames_numeric |>
  select(where(function(x) all(x %in% 0:5) ) ) |>
  select(
    -contains('bsmt'),
    -contains('kitchen')
  )

selected_counts |>
  skim()
```

We can mutate again with `across()`, and transforming the count variables into factor variables using the `.fns = factor`. Notice that for most variables the frequencies gravitate around 0 or 1. Consequently, we could lump together the variables.

```{r viz_counts_vars}
#| echo: true

selected_counts |>
  mutate(
    across(
      .cols = everything(),
      .fns = factor )
    ) |>
  skim()

```

#### Lumping factor variables

Lumping together factor variables with limited range. Instead of using `fct_lump_n`, we could use if_else to create variables capturing the lump sum. In so doing, we can use `across()` along an anonymous or custom function that leaves numbers smaller than 2 as is, but numbers greater than 2 are replaced with the text `2+`. The function to do so is `if_else().`

```{r changing_factors_lim_range}
#| echo: true

# selected_counts |>
#   mutate(
#     across(
#       .cols = everything(),
#       .fns = function(x) if_else(x < 2, x, '2+')
#   ) ) |>
#   skim()

```

#### Transforming numeric values into characters

Notice the function generated an error. Using '2+' along with factor variables prompted R to report not being able to combine integers with characters in the same function.

Consequently, we need to transform all the value labels into a character format. We can do so by using `as.character( )` function in the anonymous function. Notice the `as.character(x), '2+'` indicates that values equal or greater than 2 would be coded with the character '2+' .

The `skim()` function reveals there are three unique values. The range goes from 0 to 1. Notice the character '2+' is not listed in the range. However, skim reports 3 unique values, '2+' being one of them (0, 1, "2+").

```{r changing_factor_into_character}
#| echo: true

selected_counts |>
  mutate(
    across(
      .cols = everything(),
      .fns = function(x) if_else(x < 2,
                                 as.character(x), '2+')
  ) ) |>
  skim()


```

Next, we need to transform back the 3 values as factors by adding the `factor()` function at the end. Notice the levels = c(0:1, '2+')

```{r back_into_factor}
#| echo: true

selected_counts |>
  mutate(
    across(
      .cols = everything(),
      .fns = function(x) if_else(x < 2,
                                 as.character(x),
                                 '2+'
                                 ) |>
        factor(levels = c(0:1,'2+')) ) ) |>
  skim()

```

We can modify the dataset to include the house sale price, while excluding variables whose values are greater than 6. We also exclude basement and kitchen variables.

```{r house_sale_counts}
#| echo: true

selected_counts <- ames_numeric |>
  select(
    sale_price,
    where(function(x) all(x %in% 0:5) ) ) |>
  select(
    -contains('bsmt'),
    -contains('kitchen')
  )

selected_counts
```

And then, we have to make sure our `across()` call includes all count variables, while excluding sale_price column `across(.cols = -sale_price)`. The end result is the `grouped_counts` which includes count variables and the sale_price, which is a continuous variable.

#### Grouped accounts using `across(.cols = -sale_price)`

```{r includying_house_price_factors}
#| echo: true

selected_counts |>
  mutate(
    across(
      .cols = -sale_price,
      .fns = function(x) if_else(x < 2, as.character(x),'2+') |>
        factor(levels = c(0:1,'2+')) ) ) |>
  skim()

grouped_counts <- selected_counts |>
  mutate(
    across(
      .cols = -sale_price,
      .fns = function(x) if_else(x < 2,
                                 as.character(x),
                                 '2+'
                                 ) |>
        factor(levels = c(0:1,'2+')) ) )

grouped_counts

```

#### Displaying counts

The `grouped_counts` dataset needs to be transformed into a long format for ggplot(). We need to rearrange everything but the sale price.

```{r grp_count_pivot_longer}
#| echo: true

grouped_counts |>
  pivot_longer(
    cols = -sale_price
  ) |>
  ggplot(aes(x = sale_price)) +
  geom_density(fill = 'grey80')


```

#### Faceting the count data set

In the pivoted_longer version of the grouped_counts data set, the column name contains the values associated to baths, fireplace and garages. The value column displays the number of units, a factor variable ranging from 1, 0 to 2+.

```{r group_counts_facet_grid}

grouped_counts |>
  pivot_longer(
    cols = -sale_price
  ) |>
  ggplot(aes(x = sale_price)) +
  geom_density(fill = 'grey80') +
  scale_x_log10(label = scales::label_dollar()) +
  facet_grid(
    rows = vars(name),
    cols = vars(value)
    ) +
  theme_minimal(
    base_family = 'Source Sans Pro',
    base_size = 16
  )

```

#### Centering by amenities

We can center all distributions to their mean price using the `scale()` function. But we need to scale the distributions by the type of amenity. Notice we got lots of warning messages because of negative values that led to infinite values during the log-10 transformation. In other words, we should not use the log10 transformation.

```{r scaling_price_ammenities}
#| echo: true

grouped_counts |>
  pivot_longer(
    cols = -sale_price
  ) |>
  mutate(
    sale_price = scale(sale_price),
    .by = name
  ) |>
  ggplot(aes(x = sale_price)) +
  geom_density(fill = 'grey80') +
  scale_x_log10(label = scales::label_dollar()) +
  facet_grid(
    rows = vars(name),
    cols = vars(value)
    ) +
  theme_minimal(
    base_family = 'Source Sans Pro',
    base_size = 16
  )

```

##### Changing the log-10 transformation

Replacing the log_10 transformation by declaring x-axis scale as continuous: `scale_x_continuous()`

```{r repalcing_log_10}
#| echo: true

grouped_counts |>
  pivot_longer(
    cols = -sale_price
  ) |>
  mutate(
    sale_price = scale(sale_price),
    .by = name
  ) |>
  ggplot(aes(x = sale_price)) +
  geom_density(fill = 'grey80') +
  scale_x_continuous() +
  facet_grid(
    rows = vars(name),
    cols = vars(value)
    ) +
  theme_minimal(
    base_family = 'Source Sans Pro',
    base_size = 16
  )

```

#### log_transform variable

However, the scaling works better if you log10 scale the variable before using visualizing.

```{r log_before}
#| echo: true

grouped_counts |>
  pivot_longer(
    cols = -sale_price
  ) |>
  mutate(
    sale_price = scale(sale_price |> log()),
    .by = name
  ) |>
  ggplot(aes(x = sale_price)) +
  geom_density(fill = 'grey80') +
  scale_x_continuous() +
  facet_grid(
    rows = vars(name),
    cols = vars(value)
    ) +
  theme_minimal(
    base_family = 'Source Sans Pro',
    base_size = 16
  )

```
