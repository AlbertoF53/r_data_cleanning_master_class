---
title: "Part 2"
author: "Albert Rapp"
date: August 28, 2024
format:
  html:
    code-link: true
    code-fold: true
    code-summary: "Show the code"
    self-contained: true
    embed-resources: true
    toc: true
    theme: Zephyr
    number-sections: false
execute: 
  warning: false
  message: false
  echo: false
editor_options: 
  chunk_output_type: console
---

# Part 2

```{r loading_libraries}
#| echo: false

library(tidyverse)
library(janitor)
library(lubridate)
library(openxlsx)
library(tidyxl)
library(unpivotr)


```

## Lesson 1

Overview of different formats for the same database. Those are csv, csv with header, csv semicolon separator, tsv space separator, csv metadata, excel files, json files, which are annoying looking

## Lesson 2: Same data in in different formats

Loading csv files. It is recommended that you Rstudio project which set up the relative paths. You don't need to use here::here(path)

```{r loading_csv_files}
#| echo: true
#| warning: false
#| code-fold: true

library(tidyverse)


read_csv('data/Part_2/02_exibble.csv')

getwd()

```

## Lesson 3: Read standard csv-files

### How to read csv files with no headers.

Loading it directly the first row will be interpreted as the names of the columns. This is not what we want. Instead we need to tell `read_csv()` function that there are no column names.

By default, csv will assign generic column names.

```{r loading_csv_no_headers}
#| echo: true
#| warning: false
#| code-fold: true

read_csv('data/Part_2/01_exibble_no_headers.csv', 
         col_names = FALSE)


```

Instead, we can create a vector with the column names

```{r column_names}
#| echo: true
#| warning: false
#| code-fold: true

desired_colnames <-  c("fuit",
                       "letter",
                       "hrs",
                       "min",
                       "dates",
                       "missing_numbers",
                       "missing_NULL",
                       "missing_dates")

read_csv('data/Part_2/01_exibble_no_headers.csv', 
         col_names = desired_colnames)

```

### Reading a metadata csv file.

It displays a message with illegible data.

```{r reading_}
#| echo: true
#| warning: false
#| code-fold: true

read_csv("data/Part_2/05_exibble_with_metadata.csv")


```

We need to skip the first two rows, which are not relevant. And now the file is correctly displayed.

```{r skipping_rows_csv}
#| echo: true
#| warning: false
#| code-fold: true

read_csv("data/Part_2/05_exibble_with_metadata.csv",
         skip = 2)
```

### Read csv files in European format

Problem the first row has the name of the variables, but they are not recognized because of the use of ";" as delimiter. You need to replace the `read_csv()`function with the `read_csv2()`function.

```{r read_csv_european}
#| echo: true
#| warning: false
#| code-fold: true

read_csv2("data/Part_2/03_exibble_european.csv")

```

### Reading files with line breaks

RStudio reads this file as having two columns. Reading the quote file that appears to be connected with the line_break file

```{r reading_line_breaks}
#| echo: true
#| warning: false
#| code-fold: true

read_csv("data/Part_2/09_line_break.csv")

here::here()
read_csv("data/Part_2/10_quote.csv")

```

Instead, we need to use the `read_delim()` function. And we need to specify we want as a delimiter a `,`.

```{r read_delim_function}
#| echo: true
#| warning: false
#| code-fold: true

read_delim("data/Part_2/10_quote.csv",
           delim = ',',
           escape_backslash = TRUE)

#Another option

read_delim("data/Part_2/11_quote_doubled.csv",
           delim = ',',
           escape_double = TRUE)



```

## Lesson 4: Reading tsv and txt files

### Loading tsv files which have a separated value

```{r loading_tsv_files}
#| echo: true
#| warning: false
#| code-fold: true

read_tsv('data/Part_2/04_exibble.tsv')

```

### Loading arbitrary text files

```{r loading_arbitrary_text_files}
#| echo: true
#| warning: false
#| code-fold: true

read_delim('data/Part_2/12_weird_delimiter.txt',
           delim = '|')


```

## Lesson 5: Reading Excel Files

One package useful for reading excel files is `oepnxls`. However, the data frame looks weird. This is why we transform it into tibble.

```{r read_excel_files}
#| echo: true
#| warning: false
#| code-fold: true


library(openxlsx)

openxlsx::read.xlsx('data/Part_2/06_exibble.xlsx') |> 
  as_tibble()

```

However, the column with dates is not treated as a date but with a weird number. Consequently we could recode the variable dates as a date format using `lubridate`

```{r transforming_dates_excel}
#| echo: true
#| warning: false
#| code-fold: true


library(lubridate)

openxlsx::read.xlsx('data/Part_2/06_exibble.xlsx') |> 
  as_tibble() |> 
  mutate(dates = as_date(dates))


```

We could also use the `janitor` package to transfer dates into a date format

```{r janitor_date_transform}
#| echo: true
#| warning: false
#| code-fold: true

openxlsx::read.xlsx('data/Part_2/06_exibble.xlsx') |> 
  as_tibble() |> 
  mutate(dates = janitor::excel_numeric_to_date(dates))

```

## Lesson 6: Reading JSON files

JSON files require a particular package: `jsonlite` package. Notice that json files have a nested structure. Notice that json file contains \[\[80\]\].

The `str()` function comes handy. It displays the list signifying rows. Each row displays the names of the columns, and their contents.

### Using glimpse

You can also use the `glimpse` function.

```{r reading_json_files}
#| echo: true
#| warning: false
#| code-fold: true

library(jsonlite)

jsonlite::read_json('data/Part_2/07_exibble.json') |>
  str()


```

Using glimpse function to describe the organization of the json files

```{r json_glimpse}
#| echo: true
#| warning: false
#| code-fold: true

library(jsonlite)

jsonlite::read_json('data/Part_2/07_exibble.json') |>
  glimpse()



```

### Using bind_rows() function

Use bind rows function to find together the different rows that make up the json file. It allows us to create a nice tibble file.

```{r binding_rows_json}
#| echo: true
#| warning: false
#| code-fold: true

library(jsonlite)

jsonlite::read_json('data/Part_2/07_exibble.json') |>
  bind_rows()



```

### GLIMPSE and bind_rows functions

You can use both of them to describe the components of the file. And, then create the tibble file

```{r glimpse_bind_json}

#| echo: true
#| warning: false
#| code-fold: true

library(jsonlite)

jsonlite::read_json('data/Part_2/07_exibble.json') |>
  glimpse() |> 
  bind_rows()

#OR we can request glimpse after binding the rows

jsonlite::read_json('data/Part_2/07_exibble.json') |>
  bind_rows() |> 
  glimpse()

```

### Reading a second json file

This file is a special case. It has three columns. You cannot use`bind_rows()` function for this kind of file. But you can extract information from it

```{r another_json_file}
#| echo: true
#| warning: false
#| code-fold: true

library(jsonlite)
jsonlite::read_json('data/Part_2/08_nested_example.json') |> 
  glimpse()

```

Extracting information from `08_nested_example/json` using the `pluck()` function, which can go to a particular column. In this case, we want to go to the information column and then the origin column.

```{r extract_info_json}

#| echo: true
#| warning: false
#| code-fold: true

library(jsonlite)

jsonlite::read_json('data/Part_2/08_nested_example.json') |> 
  pluck('information','origin') |> 
  glimpse()


```

### Next we could pass this information as tibble

We need to unnest them, though.

```{r passing_structure_tibble}
#| echo: true
#| warning: false
#| code-fold: true

jsonlite::read_json('data/Part_2/08_nested_example.json') |> 
  pluck('information','origin') |> 
  as_tibble() |> 
  unnest(cols = c(country,season))

#Or

jsonlite::read_json('data/Part_2/08_nested_example.json') |> 
  pluck('information','origin') |> 
  as_tibble() |> 
  unnest(cols = everything())



```

## Lesson 7: Turning missing values into true NAs

```{r finding_na}
#| echo: true
#| warning: false
#| code-fold: true

read_csv("data/Part_2/02_exibble.csv") |> 
  select(1:6)


```

### Problematic variables

Notice that these two columns have values that need to be recoded as missing.

```{r encounter_na_values}
#| echo: true
#| warning: false
#| code-fold: true

read_csv("data/Part_2/02_exibble.csv") |> 
  select(-(1:6))

```

### Recoding missing while reading a file

Notice that once we recoded the missing values correctly, the type of variable changes from being a character to numeric or date

```{r recoding_missing_reading}
#| echo: true
#| warning: false
#| code-fold: true

read_csv("data/Part_2/02_exibble.csv",
         na = c("", "(null)", "Unknown")) |>
  select(-(1:6)) |> 
  glimpse()


```

### Handling missing values in excel files

Notice variables missing_Null and missing_dates are incorrectly identified as characters.

```{r handling_na_excel}
#| echo: true
#| warning: false
#| code-fold: true

openxlsx::read.xlsx("data/Part_2/06_exibble.xlsx") |> 
  as_tibble() |> 
  select(7:8) |> 
  glimpse()

```

### Recoding missing in an excel file

```{r recoding_na_excel}
#| echo: true
#| warning: false
#| code-fold: true

openxlsx::read.xlsx("data/Part_2/06_exibble.xlsx",
                    na.strings = c("", "(null)", "Unknown")) |> 
  as_tibble() |> 
  select(-c(1:6))


```

However, the `na.strings()` conversion do not transform the variables back to their correct format: numeric and date

## Lesson 8: Transforming/Parsing numbers

Correcting or parsing numbers in an excel file

```{r correct_na_format_excel}
#| echo: true
#| warning: false
#| code-fold: true

openxlsx::read.xlsx(
  "data/Part_2/06_exibble.xlsx"
  ) |> 
  as_tibble() |> 
  select(-c(1:6)) |> 
  mutate(missing_NULL = parse_number(
    missing_NULL,
    na = c("", "NA", "(null)", "Unknown")
  ))


```

## Lesson 9: Transforming/Parsing dates & datetimes

```{r parsin_dates_datetimes}

#| echo: true
#| warning: false
#| code-fold: true

openxlsx::read.xlsx(
  "data/Part_2/06_exibble.xlsx"
  ) |> 
  as_tibble() |> 
  select(-c(1:6)) |> 
  mutate(missing_NULL = parse_number(
    missing_NULL,
    na = c("", "NA", "(null)", "Unknown")
  ),
  missing_dates = lubridate::ymd_hms(missing_dates, 
                                     quiet = TRUE)
  )


```

Extracting only dates without times

```{r parse_dates_readr}
#| echo: true
#| warning: false
#| code-fold: true

openxlsx::read.xlsx(
  "data/Part_2/06_exibble.xlsx"
  ) |> 
  as_tibble() |> 
  select(-c(1:6)) |> 
  mutate(missing_NULL = parse_number(
    missing_NULL,
    na = c("", "NA", "(null)", "Unknown")
  ),
  missing_dates = parse_datetime(
    missing_dates,
    format = '%Y-%m-%d %H:%M:%S',
    na = "Unknown"
  )
  )

```

For extracting just dates, use the readr function `parse_date()` as shown below

```{r parse_dates_no_time}
#| echo: true
#| warning: false
#| code-fold: true

openxlsx::read.xlsx(
  "data/Part_2/06_exibble.xlsx"
  ) |> 
  as_tibble() |> 
  select(-c(1:6)) |> 
  mutate(missing_NULL = parse_number(
    missing_NULL,
    na = c("", "NA", "(null)", "Unknown")
  ),
  missing_dates = parse_date(
    missing_dates,
    format = '%Y-%m-%d %H:%M:%S',
    na = "Unknown"
  )
  )

```

## Lesson 10: Handling a mix of date(time) formats

Example below displays a vector with dates with different formats.

```{r mix_formats}
#| echo: true
#| warning: false
#| code-fold: true

tibble(
  mixed_datetimes = c(
    '23.12.2024 12:23',
    '23.12.2024',
    '2023-12-23',
    'NULL'
  )
)


```

The previous functions such as `parse_datetime()` and `parse_number()` won't work for this example. On the other hand, the `lubridate` package works best for this situation.

The option `orders` can tell lubridate the correct order of the date to be followed. This option save us the complicated format `format = '%Y-%m-%d %H:%M:%S'` used in parse_date from read.xlsx. Instead we use lubridate to specify day, month, year and hour "c(`dmyHM`)". And then we specify the correct format for each element of the vector.

Notice there is no option to handle missing values. This is why we need to use the quite argument, which codes the string `NULL` as missing.

```{r handling_mix_date_formats}
#| echo: true
#| warning: false
#| code-fold: true

tibble(
  mixed_datetimes = c(
    '23.12.2024 12:23',
    '23.12.2024',
    '2023-12-23',
    'NULL'
  )
) |> 
  mutate(
   mixed_datetimes = lubridate::parse_date_time(
     mixed_datetimes,
     orders = c('dmyHM','dmy','ymd')
     ),
   quiet = TRUE
  )



```

## Lesson 11: Combining data sets column-wise & row-wise

This lesson introduces the equivalent concepts of merging from Stata and SPSS

### Binding rows

Binding rows is flexible. You do not need to order the columns in a particular manner provided they have the same label. binding rows use the column label from the first dataset.

```{r combining_rows}
#| echo: true
#| warning: false
#| code-fold: true

tib1 <-  tibble(a = 1:5, b = 1:5)
tib2 <-  tibble(a = 6:10, b = 6:10)

bind_rows(tib1,tib2)

tib1 <-  tibble(b = 1:5, a = 1:5)
tib2 <-  tibble(a = 6:10, b = 6:10)

bind_rows(tib1,tib2)

#Another option of adding a column c with incomplete cases

tib1 <-  tibble(b = 1:5, a = 1:5)
tib2 <-  tibble(a = 6:10, b = 6:10, c = 6:10)

bind_rows(tib1,tib2)

```

### Binding columns

The only difference between binding columns and binding rows is that the former is strict about the number of rows. They have to have the same number

```{r binding_columns}
#| echo: true
#| warning: false
#| code-fold: true


tib1 <-  tibble(x = 1:5)
tib2 <-  tibble(y = fruit[1:5])

bind_cols(tib1,tib2)

```

## Lesson 12: Combining datasets by left\_ and right_join

This lesson illustrates how to combine two data sets. Each dataset has different rows, and different names for the key variable. In the flights dataset the key variable is `origin`. In the airports dataset the key variable is `faa`.

### Creating flights data set

```{r flights_data}
#| echo: true

flights <-  nycflights23::flights |> 
  as_tibble()

```

### Creating airlines datnamea set

```{r airlines_data}
#| echo: true
#| warning: false
#| code-fold: true

airlines <-  nycflights23::airlines |> 
  as_tibble()

```

### left_join

Selecting some variables from the flights dataset. We want to add the name of the carrier. They key matching variable in both data sets is `carrier`.

```{r selecting_columns_flights_dataset}
#| echo: true
#| warning: false
#| code-fold: true

flights |> 
  select(year:dep_time, carrier) |> 
  left_join(airlines, by = 'carrier')

```

Another example of left join that may not be as easier as the previous example. In this example we have abbreviations for airport destinations. For merging, we need another database with the full name of the airports. This name is found in the `airports` data set under the label `faa`.

In the example below, we are filtering from the airports' column faa the names that appear in the flights dataset variable origin. In other words the two variables are the same (faa = origin). The trick is to eliminate duplicates.

If we try to merge the two files as before a mistake will occur because R does not know that faa is the same as origin.

```{r left_join_calling_for_key}
#| echo: true
#| warning: false
#| code-fold: true

library(nycflights23)

#* Eliminating duplicate names of airports.  
#* The end result of filtering is selecting NY airports only


airports <- nycflights23::airports |> 
  filter(faa %in% unique(flights$origin))

#The following script generates an error. There is no common key
# flights |> 
#   select(year:dep_time, origin) |> 
#   left_join(airports)
#   


```

### Adding key in left_join.

The last script generated an error when trying to merge flights with airports. There is no common key across the two data sets. The key is has a different label. Under the flights data set the airport abbreviation appears under the variable `origin`. In the `airport` data set the abbreviation is `faa`. In other words `origin == faa`.

```{r adding_key_left_join}
#| echo: true
#| warning: false
#| code-fold: true

flights |> 
  select(year:dep_time, origin) |> 
  left_join(airports, by = join_by(origin == faa))



```

### Adding key in right_join.

We get exactly the same dataset made up of 435,352 rows. The reason it produced the same results is the filtering we applied to the airports dataset `filter(faa %in% unique(flights$origin))`

```{r right_join_adding_key}
#| echo: true
#| warning: false
#| code-fold: true

flights |> 
  select(year:dep_time, origin) |> 
  right_join(airports,
             by = join_by(origin == faa))
  
```

Let's see what happens when eliminating the filtering that allowed both dataset to have the same airports.

Redoing the right and left join don't produce the same number of rows. The left join yield 435,342 rows while the right produces 436,590 rows.

```{r right_join_no_filtering}
#| echo: true
#| warning: false
#| code-fold: true

airports <- nycflights23::airports

flights |> 
  select(year:dep_time, origin) |> 
  left_join(airports,
             by = join_by(origin == faa))


flights |> 
  select(year:dep_time, origin) |> 
  right_join(airports,
             by = join_by(origin == faa))



```

Notice there is a discrepancy of about 1,000 airports. The number of airports produced under left_join is smaller than the one produced by right_join.

Providing a more detailed explanation about the discrepancies on the number of cases.

The **left_join** makes the number of cases in the first database to **prevail** in the **merger**. The contrary is true when doing right_join.

```{r illustrating_rows_merged_left_join}
#| echo: true
#| warning: false
#| code-fold: true

reduced_left <- flights |> 
  select(year:dep_time, origin) |> 
  slice(1, .by = origin) |> 
  left_join(airports,
             by = join_by(origin == faa))

reduced_left

```

Reducing the airports by right_join. It produces 1,248 missing rows.

```{r illustrating_reduced_right}
#| echo: true
#| warning: false
#| code-fold: true

reduced_right <- flights |> 
  select(year:dep_time, origin) |> 
  slice(1, .by = origin) |> 
  right_join(airports,
             by = join_by(origin == faa))

reduced_right

```

## Lesson 13: Other join types

Other joins being illustrated with a small dataset. In this example the left_join kept all the airports from the `flights` data set, or 1,251. But the columns are empty except for the New York airports that are contained in the `flights` dataset.

```{r other_joins}
#| echo: true
#| warning: false
#| code-fold: true

flights |> 
  select(year:dep_time, origin) |> 
  slice(1, .by = origin) |> 
  left_join(airports,
             by = join_by(origin == faa))

airports |> 
  left_join(
    flights |> 
      select(year:dep_time, origin) |> 
      slice(1, .by = origin), 
    by = join_by(faa == origin))



```

### Full join

Using two rows, or two rows per airpot

```{r full_join}

airports |> 
  full_join(
    flights |> 
      select(year:dep_time, origin) |> 
      slice(1:2, .by = origin), 
    by = join_by(faa == origin))


```

### Anti join

It is more useful than full join because it can tell where unexpected things may go wrong. In this case it tells you the airports that could not be matched. In this case, 1,248 airports that could not be matched.

```{r anti_join}
#| echo: true
#| warning: false
#| code-fold: true

airports |> 
  anti_join(
    flights |> 
      select(year:dep_time, origin) |> 
      slice(1, .by = origin), 
    by = join_by(faa == origin))


```

## Lesson 14: Join by multiple criteria

Using join based on multiple criteria.

### Creating wind speeds dataset

```{r wind_speeds_df}
#| echo: true
#| warning: false
#| code-fold: true


wind_speeds <-  nycflights23::weather |> 
  select(origin:hour, wind_speed)

wind_speeds

```

Matching based on multiple criteria instead of just two columns. We want to enrich the flights data set with information about wind speeds.

```{r multiple_criteria_join}
#| echo: true
#| warning: false
#| code-fold: true

library(nycflights23)

flights |> 
  select(year:day,hour,origin,tailnum) |> 
  left_join(wind_speeds)


```

Notice that without specifying the matching criteria the left_join worked. It did so because the two data set share some variables in common as shown in the output:

\`by = join_by(year, month, day, hour, origin)\`

```{r explicit_multiple_join_criteria}
#| echo: true
#| warning: false
#| code-fold: true


flights |> 
  select(year:day,hour,origin,tailnum) |> 
  left_join(wind_speeds,
            by = join_by(year,month,day,hour,origin)) 

```

Matching on a certain types of column names. Simulating that the flight's column origin is labeled `dep_airport` . If we try to match these two files it won't work.

```{r incorrect_selective_multiple_matching}
#| echo: true
#| warning: false
#| code-fold: true

# flights |> 
#   select(year:day,hour,dep_airport = origin,tailnum) |> 
#   left_join(wind_speeds,
#             by = join_by(year,month,day,hour,origin)) 



```

To make it match, you have to specify that dep_airport == origin in the join_by statement.

```{r correct_selective_matching}
#| echo: true
#| warning: false
#| code-fold: true

flights |> 
  select(year:day,hour,dep_airport = origin,tailnum) |> 
  left_join(wind_speeds,
            by = join_by(year,month,day,hour,dep_airport == origin)) 


```

## Lesson 15: Rearranging data from wide to long format with pivot_longer()

Differences between wide and long format

![](images/differences_wide_longer.PNG){fig-align="center" width="352"}

### Pivot longer

illustrating these two concepts using the scores data set titled scores, which is in a wide format. Accordingly, date is the id, scores A to C are the key and the values are the score.

Let's illustrate the utility of pivot_longer by summarizing

```{r changing_wide_to_longer}
#| echo: true
#| warning: false
#| code-fold: true

scores <- read_csv('data/Part_2/13_scores_by_date.csv')

scores |> 
  #Other options for cols: cols = 2:5 cols = contains('score')
  pivot_longer(cols = -date, 
               names_to = "score_category",
               values_to = "score")

scores |> 
  pivot_longer(cols = -date, #cols = 2:5 cols = contains('score')
               names_to = "score_category",
               values_to = "score") |> 
  summarise(
    mean_score = mean(score),
    .by = score_category
  )



```

### Another example

This file needs to be rearrange before pivoting longer. Notice that `payment` the value column appears first.

```{r pivot_longer_example_payments}
#| echo: true
#| warning: false
#| code-fold: true

payments <-  read_csv('data/Part_2/14_payments.csv')

payments |> 
  pivot_longer(
    cols = contains('date')
  )

#We need meaning names for the new columns

payments |> 
  pivot_longer(
    cols = contains('date'),
    names_to = "payment_date",
    values_to = "date"
  )

```

## Lesson 16: Advanced column name tricks with pivot_longer()

### scores data set example

Regarding the scores data set. The end result of the pivot longer is to have score categories with the label score_cat repeated. This does not make sense afterwards. We could do some cleaning. We can target the score_category column and using regex to remove unwanted strings. The `names_prefix()` can be used to removed the unwanted `score_` at the start of the string.

```{r cleaning_scores_dataset}
#| echo: true
#| warning: false
#| code-fold: true

scores |> 
  pivot_longer(cols = -date, 
               names_to = "score_category",
               values_to = "score",
               names_prefix = "score_")



```

### payments example

In the payments example, we have suffixes that need to be removed. Under the name column, the suffix `_payment_date` needs to be removed. This string appears at the end of the column. Unfortunately pivot longer does not have suffix function, however, it has `names_pattern()` function, which works similarly to `str_extract()`. We need to add a group `(\\w*)` of characters preceding the strings to be removed. This group of characters is to be kept.

Another option is to use the regular expression find me anything `(.)`. But one dot stands for only one letter. But adding `+` makes it plural. This means: "of this dot or character, find me as many as you can". Notice the dot signifies to count at the end of the string.

```{r cleaing_payments_example}
#| echo: true
#| warning: false
#| code-fold: true

#One option to handle the group of characters
payments |> 
  pivot_longer(
    cols = contains('date'),
    names_to = "payment_date",
    values_to = 'date',
    names_pattern = "(\\w+)_payment_date"
  )

#The next option is to list the characters

payments |> 
  pivot_longer(
    cols = contains('date'),
    names_to = "payment_date",
    values_to = 'date',
    names_pattern = "(last|next|final)_payment_date"
  ) |> 
  arrange(utility,date)

# Another regular expression
# Find me anything (.)

payments |> 
  pivot_longer(
    cols = contains('date'),
    names_to = "payment_date",
    values_to = 'date',
    names_pattern = "(.+)_payment_date"
  ) |> 
  arrange(utility,date)
```

## Lesson 17: Making long data wide with pivot_wider()

Pivot longer is more frequently used than `pivot-wider()` . However, pivot_wider is extremely useful when creating tables. In pivot wider you need to revert the `names_to` and `values_to`.

```{r making_long_data_pivot_wider}
#| echo: true
#| warning: false
#| code-fold: true


payments |> 
  pivot_longer(
    cols = contains('date'),
    names_to = "payment_date",
    values_to = "date",
    names_pattern = "(.+)_payment_date"
  ) |> 
  pivot_wider(
    names_from = 'payment_date',
    values_from = 'date'
  )



```

Next is a reserve name, this is why the label appears with ticks. We can use the pivot_wider function `names_glue()`. If we want a variable name we use curly brackets `{ }`. And then we write the text we want to add outside the curly brackets. In this case we want to rebuild the original value labels last_payment_date, next_payment_date and final_payment_date. Thus we add the name of the variable containing the labels; namely, payment_date

```{r removing_reserve_label_pivot_wider}
#| echo: true
#| warning: false
#| code-fold: true

payments |> 
  pivot_longer(
    cols = contains('date'),
    names_to = "payment_date",
    values_to = "date",
    names_pattern = "(.+)_payment_date"
  ) |> 
  pivot_wider(
    names_from = 'payment_date',
    values_from = 'date',
    #identify variable within {}, then add legend outside {}
    names_glue = '{payment_date}_payment_date'
  )

```

### Identifying id column

pivot_wider has an option to identify the variable or variables that are key. It is identified as `id_cols()`. By identifying utility as id column eliminates the payment column.

```{r pivot_longwer_id_columns}
#| echo: true
#| warning: false
#| code-fold: true

payments |> 
  pivot_longer(
    cols = contains('date'),
    names_to = "payment_date",
    values_to = "date",
    names_pattern = "(.+)_payment_date"
  ) |> 
  pivot_wider(
    id_cols = c('utility'),
    names_from = 'payment_date',
    values_from = 'date',
    #identify variable within {}, then add legend outside {}
    names_glue = '{payment_date}_payment_date'
  )

#Includying payment as an id column

payments |> 
  pivot_longer(
    cols = contains('date'),
    names_to = "payment_date",
    values_to = "date",
    names_pattern = "(.+)_payment_date"
  ) |> 
  pivot_wider(
    id_cols = c('utility', 'payment'),
    names_from = 'payment_date',
    values_from = 'date',
    #identify variable within {}, then add legend outside {}
    names_glue = '{payment_date}_payment_date'
  )

```

## Lesson 18: Cleaning up Excel Pivot Tables

Watch [tidyxl](https://nacnudus.github.io/tidyxl/) animation. More information on tidyxl is [here](https://cran.r-project.org/web/packages/tidyxl/vignettes/tidyxl.html).

Using `xlsx_cells()` function from tidyxl package. The tidyxl package is new. The `xlsx_cells()` function imports data from spreadsheets without coercing it into a rectangle. Each cell is represented by a row in a data frame, giving the cell's address, contents, formula, height, width, and keys to look up the cell's formatting in the return value of `xlsx_formats()`.

The original file, 17_rand_numbers.xlsx, is a pivot table. Variables are organized by country, region, age and gender. The tidyxl package help us to prepare the excel pivot data to be processed by other packages, like unpivotr with its function `rectify()`. The function transform the data into a rectangular fashion.

```{r cleaning_pivot_part_1}
#| echo: true
#| warning: false
#| code-fold: true

library(tidyxl)
library(unpivotr)

rand_numbers_cells <- tidyxl::xlsx_cells(
  'data/Part_2/17_rand_numbers.xlsx')

rand_numbers_cells |> 
  unpivotr::rectify()

```

Once the file is rectified we can strip the labels step by step. We can get rid of the Country A and Country B labels. They are located under the column labeled "3(C)". Then we can get rid of the regions. And so on.

We start by stripping the labels step by step. The first is to target Gender, Male, Female and NAs. These descriptors are metadata, not data. To get rid off those entries, we need to eliminate those entries under the character column that have the words, gender and age. We need to filter the cells where we have missing values or things are not gender or age.

Next we can start rectifying it by `behading` it. [`behead()`](http://127.0.0.1:42565/help/library/unpivotr/help/behead) takes one level of headers from a pivot table and makes it part of the data. The main purpose is to eliminate 3(C) column. We want to behave on the left style, because the country labels are at the left of our original pivot table. So we use `left-up`. And then we want to assign the contents into a column we would label `country`.

But, if we look at the cells, we discover where the column country is.

```{r cleaning_pivot_part_2}
#| echo: true
#| warning: false
#| code-fold: true

rand_numbers_cells |> 
  filter(
    is.na(character) | !(
      character %in% c('Gender', 'Age')
      )
    ) |>
  unpivotr::rectify()
  
#Now we can start behading this 
  
test3 <- rand_numbers_cells |> 
  filter(is.na(character) | !(character %in% c('Gender', 'Age')
      )
    ) |>
  unpivotr::behead('left-up', name = 'country') |> 
  unpivotr::rectify()

```

Next step eliminating the extra country column. And we put the values in a new column label 'region'.

```{r cleaning_pivot_part_3}
#| echo: true
#| warning: false
#| code-fold: true

rand_numbers_cells |> 
  filter(
    is.na(character) | !(
      character %in% c('Gender', 'Age')
      )
    ) |>
  unpivotr::behead('left-up',name = 'country'
  ) |> 
   unpivotr::behead('left',name = 'region'
  ) |> 
  unpivotr::rectify()


```

Doing the same for gender.

```{r cleaning_pivot_part_4}
#| echo: true
#| warning: false
#| code-fold: true

rand_numbers_cells |> 
  filter(
    is.na(character) | !(
      character %in% c('Gender', 'Age')
      )
    ) |>
  unpivotr::behead('left-up',name = 'country'
  ) |>
   unpivotr::behead('left',name = 'region'
  ) |>
  unpivotr::behead('up-left',name = 'gender'
  ) |>
  unpivotr::rectify()
  
  
  
  unpivotr::behead('up-left',name = 'age'
  ) |>
  unpivotr::rectify()



```

Next we need to do the same for age

```{r cleaning_pivot_part_5}
#| echo: true
#| warning: false
#| code-fold: true

rand_numbers_cells |> 
  filter(
    is.na(character) | !(
      character %in% c('Gender', 'Age')
      )
    ) |>
  unpivotr::behead('left-up',name = 'country'
  ) |>
   unpivotr::behead('left',name = 'region'
  ) |>
  unpivotr::behead('up-left',name = 'gender'
  ) |>
  unpivotr::behead('up',name = 'age'
  ) |>
  unpivotr::rectify()
  

```

### Final data creation

We no longer need to rectify. All what we need is to select the new columns. Those are located at the end of the data frame. To see those 4 new columns (country, region, gender and age) you need to create a temporary file. And also include the variable/column numeric. For this procedure to work, you need to eliminate the rectify function.

```{r cleaning_pivot_part_final}
#| echo: true
#| warning: false
#| code-fold: true


rand_numbers_cells |> 
  filter(
    is.na(character) | !(
      character %in% c('Gender', 'Age')
      )
    ) |>
  unpivotr::behead('left-up',name = 'country'
  ) |>
   unpivotr::behead('left',name = 'region'
  ) |>
  unpivotr::behead('up-left',name = 'gender'
  ) |>
  unpivotr::behead('up',name = 'age'
  ) |>
  unpivotr::rectify()

#Selecting country, region, age, gender, numeric columns

rand_numbers_cells |> 
  filter(
    is.na(character) | !(
      character %in% c('Gender', 'Age')
      )
    ) |>
  unpivotr::behead('left-up',name = 'country'
  ) |>
   unpivotr::behead('left',name = 'region'
  ) |>
  unpivotr::behead('up-left',name = 'gender'
  ) |>
  unpivotr::behead('up',name = 'age'
  ) |>
  #Excludying rectity
  #unpivotr::rectify() |>
  select(country:age,numeric)

```

## Lesson 19 Beheading with multiple data formats

### Locating unwanted labels

Dealing with a complex and unbalanced spreadsheet with several pivot tables. Under the column `1(A)` we have a couple of labels, City, Department, that are not part of the data, but they describe the data

```{r beheading_multiple_formats_1}
#| echo: true
#| warning: false
#| code-fold: true

library(tidyxl)

sales_cells <- tidyxl::xlsx_cells('data/Part_2/16_faker_sales_modified.xlsx')

test <- sales_cells |> 
  unpivotr::rectify()

sales_cells |> 
  unpivotr::rectify()


```

### Eliminating unwanted labels

We first want to focus our attention to the second column and on only. The first column refers to the excel spreadsheet (sheet 1)

```{r beheading_multiple_formats_2}
#| echo: true
#| warning: false
#| code-fold: true

sales_cells |> 
  filter(col > 1) |> 
  unpivotr::rectify()



```

### Cleaning city rows

Using up-left again mode. The next row under 2(B) displays the sales department, groceries. We are left with the data observations displaying customer number, name, gender, etc.

```{r beheading_multiple_formats_3}
#| echo: true
#| warning: false
#| code-fold: true

sales_cells |> 
  filter(col > 1) |> 
   unpivotr::behead(
     'up-left',
     name = 'city') |>
  unpivotr::behead(
     'up-left',
     name = 'department') |>
  unpivotr::behead(
     'up',
     name = 'variable') |>
    unpivotr::rectify()




```

### Displaying variables created

The variables created are city, department and variable.

```{r displaying_variables_created}
#| echo: true
#| warning: false
#| code-fold: true

test <- sales_cells |> 
  filter(col > 1) |> 
   unpivotr::behead(
     'up-left',
     name = 'city') |>
  unpivotr::behead(
     'up-left',
     name = 'department') |>
  unpivotr::behead(
     'up',
     name = 'variable')


behaded_sales_cells <- sales_cells |> 
  filter(col > 1) |> 
   unpivotr::behead(
     'up-left',
     name = 'city') |>
  unpivotr::behead(
     'up-left',
     name = 'department') |>
  unpivotr::behead(
     'up',
     name = 'variable') |> 
  select(city:variable, character,numeric,row)
```

### Further cleaning the file

We want to have the character and numeric columns in one single column using the function `coalesce()`. Given a set of vectors, `coalesce()` finds the first non-missing value at each position. It's inspired by the SQL `COALESCE` function which does the same thing for SQL `NULL`s. We need to temporarily convert the numeric variable as character otherwise coalesce won't work.

After the merging or coalescing of character and numeric columns into cell values, we won't need the two columns.

```{r cleaning_the_file_behaded}
#| echo: true
#| warning: false
#| code-fold: true

behaded_sales_cells |> 
  mutate(
    cell_values = coalesce(character, as.character(numeric))
  ) |> 
  select(-c(character,numeric))



```

### Wider transforming

Now we can transform the database into a wider format. Take the names of the column `variable` and the values from the `cell_values` column. The rest of the variables, `city` and `department`, would serve as unique identifier.

```{r widening_behaded_file}
#| echo: true
#| warning: false
#| code-fold: true


behaded_sales_cells |> 
  mutate(
    cell_values = coalesce(character, as.character(numeric))
  ) |> 
  select(-c(character,numeric)) |> 
  pivot_wider(
    names_from = 'variable',
    values_from = 'cell_values'
  ) |> 
  #Getting rid of the row column
  select(-row) |> 
  #Recoding price as numeric
  mutate(price = parse_number(price))



```
