---
title: "Part 2"
author: "Albert Rapp"
date: August 28, 2024
format:
  html:
    code-link: true
    code-fold: true
    code-summary: "Show the code"
    self-contained: true
    embed-resources: true
    toc: true
    theme: Zephyr
    number-sections: false
execute: 
  warning: false
  message: false
  echo: false
editor_options: 
  chunk_output_type: console
---

# Part 2

```{r loading_libraries}
#| echo: false

library(tidyverse)
library(janitor)
library(lubridate)
library(openxlsx)
library(jsonlite)


```

## Lesson 1

Overview of different formats for the same database. Those are csv, csv with header, csv semicolon separator, tsv space separator, csv metadata, excel files, json files, which are annoying looking

## Lesson 2: Same data in in different formats

Loading csv files. It is recommended that you Rstudio project which set up the relative paths. You don't need to use here::here(path)

```{r loading_csv_files}
#| echo: true
#| warning: false
#| code-fold: true

library(tidyverse)


read_csv('data/Part_2/02_exibble.csv')

getwd()

```

## Lesson 3: Read standard csv-files

### How to read csv files with no headers.

Loading it directly the first row will be interpreted as the names of the columns. This is not what we want. Instead we need to tell `read_csv()` function that there are no column names.

By default, csv will assign generic column names.

```{r loading_csv_no_headers}
#| echo: true
#| warning: false
#| code-fold: true

read_csv('data/Part_2/01_exibble_no_headers.csv', 
         col_names = FALSE)


```

Instead, we can create a vector with the column names

```{r column_names}
#| echo: true
#| warning: false
#| code-fold: true

desired_colnames <-  c("fuit",
                       "letter",
                       "hrs",
                       "min",
                       "dates",
                       "missing_numbers",
                       "missing_NULL",
                       "missing_dates")

read_csv('data/Part_2/01_exibble_no_headers.csv', 
         col_names = desired_colnames)

```

### Reading a metadata csv file.

It displays a message with illegible data.

```{r reading_}
#| echo: true
#| warning: false
#| code-fold: true

read_csv("data/Part_2/05_exibble_with_metadata.csv")


```

We need to skip the first two rows, which are not relevant. And now the file is correctly displayed.

```{r skipping_rows_csv}
#| echo: true
#| warning: false
#| code-fold: true

read_csv("data/Part_2/05_exibble_with_metadata.csv",
         skip = 2)
```

### Read csv files in European format

Problem the first row has the name of the variables, but they are not recognized because of the use of ";" as delimiter. You need to replace the `read_csv()`function with the `read_csv2()`function.

```{r read_csv_european}
#| echo: true
#| warning: false
#| code-fold: true

read_csv2("data/Part_2/03_exibble_european.csv")

```

### Reading files with line breaks

RStudio reads this file as having two columns. Reading the quote file that appears to be connected with the line_break file

```{r reading_line_breaks}
#| echo: true
#| warning: false
#| code-fold: true

read_csv("data/Part_2/09_line_break.csv")

here::here()
read_csv("data/Part_2/10_quote.csv")

```

Instead, we need to use the `read_delim()` function. And we need to specify we want as a delimiter a `,`.

```{r read_delim_function}
#| echo: true
#| warning: false
#| code-fold: true

read_delim("data/Part_2/10_quote.csv",
           delim = ',',
           escape_backslash = TRUE)

#Another option

read_delim("data/Part_2/11_quote_doubled.csv",
           delim = ',',
           escape_double = TRUE)



```

## Lesson 4: Reading tsv and txt files

### Loading tsv files which have a separated value

```{r loading_tsv_files}
#| echo: true
#| warning: false
#| code-fold: true

read_tsv('data/Part_2/04_exibble.tsv')

```

### Loading arbitrary text files

```{r loading_arbitrary_text_files}
#| echo: true
#| warning: false
#| code-fold: true

read_delim('data/Part_2/12_weird_delimiter.txt',
           delim = '|')


```

## Lesson 5: Reading Excel Files

One package useful for reading excel files is `oepnxls`. However, the data frame looks weird. This is why we transform it into tibble.

```{r read_excel_files}
#| echo: true
#| warning: false
#| code-fold: true


library(openxlsx)

openxlsx::read.xlsx('data/Part_2/06_exibble.xlsx') |> 
  as_tibble()

```

However, the column with dates is not treated as a date but with a weird number. Consequently we could recode the variable dates as a date format using `lubridate`

```{r transforming_dates_excel}
#| echo: true
#| warning: false
#| code-fold: true


library(lubridate)

openxlsx::read.xlsx('data/Part_2/06_exibble.xlsx') |> 
  as_tibble() |> 
  mutate(dates = as_date(dates))


```

We could also use the `janitor` package to transfer dates into a date format

```{r janitor_date_transform}
#| echo: true
#| warning: false
#| code-fold: true

openxlsx::read.xlsx('data/Part_2/06_exibble.xlsx') |> 
  as_tibble() |> 
  mutate(dates = janitor::excel_numeric_to_date(dates))

```

## Lesson 6: Reading JSON files

JSON files require a particular package: `jsonlite` package. Notice that json files have a nested structure. Notice that json file contains \[\[80\]\].

The `str()` function comes handy. It displays the list signifying rows. Each row displays the names of the columns, and their contents.

### Using glimpse

You can also use the `glimpse` function.

```{r reading_json_files}
#| echo: true
#| warning: false
#| code-fold: true

library(jsonlite)

jsonlite::read_json('data/Part_2/07_exibble.json') |>
  str()


```

Using glimpse function to describe the organization of the json files

```{r json_glimpse}
#| echo: true
#| warning: false
#| code-fold: true

library(jsonlite)

jsonlite::read_json('data/Part_2/07_exibble.json') |>
  glimpse()



```

### Using bind_rows() function

Use bind rows function to find together the different rows that make up the json file. It allows us to create a nice tibble file.

```{r binding_rows_json}
#| echo: true
#| warning: false
#| code-fold: true

library(jsonlite)

jsonlite::read_json('data/Part_2/07_exibble.json') |>
  bind_rows()



```

### GLIMPSE and bind_rows functions

You can use both of them to describe the components of the file. And, then create the tibble file

```{r glimpse_bind_json}

#| echo: true
#| warning: false
#| code-fold: true

library(jsonlite)

jsonlite::read_json('data/Part_2/07_exibble.json') |>
  glimpse() |> 
  bind_rows()

#OR we can request glimpse after binding the rows

jsonlite::read_json('data/Part_2/07_exibble.json') |>
  bind_rows() |> 
  glimpse()

```

### Reading a second json file

This file is a special case. It has three columns. You cannot use`bind_rows()` function for this kind of file. But you can extract information from it

```{r another_json_file}
#| echo: true
#| warning: false
#| code-fold: true

library(jsonlite)
jsonlite::read_json('data/Part_2/08_nested_example.json') |> 
  glimpse()

```

Extracting information from `08_nested_example/json` using the `pluck()` function, which can go to a particular column. In this case, we want to go to the information column and then the origin column.

```{r extract_info_json}

#| echo: true
#| warning: false
#| code-fold: true

library(jsonlite)

jsonlite::read_json('data/Part_2/08_nested_example.json') |> 
  pluck('information','origin') |> 
  glimpse()


```

### Next we could pass this information as tibble

We need to unnest them, though.

```{r passing_structure_tibble}
#| echo: true
#| warning: false
#| code-fold: true

jsonlite::read_json('data/Part_2/08_nested_example.json') |> 
  pluck('information','origin') |> 
  as_tibble() |> 
  unnest(cols = c(country,season))

#Or

jsonlite::read_json('data/Part_2/08_nested_example.json') |> 
  pluck('information','origin') |> 
  as_tibble() |> 
  unnest(cols = everything())



```

## Lesson 7: Turning missing values into true NAs

```{r finding_na}
#| echo: true
#| warning: false
#| code-fold: true

read_csv("data/Part_2/02_exibble.csv") |> 
  select(1:6)


```

### Problematic variables

Notice that these two columns have values that need to be recoded as missing.

```{r encounter_na_values}
#| echo: true
#| warning: false
#| code-fold: true

read_csv("data/Part_2/02_exibble.csv") |> 
  select(-(1:6))

```

### Recoding missing while reading a file

Notice that once we recoded the missing values correctly, the type of variable changes from being a character to numeric or date

```{r recoding_missing_reading}
#| echo: true
#| warning: false
#| code-fold: true

read_csv("data/Part_2/02_exibble.csv",
         na = c("", "(null)", "Unknown")) |>
  select(-(1:6)) |> 
  glimpse()


```

### Handling missing values in excel files

Notice variables missing_Null and missing_dates are incorrectly identified as characters.

```{r handling_na_excel}
#| echo: true
#| warning: false
#| code-fold: true

openxlsx::read.xlsx("data/Part_2/06_exibble.xlsx") |> 
  as_tibble() |> 
  select(7:8) |> 
  glimpse()

```

### Recoding missing in an excel file

```{r recoding_na_excel}
#| echo: true
#| warning: false
#| code-fold: true

openxlsx::read.xlsx("data/Part_2/06_exibble.xlsx",
                    na.strings = c("", "(null)", "Unknown")) |> 
  as_tibble() |> 
  select(-c(1:6))


```

However, the `na.strings()` conversion do not transform the variables back to their correct format: numeric and date

## Lesson 8: Transforming/Parsing numbers

Correcting or parsing numbers in an excel file

```{r correct_na_format_excel}
#| echo: true
#| warning: false
#| code-fold: true

openxlsx::read.xlsx(
  "data/Part_2/06_exibble.xlsx"
  ) |> 
  as_tibble() |> 
  select(-c(1:6)) |> 
  mutate(missing_NULL = parse_number(
    missing_NULL,
    na = c("", "NA", "(null)", "Unknown")
  ))


```

## Lesson 9: Transforming/Parsing dates & datetimes

```{r parsin_dates_datetimes}

#| echo: true
#| warning: false
#| code-fold: true

openxlsx::read.xlsx(
  "data/Part_2/06_exibble.xlsx"
  ) |> 
  as_tibble() |> 
  select(-c(1:6)) |> 
  mutate(missing_NULL = parse_number(
    missing_NULL,
    na = c("", "NA", "(null)", "Unknown")
  ),
  missing_dates = lubridate::ymd_hms(missing_dates, 
                                     quiet = TRUE)
  )


```

Extracting only dates without times

```{r parse_dates_readr}
#| echo: true
#| warning: false
#| code-fold: true

openxlsx::read.xlsx(
  "data/Part_2/06_exibble.xlsx"
  ) |> 
  as_tibble() |> 
  select(-c(1:6)) |> 
  mutate(missing_NULL = parse_number(
    missing_NULL,
    na = c("", "NA", "(null)", "Unknown")
  ),
  missing_dates = parse_datetime(
    missing_dates,
    format = '%Y-%m-%d %H:%M:%S',
    na = "Unknown"
  )
  )

```

For extracting just dates, use the readr function `parse_date()` as shown below

```{r parse_dates_no_time}
#| echo: true
#| warning: false
#| code-fold: true

openxlsx::read.xlsx(
  "data/Part_2/06_exibble.xlsx"
  ) |> 
  as_tibble() |> 
  select(-c(1:6)) |> 
  mutate(missing_NULL = parse_number(
    missing_NULL,
    na = c("", "NA", "(null)", "Unknown")
  ),
  missing_dates = parse_date(
    missing_dates,
    format = '%Y-%m-%d %H:%M:%S',
    na = "Unknown"
  )
  )

```

## Lesson 10: Handling a mix of date(time) formats

Example below displays a vector with dates with different formats.

```{r mix_formats}
#| echo: true
#| warning: false
#| code-fold: true

tibble(
  mixed_datetimes = c(
    '23.12.2024 12:23',
    '23.12.2024',
    '2023-12-23',
    'NULL'
  )
)


```

The previous functions such as `parse_datetime()` and `parse_number()` won't work for this example. On the other hand, the `lubridate` package works best for this situation.

The option `orders` can tell lubridate the correct order of the date to be followed. This option save us the complicated format `format = '%Y-%m-%d %H:%M:%S'` used in parse_date from read.xlsx. Instead we use lubridate to specify day, month, year and hour "c(`dmyHM`)". And then we specify the correct format for each element of the vector.

Notice there is no option to handle missing values. This is why we need to use the quite argument, which codes the string `NULL` as missing.

```{r handling_mix_date_formats}
#| echo: true
#| warning: false
#| code-fold: true

tibble(
  mixed_datetimes = c(
    '23.12.2024 12:23',
    '23.12.2024',
    '2023-12-23',
    'NULL'
  )
) |> 
  mutate(
   mixed_datetimes = lubridate::parse_date_time(
     mixed_datetimes,
     orders = c('dmyHM','dmy','ymd')
     ),
   quiet = TRUE
  )



```

## Lesson 11: Combining data sets column-wise & row-wise

This lesson introduces the equivalent concepts of merging from Stata and SPSS

### Binding rows

Binding rows is flexible. You do not need to order the columns in a particular manner provided they have the same label. binding rows use the column label from the first dataset.

```{r combining_rows}
#| echo: true
#| warning: false
#| code-fold: true

tib1 <-  tibble(a = 1:5, b = 1:5)
tib2 <-  tibble(a = 6:10, b = 6:10)

bind_rows(tib1,tib2)

tib1 <-  tibble(b = 1:5, a = 1:5)
tib2 <-  tibble(a = 6:10, b = 6:10)

bind_rows(tib1,tib2)

#Another option of adding a column c with incomplete cases

tib1 <-  tibble(b = 1:5, a = 1:5)
tib2 <-  tibble(a = 6:10, b = 6:10, c = 6:10)

bind_rows(tib1,tib2)

```

### Binding columns

The only difference between binding columns and binding rows is that the former is strict about the number of rows. They have to have the same number

```{r binding_columns}
#| echo: true
#| warning: false
#| code-fold: true


tib1 <-  tibble(x = 1:5)
tib2 <-  tibble(y = fruit[1:5])

bind_cols(tib1,tib2)

```

## Lesson 12: Combining datasets by left\_ and right_join

This lesson illustrates how to combine two data sets. Each dataset has different rows, and different names for the key variable. In the flights dataset the key variable is `origin`. In the airports dataset the key variable is `faa`.

### Creating flights data set

```{r flights_data}
#| echo: true

flights <-  nycflights23::flights |> 
  as_tibble()

```

### Creating airlines datnamea set

```{r airlines_data}
#| echo: true
#| warning: false
#| code-fold: true

airlines <-  nycflights23::airlines |> 
  as_tibble()

```

### left_join

Selecting some variables from the flights dataset. We want to add the name of the carrier. They key matching variable in both data sets is `carrier`.

```{r selecting_columns_flights_dataset}
#| echo: true
#| warning: false
#| code-fold: true

flights |> 
  select(year:dep_time, carrier) |> 
  left_join(airlines, by = 'carrier')

```

Another example of left join that may not be as easier as the previous example. In this example we have abbreviations for airport destinations. For merging, we need another database with the full name of the airports. This name is found in the `airports` data set under the label `faa`.

In the example below, we are filtering from the airports' column faa the names that appear in the flights dataset variable origin. In other words the two variables are the same (faa = origin). The trick is to eliminate duplicates.

If we try to merge the two files as before a mistake will occur because R does not know that faa is the same as origin.

```{r left_join_calling_for_key}
#| echo: true
#| warning: false
#| code-fold: true

library(nycflights23)

#* Eliminating duplicate names of airports.  
#* The end result of filtering is selecting NY airports only


airports <- nycflights23::airports |> 
  filter(faa %in% unique(flights$origin))

#The following script generates an error. There is no common key
# flights |> 
#   select(year:dep_time, origin) |> 
#   left_join(airports)
#   


```

### Adding key in left_join.

The last script generated an error when trying to merge flights with airports. There is no common key across the two data sets. The key is has a different label. Under the flights data set the airport abbreviation appears under the variable `origin`. In the `airport` data set the abbreviation is `faa`. In other words `origin == faa`.

```{r adding_key_left_join}
#| echo: true
#| warning: false
#| code-fold: true

flights |> 
  select(year:dep_time, origin) |> 
  left_join(airports, by = join_by(origin == faa))



```

### Adding key in right_join.

We get exactly the same dataset made up of 435,352 rows. The reason it produced the same results is the filtering we applied to the airports dataset `filter(faa %in% unique(flights$origin))`

```{r right_join_adding_key}
#| echo: true
#| warning: false
#| code-fold: true

flights |> 
  select(year:dep_time, origin) |> 
  right_join(airports,
             by = join_by(origin == faa))
  
```

Let's see what happens when eliminating the filtering that allowed both dataset to have the same airports.

Redoing the right and left join don't produce the same number of rows. The left join yield 435,342 rows while the right produces 436,590 rows.

```{r right_join_no_filtering}
#| echo: true
#| warning: false
#| code-fold: true

airports <- nycflights23::airports

flights |> 
  select(year:dep_time, origin) |> 
  left_join(airports,
             by = join_by(origin == faa))


flights |> 
  select(year:dep_time, origin) |> 
  right_join(airports,
             by = join_by(origin == faa))



```

Notice there is a discrepancy of about 1,000 airports. The number of airports produced under left_join is smaller than the one produced by right_join.

Providing a more detailed explanation about the discrepancies on the number of cases.

The **left_join** makes the number of cases in the first database to **prevail** in the **merger**. The contrary is true when doing right_join.

```{r illustrating_rows_merged_left_join}
#| echo: true
#| warning: false
#| code-fold: true

reduced_left <- flights |> 
  select(year:dep_time, origin) |> 
  slice(1, .by = origin) |> 
  left_join(airports,
             by = join_by(origin == faa))

reduced_left

```

Reducing the airports by right_join. It produces 1,248 missing rows.

```{r illustrating_reduced_right}
#| echo: true
#| warning: false
#| code-fold: true

reduced_right <- flights |> 
  select(year:dep_time, origin) |> 
  slice(1, .by = origin) |> 
  right_join(airports,
             by = join_by(origin == faa))

reduced_right

```

## Lesson 13: Other join types

Other joins being illustrated with a small dataset. In this example the left_join kept all the airports from the `flights` data set, or 1,251. But the columns are empty except for the New York airports that are contained in the `flights` dataset.

```{r other_joins}
#| echo: true
#| warning: false
#| code-fold: true

flights |> 
  select(year:dep_time, origin) |> 
  slice(1, .by = origin) |> 
  left_join(airports,
             by = join_by(origin == faa))

airports |> 
  left_join(
    flights |> 
      select(year:dep_time, origin) |> 
      slice(1, .by = origin), 
    by = join_by(faa == origin))



```

### Full join

Using two rows, or two rows per airpot

```{r full_join}

airports |> 
  full_join(
    flights |> 
      select(year:dep_time, origin) |> 
      slice(1:2, .by = origin), 
    by = join_by(faa == origin))


```

### Anti join

It is more useful than full join because it can tell where unexpected things may go wrong. In this case it tells you the airports that could not be matched. In this case, 1,248 airports that could not be matched.

```{r anti_join}
#| echo: true
#| warning: false
#| code-fold: true

airports |> 
  anti_join(
    flights |> 
      select(year:dep_time, origin) |> 
      slice(1, .by = origin), 
    by = join_by(faa == origin))


```

## Lesson 14: Join by multiple criteria

Using join based on multiple criteria.

### Creating wind speeds dataset

```{r wind_speeds_df}
#| echo: true
#| warning: false
#| code-fold: true


wind_speeds <-  nycflights23::weather |> 
  select(origin:hour, wind_speed)

wind_speeds

```

Matching based on multiple criteria instead of just two columns. We want to enrich the flights data set with information about wind speeds.

```{r multiple_criteria_join}
#| echo: true
#| warning: false
#| code-fold: true

library(nycflights23)

flights |> 
  select(year:day,hour,origin,tailnum) |> 
  left_join(wind_speeds)


```

Notice that without specifying the matching criteria the left_join worked. It did so because the two data set share some variables in common as shown in the output:

\`by = join_by(year, month, day, hour, origin)\`

```{r explicit_multiple_join_criteria}
#| echo: true
#| warning: false
#| code-fold: true


flights |> 
  select(year:day,hour,origin,tailnum) |> 
  left_join(wind_speeds,
            by = join_by(year,month,day,hour,origin)) 

```

Matching on a certain types of column names. Simulating that the flight's column origin is labeled `dep_airport` . If we try to match these two files it won't work.

```{r incorrect_selective_multiple_matching}
#| echo: true
#| warning: false
#| code-fold: true

# flights |> 
#   select(year:day,hour,dep_airport = origin,tailnum) |> 
#   left_join(wind_speeds,
#             by = join_by(year,month,day,hour,origin)) 



```

To make it match, you have to specify that dep_airport == origin in the join_by statement.

```{r correct_selective_matching}
#| echo: true
#| warning: false
#| code-fold: true

flights |> 
  select(year:day,hour,dep_airport = origin,tailnum) |> 
  left_join(wind_speeds,
            by = join_by(year,month,day,hour,dep_airport == origin)) 


```

## Lesson 15: Rearranging data from wide to long format with pivot_longer()

Differences between wide and long format

![](images/differences_wide_longer.PNG){fig-align="center" width="352"}

### Pivot longer

illustrating these two concepts using the scores data set titled scores, which is in a wide format. Accordingly, date is the id, scores A to C are the key and the values are the score.

Let's illustrate the utility of pivot_longer by summarizing

```{r changing_wide_to_longer}
#| echo: true
#| warning: false
#| code-fold: true

scores <- read_csv('data/Part_2/13_scores_by_date.csv')

scores |> 
  #Other options for cols: cols = 2:5 cols = contains('score')
  pivot_longer(cols = -date, 
               names_to = "score_category",
               values_to = "score")

scores |> 
  pivot_longer(cols = -date, #cols = 2:5 cols = contains('score')
               names_to = "score_category",
               values_to = "score") |> 
  summarise(
    mean_score = mean(score),
    .by = score_category
  )



```

### Another example

This file needs to be rearrange before pivoting longer. Notice that `payment` the value column appears first.

```{r pivot_longer_example_payments}
#| echo: true
#| warning: false
#| code-fold: true

payments <-  read_csv('data/Part_2/14_payments.csv')

payments |> 
  pivot_longer(
    cols = contains('date')
  )

#We need meaning names for the new columns

payments |> 
  pivot_longer(
    cols = contains('date'),
    names_to = "payment_date",
    values_to = "date"
  )

```

## Lesson 16: Advanced column name tricks with pivot_longer()

### scores data set example

Regarding the scores data set. The end result of the pivot longer is to have score categories with the label score_cat repeated. This does not make sense afterwards. We could do some cleaning. We can target the score_category column and using regex to remove unwanted strings. The `names_prefix()` can be used to removed unwanted `score_` text.

```{r cleaning_scores_dataset}
#| echo: true
#| warning: false
#| code-fold: true

scores |> 
  pivot_longer(cols = -date, 
               names_to = "score_category",
               values_to = "score",
               names_prefix = "score_")



```

### payments example

In the payments example, we have suffixes that need to be removed. Under the name column, the suffix `_payment_date` needs to be removed. This string appears at the end of the column. Unfortunately pivot longer does not have suffix function, however, it has `names_pattern()` function, which works similarly to `str_extract()`. We need to add a group `(\\w*)` of characters preceding the strings to be removed. This group of characters is to be kept.

Another option is to use the regular expression find me anything `(.)`. But one dot stands for only one letter. But adding `+` makes it plural.

```{r cleaing_payments_example}
#| echo: true
#| warning: false
#| code-fold: true

#One option to handle the group of characters
payments |> 
  pivot_longer(
    cols = contains('date'),
    names_pattern = "(\\w*)_payment_date"
  )

#The next option is to list the characters

payments |> 
  pivot_longer(
    cols = contains('date'),
    names_pattern = "(last|next|final)_payment_date"
  ) |> 
  arrange(utility,value)

# Another regular expression
# Find me anything (.)

payments |> 
  pivot_longer(
    cols = contains('date'),
    names_pattern = "(.+)_payment_date"
  ) |> 
  arrange(utility,value)
```
